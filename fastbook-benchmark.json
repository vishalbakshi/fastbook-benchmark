{
    "questions": [
        {
            "chapter": 1,
            "question_number": 1,
            "question_text": "Do you need these for deep learning?\n\n- Lots of math T / F\n   - Lots of data T / F\n   - Lots of expensive computers T / F\n   - A PhD T / F",
            "gold_standard_answer": "\"Lots of math - False\nLots of data - False\nLots of expensive computers - False\nA PhD - False\"",
            "answer_context": [
                {
                    "answer_component": "\"Lots of math - False\nLots of data - False\nLots of expensive computers - False\nA PhD - False\"",
                    "scoring_type": "simple", 
                    "context": [
                        "```asciidoc\n[[myths]]\n.What you don't need to do deep learning\n[options=\"header\"]\n|======\n| Myth (don't need) | Truth\n| Lots of math | Just high school math is sufficient\n| Lots of data | We've seen record-breaking results with <50 items of data\n| Lots of expensive computers | You can get what you need for state of the art work for free\n|======\n```"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 2,
            "question_text": "Name five areas where deep learning is now the best in the world",
            "gold_standard_answer": "\"Any five of the following:\nNatural Language Processing (NLP) \u2013 Question Answering, Document Summarization and Classification, etc.\nComputer Vision \u2013 Satellite and drone imagery interpretation, face detection and recognition, image captioning, etc.\nMedicine \u2013 Finding anomalies in medical images (ex: CT, X-ray, MRI), detecting features in tissue slides (pathology), diagnosing diabetic retinopathy, etc.\nBiology \u2013 Folding proteins, classifying, genomics tasks, cell classification, etc.\nImage generation/enhancement \u2013 colorizing images, improving image resolution (super-resolution), removing noise from images (denoising), converting images to art in style of famous artists (style transfer), etc.\nRecommendation systems \u2013 web search, product recommendations, etc.\nPlaying games \u2013 Super-human performance in Chess, Go, Atari games, etc\nRobotics \u2013 handling objects that are challenging to locate (e.g. transparent, shiny, lack of texture) or hard to pick up\nOther applications \u2013 financial and logistical forecasting; text to speech; much much more.\"",
            "answer_context": [
                {
                    "answer_component": "\"Any five of the following:\nNatural Language Processing (NLP) \u2013 Question Answering, Document Summarization and Classification, etc.\nComputer Vision \u2013 Satellite and drone imagery interpretation, face detection and recognition, image captioning, etc.\nMedicine \u2013 Finding anomalies in medical images (ex: CT, X-ray, MRI), detecting features in tissue slides (pathology), diagnosing diabetic retinopathy, etc.\nBiology \u2013 Folding proteins, classifying, genomics tasks, cell classification, etc.\nImage generation/enhancement \u2013 colorizing images, improving image resolution (super-resolution), removing noise from images (denoising), converting images to art in style of famous artists (style transfer), etc.\nRecommendation systems \u2013 web search, product recommendations, etc.\nPlaying games \u2013 Super-human performance in Chess, Go, Atari games, etc\nRobotics \u2013 handling objects that are challenging to locate (e.g. transparent, shiny, lack of texture) or hard to pick up\nOther applications \u2013 financial and logistical forecasting; text to speech; much much more.\"",
                    "scoring_type": "simple", 

                    "context": [
                        "- Natural language processing (NLP):: Answering questions; speech recognition; summarizing documents; classifying documents; finding names, dates, etc. in documents; searching for articles mentioning a concept\n- Computer vision:: Satellite and drone imagery interpretation (e.g., for disaster resilience); face recognition; image captioning; reading traffic signs; locating pedestrians and vehicles in autonomous vehicles\n- Medicine:: Finding anomalies in radiology images, including CT, MRI, and X-ray images; counting features in pathology slides; measuring features in ultrasounds; diagnosing diabetic retinopathy\n- Biology:: Folding proteins; classifying proteins; many genomics tasks, such as tumor-normal sequencing and classifying clinically actionable genetic mutations; cell classification; analyzing protein/protein interactions\n- Image generation:: Colorizing images; increasing image resolution; removing noise from images; converting images to art in the style of famous artists\n- Recommendation systems:: Web search; product recommendations; home page layout\n- Playing games:: Chess, Go, most Atari video games, and many real-time strategy games\n- Robotics:: Handling objects that are challenging to locate (e.g., transparent, shiny, lacking texture) or hard to pick up\n- Other applications:: Financial and logistical forecasting, text to speech, and much more..."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "Name five areas where deep learning is now the best in the world", 
                    "context": [
                        "Here's a list of some of the thousands of tasks in different areas at which deep learning, or methods heavily using deep learning, is now the best in the world:"
                    ]
                }
            ]
        },
        {
            "chapter": 1,
            "question_number": 3,
            "question_text": "What was the name of the first device that was based on the principle of the artificial neuron?",
            "gold_standard_answer": "\"Mark I perceptron built by Frank Rosenblatt\"",
            "answer_context": [
                {
                    "answer_component": "\"Mark I perceptron built by Frank Rosenblatt\"",
                    "scoring_type": "simple",
                    "context": [
                        "Rosenblatt further developed the artificial neuron to give it the ability to learn. Even more importantly, he worked on building the first device that actually used these principles, the Mark I Perceptron. In \"The Design of an Intelligent Automaton\" Rosenblatt wrote about this work: \"We are now about to witness the birth of such a machine\u2013-a machine capable of perceiving, recognizing and identifying its surroundings without any human training or control.\" The perceptron was built, and was able to successfully recognize simple shapes."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 4,
            "question_text": "Based on the book of the same name, what are the requirements for parallel distributed processing (PDP)?",
            "gold_standard_answer": "\"A set of processing units\nA state of activation\nAn output function for each unit\nA pattern of connectivity among units\nA propagation rule for propagating patterns of activities through the network of connectivities\nAn activation rule for combining the inputs impinging on a unit with the current state of that unit to produce a new level of activation for the unit\nA learning rule whereby patterns of connectivity are modified by experience\nAn environment within which the system must operate\"",
            "answer_context": [
                {
                    "answer_component": "\"A set of processing units\nA state of activation\nAn output function for each unit\nA pattern of connectivity among units\nA propagation rule for propagating patterns of activities through the network of connectivities\nAn activation rule for combining the inputs impinging on a unit with the current state of that unit to produce a new level of activation for the unit\nA learning rule whereby patterns of connectivity are modified by experience\nAn environment within which the system must operate\"",
                    "scoring_type": "simple",
                    "context": [
                        "1. A set of *processing units*\n1. A *state of activation*\n1. An *output function* for each unit \n1. A *pattern of connectivity* among units \n1. A *propagation rule* for propagating patterns of activities through the network of connectivities \n1. An *activation rule* for combining the inputs impinging on a unit with the current state of that unit to produce an output for the unit\n1. A *learning rule* whereby patterns of connectivity are modified by experience \n1. An *environment* within which the system must operate"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "Based on the book of the same name, what are the requirements for parallel distributed processing (PDP)?", 
                    "context": [
                        "In fact, the approach laid out in PDP is very similar to the approach used in today's neural networks. The book defined parallel distributed processing as requiring:"
                    ]
                }
            ]
        },
        {
            "chapter": 1,
            "question_number": 5,
            "question_text": "What were the two theoretical misunderstandings that held back the field of neural networks?",
            "gold_standard_answer": "\"In 1969, Marvin Minsky and Seymour Papert demonstrated in their book, \u201cPerceptrons\u201d, that a single layer of artificial neurons cannot learn simple, critical mathematical functions like XOR logic gate. While they subsequently demonstrated in the same book that additional layers can solve this problem, only the first insight was recognized, leading to the start of the first AI winter.\n\nIn the 1980\u2019s, models with two layers were being explored. Theoretically, it is possible to approximate any mathematical function using two layers of artificial neurons. However, in practices, these networks were too big and too slow. While it was demonstrated that adding additional layers improved performance, this insight was not acknowledged, and the second AI winter began. In this past decade, with increased data availability, and improvements in computer hardware (both in CPU performance but more importantly in GPU performance), neural networks are finally living up to its potential.\"",
            "answer_context": [
                {
                    "answer_component": "\"In 1969, Marvin Minsky and Seymour Papert demonstrated in their book, \u201cPerceptrons\u201d, that a single layer of artificial neurons cannot learn simple, critical mathematical functions like XOR logic gate. While they subsequently demonstrated in the same book that additional layers can solve this problem, only the first insight was recognized, leading to the start of the first AI winter.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "An MIT professor named Marvin Minsky (who was a grade behind Rosenblatt at the same high school!), along with Seymour Papert, wrote a book called _Perceptrons_ (MIT Press), about Rosenblatt's invention. They showed that a single layer of these devices was unable to learn some simple but critical mathematical functions (such as XOR). In the same book, they also showed that using multiple layers of the devices would allow these limitations to be addressed. Unfortunately, only the first of these insights was widely recognized. As a result, the global academic community nearly entirely gave up on neural networks for the next two decades."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"\n\nIn the 1980\u2019s, models with two layers were being explored. Theoretically, it is possible to approximate any mathematical function using two layers of artificial neurons. However, in practices, these networks were too big and too slow. While it was demonstrated that adding additional layers improved performance, this insight was not acknowledged, and the second AI winter began. In this past decade, with increased data availability, and improvements in computer hardware (both in CPU performance but more importantly in GPU performance), neural networks are finally living up to its potential.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "In the 1980's most models were built with a second layer of neurons, thus avoiding the problem that had been identified by Minsky and Papert (this was their \"pattern of connectivity among units,\" to use the framework above). And indeed, neural networks were widely used during the '80s and '90s for real, practical projects. However, again a misunderstanding of the theoretical issues held back the field. In theory, adding just one extra layer of neurons was enough to allow any mathematical function to be approximated with these neural networks, but in practice such networks were often too big and too slow to be useful."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 6,
            "question_text": "What is a GPU?",
            "gold_standard_answer": "\"GPU stands for Graphics Processing Unit (also known as a graphics card). Standard computers have various components like CPUs, RAM, etc. CPUs, or central processing units, are the core units of all standard computers, and they execute the instructions that make up computer programs. GPUs, on the other hand, are specialized units meant for displaying graphics, especially the 3D graphics in modern computer games. The hardware optimizations used in GPUs allow it to handle thousands of tasks at the same time. Incidentally, these optimizations allow us to run and train neural networks hundreds of times faster than a regular CPU.\"",
            "answer_context": [
                {
                    "answer_component": "\"GPU stands for Graphics Processing Unit (also known as a graphics card). Standard computers have various components like CPUs, RAM, etc. CPUs, or central processing units, are the core units of all standard computers, and they execute the instructions that make up computer programs. GPUs, on the other hand, are specialized units meant for displaying graphics, especially the 3D graphics in modern computer games. The hardware optimizations used in GPUs allow it to handle thousands of tasks at the same time. Incidentally, these optimizations allow us to run and train neural networks hundreds of times faster than a regular CPU.\"",
                    "scoring_type": "simple",
                    "context": [
                        "> jargon: Graphics Processing Unit (GPU): Also known as a _graphics card_. A special kind of processor in your computer that can handle thousands of single tasks at the same time, especially designed for displaying 3D environments on a computer for playing games. These same basic tasks are very similar to what neural networks do, such that GPUs can run neural networks hundreds of times faster than regular CPUs. All modern computers contain a GPU, but few contain the right kind of GPU necessary for deep learning."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 7,
            "question_text": "Open a notebook and execute a cell containing: 1+1. What happens?",
            "gold_standard_answer": "\"In a Jupyter Notebook, we can create code cells and run code in an interactive manner. When we execute a cell containing some code (in this case: 1+1), the code is run by Python and the output is displayed underneath the code cell (in this case: 2).\"",
            "answer_context": [
                {
                    "answer_component": "When we execute a cell containing some code (in this case: 1+1), the code is run by Python and the output is displayed underneath the code cell (in this case: 2).", 
                    "scoring_type": "simple",
                    "context": [
                        "```python\n1+1\n```\nOutput:\n2"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "In a Jupyter Notebook, we can create code cells and run code in an interactive manner", 
                    "scoring_type": "simple",
                    "context": [
                        "> jargon: Jupyter Notebook: A piece of software that allows you to include formatted text, code, images, videos, and much more, all within a single interactive document",

                        "Cells containing code that can be executed, and outputs will appear immediately underneath (which could be plain text, tables, images, animations, sounds, or even interactive applications)"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 10,
            "question_text": "Why is it hard to use a traditional computer program to recognize images in a photo?",
            "gold_standard_answer": "\"For us humans, it is easy to identify images in a photos, such as identifying cats vs dogs in a photo. This is because, subconsciously our brains have learned which features define a cat or a dog for example. But it is hard to define set rules for a traditional computer program to recognize a cat or a dog. Can you think of a universal rule to determine if a photo contains a cat or dog? How would you encode that as a computer program? This is very difficult because cats, dogs, or other objects, have a wide variety of shapes, textures, colors, and other features, and it is close to impossible to manually encode this in a traditional computer program.\"",
            "answer_context": [
                {
                    "answer_component": "For us humans, it is easy to identify images in a photos, such as identifying cats vs dogs in a photo. This is because, subconsciously our brains have learned which features define a cat or a dog for example.", 
                    "scoring_type": "simple",
                    "context": [
                        "But for recognizing objects in a photo that's a bit tricky; what *are* the steps we take when we recognize an object in a picture? We really don't know, since it all happens in our brain without us being consciously aware of it!"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "But it is hard to define set rules for a traditional computer program to recognize a cat or a dog. Can you think of a universal rule to determine if a photo contains a cat or dog? How would you encode that as a computer program? This is very difficult because cats, dogs, or other objects, have a wide variety of shapes, textures, colors, and other features, and it is close to impossible to manually encode this in a traditional computer program.", 
                    "scoring_type": "simple",
                    "context": [
                        "*Machine learning* is, like regular programming, a way to get computers to complete a specific task. But how would we use regular programming to do what we just did in the last section: recognize dogs versus cats in photos? We would have to write down for the computer the exact steps necessary to complete the task.",

                        "Normally, it's easy enough for us to write down the steps to complete a task when we're writing a program. We just think about the steps we'd take if we had to do the task by hand, and then we translate them into code."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },

        {
            "chapter": 1,
            "question_number": 11,
            "question_text": "\"What did Samuel mean by \"weight assignment\"?\"",
            "gold_standard_answer": "\"\u201cweight assignment\u201d refers to the current values of the model parameters. Arthur Samuel further mentions an \u201c automatic means of testing the effectiveness of any current weight assignment \u201d and a \u201c mechanism for altering the weight assignment so as to maximize the performance \u201d. This refers to the evaluation and training of the model in order to obtain a set of parameter values that maximizes model performance.\"",
            "answer_context": [
                {
                    "answer_component": "\"\u201cweight assignment\u201d", 
                    "scoring_type": "simple",
                    "context": [
                        "Weights are just variables, and a weight assignment is a particular choice of values for those variables."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "model parameters", 
                    "scoring_type": "simple",
                    "context": [
                        "(By the way, what Samuel called \"weights\" are most generally referred to as model *parameters* these days, in case you have encountered that term. The term *weights* is reserved for a particular type of model parameter.)"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Arthur Samuel further mentions an \u201c automatic means of testing the effectiveness of any current weight assignment", 
                    "scoring_type": "simple",
                    "context": [
                        "Next, Samuel said we need an *automatic means of testing the effectiveness of any current weight assignment in terms of actual performance*. In the case of his checkers program, the \"actual performance\" of a model would be how well it plays. And you could automatically test the performance of two models by setting them to play against each other, and seeing which one usually wins."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "true"
                },
                {
                    "answer_component": "\u201d and a \u201c mechanism for altering the weight assignment so as to maximize the performance \u201d.", 
                    "scoring_type": "simple",
                    "context": [
                        "Finally, he says we need *a mechanism for altering the weight assignment so as to maximize the performance*. For instance, we could look at the difference in weights between the winning model and the losing model, and adjust the weights a little further in the winning direction."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "true"
                },
                {
                    "answer_component": "This refers to the evaluation and training of the model in order to obtain a set of parameter values that maximizes model performance.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "We can now see why he said that such a procedure *could be made entirely automatic and... a machine so programmed would \"learn\" from its experience*. Learning would become entirely automatic when the adjustment of the weights was also automatic\u2014when instead of us improving a model by adjusting its weights manually, we relied on an automated mechanism that produced adjustments based on performance."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "true"
                }
            ],
            "question_context": [
                {
                    "question_component": "\"What did Samuel mean by \"weight assignment\"?\"", 
                    "context": [
                        "First, we need to understand what Samuel means by a *weight assignment*."
                    ]
                }
            ]
        },
        {
            "chapter": 1,
            "question_number": 12,
            "question_text": "\"What term do we normally use in deep learning for what Samuel called \"weights\"?\"",
            "gold_standard_answer": "\"We instead use the term parameters. In deep learning, the term \u201cweights\u201d has a separate meaning. (The neural network has various parameters that we fit our data to. As shown in upcoming chapters, the two types of neural network parameters are weights and biases)\"",
            "answer_context": [
                {
                    "answer_component": "We instead use the term parameters.", 
                    "scoring_type": "simple",
                    "context": [
                        "By the way, what Samuel called \"weights\" are most generally referred to as model *parameters* these days",

                        "The *weights* are called *parameters*"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "In deep learning, the term \u201cweights\u201d has a separate meaning.", 
                    "scoring_type": "simple",
                    "context": [
                        "The term *weights* is reserved for a particular type of model parameter"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "The neural network has various parameters that we fit our data to", 
                    "scoring_type": "simple",
                    "context": [
                        "This is the key to deep learning\u2014determining how to fit the parameters of a model to get it to solve your problem",

                        "The fact that neural networks are so flexible means that, in practice, they are often a suitable kind of model, and you can focus your effort on the process of training them\u2014that is, of finding good weight assignments",

                        "update the weights of a neural network, to make it improve at any given task",

                        "which does indeed fit a model (i.e. look at images in the training set multiple times, each time updating the parameters to make the predictions closer and closer to the target labels)",

                        "Fit | Update the parameters of the model such that the predictions of the model using the input data match the target labels",

                        "The process of *training* (or *fitting*) the model is the process of finding a set of *parameter values* (or *weights*) that specialize that general architecture into a model that works well for our particular kind of data"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "true"
                },
                {
                    "answer_component": "As shown in upcoming chapters, the two types of neural network parameters are weights", 
                    "scoring_type": "simple",
                    "context": [
                        "The term *weights* is reserved for a particular type of model parameter"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "true"
                },
                {
                    "answer_component": "and biases", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "true"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 14,
            "question_text": "\"Why is it hard to understand why a deep learning model makes a particular prediction?\"",
            "gold_standard_answer": "\"This is a highly-researched topic known as interpretability of deep learning models. Deep learning models are hard to understand in part due to their \u201cdeep\u201d nature. Think of a linear regression model. Simply, we have some input variables/data that are multiplied by some weights, giving us an output. We can understand which variables are more important and which are less important based on their weights. A similar logic might apply for a small neural network with 1-3 layers. However, deep neural networks have hundreds, if not thousands, of layers. It is hard to determine which factors are important in determining the final output. The neurons in the network interact with each other, with the outputs of some neurons feeding into other neurons. Altogether, due to the complex nature of deep learning models, it is very difficult to understand why a neural network makes a given prediction.\n\nHowever, in some cases, recent research has made it easier to better understand a neural network\u2019s prediction. For example, as shown in this chapter, we can analyze the sets of weights and determine what kind of features activate the neurons. When applying CNNs to images, we can also see which parts of the images highly activate the model. We will see how we can make our models interpretable later in the book.\"",

            "answer_context": [
                {
                    "answer_component": "This is a highly-researched topic", 
                    "scoring_type": "simple",
                    "context": [
                        "There is a vast body of research showing how to deeply inspect deep learning models, and get rich insights from them."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": [
                        "Deep learning models are hard to understand",
                        "Altogether, due to the complex nature of deep learning models, it is very difficult to understand why a neural network makes a given prediction."
                    ], 
                    "scoring_type": "simple",
                    "context": [
                        "Having said that, all kinds of machine learning models (including deep learning, and traditional statistical models) can be challenging to fully understand"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "However, deep neural networks have hundreds, if not thousands, of layers.", 
                    "scoring_type": "simple",
                    "context": [
                        "Networks developed since then can have hundreds of layers"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "However, in some cases, recent research has made it easier to better understand a neural network\u2019s prediction. For example, as shown in this chapter, we can analyze the sets of weights and determine what kind of features activate the neurons. When applying CNNs to images, we can also see which parts of the images highly activate the model. We will see how we can make our models interpretable later in the book.", 
                    "scoring_type": "simple",
                    "context": [
                        "In 2013 a PhD student, Matt Zeiler, and his supervisor, Rob Fergus, published the paper [\"Visualizing and Understanding Convolutional Networks\"](https://arxiv.org/pdf/1311.2901.pdf), which showed how to visualize the neural network weights learned in each layer of a model. They carefully analyzed the model that won the 2012 ImageNet competition, and used this analysis to greatly improve the model, such that they were able to go on to win the 2013 competition! <<img_layer1>> is the picture that they published of the first layer's weights."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "known as interpretability of deep learning models.", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "in part due to their \u201cdeep\u201d nature", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Think of a linear regression model. Simply, we have some input variables/data that are multiplied by some weights, giving us an output. We can understand which variables are more important and which are less important based on their weights. A similar logic might apply for a small neural network with 1-3 layers.", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "true"
                },
                {
                    "answer_component": "It is hard to determine which factors are important in determining the final output. The neurons in the network interact with each other, with the outputs of some neurons feeding into other neurons.", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                }                
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 15,
            "question_text": "\"What is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy?\"",
            "gold_standard_answer": "\"The universal approximation theorem states that neural networks can theoretically represent any mathematical function. However, it is important to realize that practically, due to the limits of available data and computer hardware, it is impossible to practically train a model to do so. But we can get very close!\"",
            "answer_context": [
                {
                    "answer_component": "The universal approximation theorem", 
                    "scoring_type": "simple",
                    "context": [
                        "A mathematical proof called the *universal approximation theorem* shows that this function can solve any problem to any level of accuracy, in theory."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "neural networks can theoretically represent any mathematical function.", 
                    "scoring_type": "simple",
                    "context": [
                        "What we would like is some kind of function that is so flexible that it could be used to solve any given problem, just by varying its weights. Amazingly enough, this function actually exists! It's the neural network, which we already discussed. That is, if you regard a neural network as a mathematical function, it turns out to be a function which is extremely flexible depending on its weights."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "However, it is important to realize that practically, due to the limits of available data and computer hardware, it is impossible to practically train a model to do so. But we can get very close!", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 16,
            "question_text": "\"What do you need in order to train a model?\"",
            "gold_standard_answer": "\"You will need an architecture for the given problem. You will need data to input to your model. For most use-cases of deep learning, you will need labels for your data to compare your model predictions to. You will need a loss function that will quantitatively measure the performance of your model. And you need a way to update the parameters of the model in order to improve its performance (this is known as an optimizer).\"",
            "answer_context": [
                {
                    "answer_component": "You will need an architecture for the given problem.", 
                    "scoring_type": "simple",
                    "context": [
                        "Every model starts with a choice of *architecture*, a general template for how that kind of model works internally."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "You will need data to input to your model.", 
                    "scoring_type": "simple",
                    "context": [
                        "A model cannot be created without data."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "For most use-cases of deep learning, you will need labels for your data to compare your model predictions to.", 
                    "scoring_type": "simple",
                    "context": [
                        "It's not enough to just have examples of input data; we need *labels* for that data too (e.g., pictures of dogs and cats aren't enough to train a model; we need a label for each one, saying which ones are dogs, and which are cats)."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "You will need a loss function that will quantitatively measure the performance of your model.", 
                    "scoring_type": "simple",
                    "context": [
                        "In order to define how well a model does on a single prediction, we need to define a *loss function*, which determines how we score a prediction as good or bad."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "And you need a way to update the parameters of the model in order to improve its performance (this is known as an optimizer)", 
                    "scoring_type": "simple",
                    "context": [
                        "provide a mechanism for altering the weight assignment so as to maximize the performance",

                        "The need for a \"mechanism\" (i.e., another automatic process) for improving the performance by changing the weight assignments"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 17,
            "question_text": "\"How could a feedback loop impact the rollout of a predictive policing model?\"",
            "gold_standard_answer": "\"In a predictive policing model, we might end up with a positive feedback loop, leading to a highly biased model with little predictive power. For example, we may want a model that would predict crimes, but we use information on arrests as a proxy . However, this data itself is slightly biased due to the biases in existing policing processes. Training with this data leads to a biased model. Law enforcement might use the model to determine where to focus police activity, increasing arrests in those areas. These additional arrests would be used in training future iterations of models, leading to an even more biased model. This cycle continues as a positive feedback loop\"",
            "answer_context": [
                {
                    "answer_component": "For example, we may want a model that would predict crimes, but we use information on arrests as a proxy . However, this data itself is slightly biased due to the biases in existing policing processes.", 
                    "scoring_type": "simple",
                    "context": [
                        "A *predictive policing* model is created based on where arrests have been made in the past. In practice, this is not actually predicting crime, but rather predicting arrests, and is therefore partially simply reflecting biases in existing policing processes"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Law enforcement might use the model to determine where to focus police activity, increasing arrests in those areas. These additional arrests would be used in training future iterations of models, leading to an even more biased model.", 
                    "scoring_type": "simple",
                    "context": [
                        "Law enforcement officers then might use that model to decide where to focus their police activity, resulting in increased arrests in those areas.\n- Data on these additional arrests would then be fed back in to retrain future versions of the model."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": [
                        "This cycle continues as a positive feedback loop",
                        "In a predictive policing model, we might end up with a positive feedback loop, leading to a highly biased model with little predictive power"
                        ], 
                    "scoring_type": "simple",
                    "context": [
                        "This is a *positive feedback loop*, where the more the model is used, the more biased the data becomes, making the model even more biased, and so forth."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 18,
            "question_text": "\"Do we always have to use 224\u00d7224-pixel images with the cat recognition model?\"",
            "gold_standard_answer": "\"No we do not. 224x224 is commonly used for historical reasons. You can increase the size and get better performance, but at the price of speed and memory consumption.\"",
            "answer_context": [
                {
                    "answer_component": "\"No we do not. 224x224 is commonly used for historical reasons. You can increase the size and get better performance, but at the price of speed and memory consumption.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Why 224 pixels? This is the standard size for historical reasons (old pretrained models require this size exactly), but you can pass pretty much anything. If you increase the size, you'll often get a model with better results (since it will be able to focus on more details), but at the price of speed and memory consumption; the opposite is true if you decrease the size."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 19,
            "question_text": "\"What is the difference between classification and regression?\"",
            "gold_standard_answer": "\"Classification is focused on predicting a class or category (ex: type of pet). Regression is focused on predicting a numeric quantity (ex: age of pet).\"",
            "answer_context": [
                {
                    "answer_component": "Classification is focused on predicting a class or category (ex: type of pet)", 
                    "scoring_type": "simple",
                    "context": [
                        "A classification model is one which attempts to predict a class, or category. That is, it's predicting from a number of discrete possibilities, such as \"dog\" or \"cat.\""
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Regression is focused on predicting a numeric quantity (ex: age of pet)", 
                    "scoring_type": "simple",
                    "context": [
                        "A regression model is one which attempts to predict one or more numeric quantities, such as a temperature or a location."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 20,
            "question_text": "\"What is a validation set? What is a test set? Why do we need them?\"",
            "gold_standard_answer": "\"The validation set is the portion of the dataset that is not used for training the model, but for evaluating the model during training, in order to prevent overfitting. This ensures that the model performance is not due to \u201ccheating\u201d or memorization of the dataset, but rather because it learns the appropriate features to use for prediction. However, it is possible that we overfit the validation data as well. This is because the human modeler is also part of the training process, adjusting hyperparameters (see question 32 for definition) and training procedures according to the validation performance. Therefore, another unseen portion of the dataset, the test set, is used for final evaluation of the model. This splitting of the dataset is necessary to ensure that the model generalizes to unseen data.\"",
            "answer_context": [
                {
                    "answer_component": "The validation set is the portion of the dataset that is not used for training the model, but for evaluating the model during training", 
                    "scoring_type": "simple",
                    "context": [
                        "To avoid this, our first step was to split our dataset into two sets: the *training set* (which our model sees in training) and the *validation set*, also known as the *development set* (which is used only for evaluation). This lets us test that the model learns lessons from the training data that generalize to new data, the validation data."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": [
                        "This ensures that the model performance is not due to \u201ccheating\u201d or memorization of the dataset, but rather because it learns the appropriate features to use for prediction",

                        "overfitting"
                    ], 
                    "scoring_type": "simple",
                    "context": [
                        "One way to understand this situation is that, in a sense, we don't want our model to get good results by \"cheating.\" If it makes an accurate prediction for a data item, that should be because it has learned characteristics of that kind of item, and not because the model has been shaped by *actually having seen that particular item*.",

                        "Even when your model has not fully memorized all your data, earlier on in training it may have memorized certain parts of it. As a result, the longer you train for, the better your accuracy will get on the training set; the validation set accuracy will also improve for a while, but eventually it will start getting worse as the model starts to memorize the training set, rather than finding generalizable underlying patterns in the data. When this happens, we say that the model is *overfitting*."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "However, it is possible that we overfit the validation data as well.", 
                    "scoring_type": "simple",
                    "context": [
                        "The problem is that even though the ordinary training process is only looking at predictions on the training data when it learns values for the weight parameters, the same is not true of us. We, as modelers, are evaluating the model by looking at predictions on the validation data when we decide to explore new hyperparameter values! So subsequent versions of the model are, indirectly, shaped by us having seen the validation data. Just as the automatic training process is in danger of overfitting the training data, we are in danger of overfitting the validation data through human trial and error and exploration."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "This is because the human modeler is also part of the training process, adjusting hyperparameters (see question 32 for definition) and training procedures according to the validation performance.", 
                    "scoring_type": "simple",
                    "context": [
                        "In fact, not necessarily. The situation is more subtle. This is because in realistic scenarios we rarely build a model just by training its weight parameters once. Instead, we are likely to explore many versions of a model through various modeling choices regarding network architecture, learning rates, data augmentation strategies, and other factors we will discuss in upcoming chapters. Many of these choices can be described as choices of *hyperparameters*. The word reflects that they are parameters about parameters, since they are the higher-level choices that govern the meaning of the weight parameters."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Therefore, another unseen portion of the dataset, the test set, is used for final evaluation of the model.", 
                    "scoring_type": "simple",
                    "context": [
                        "The solution to this conundrum is to introduce another level of even more highly reserved data, the *test set*. Just as we hold back the validation data from the training process, we must hold back the test set data even from ourselves. It cannot be used to improve the model; it can only be used to evaluate the model at the very end of our efforts. In effect, we define a hierarchy of cuts of our data, based on how fully we want to hide it from training and modeling processes: training data is fully exposed, the validation data is less exposed, and test data is totally hidden. This hierarchy parallels the different kinds of modeling and evaluation processes themselves\u2014the automatic training process with back propagation, the more manual process of trying different hyper-parameters between training sessions, and the assessment of our final result."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "This splitting of the dataset is necessary to ensure that the model generalizes to unseen data.", 
                    "scoring_type": "simple",
                    "context": [
                        "Having two levels of \"reserved data\"\u2014a validation set and a test set, with one level representing data that you are virtually hiding from yourself\u2014may seem a bit extreme. But the reason it is often necessary is because models tend to gravitate toward the simplest way to do good predictions (memorization), and we as fallible humans tend to gravitate toward fooling ourselves about how well our models are performing. The discipline of the test set helps us keep ourselves intellectually honest. That doesn't mean we *always* need a separate test set\u2014if you have very little data, you may need to just have a validation set\u2014but generally it's best to use one if at all possible."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 21,
            "question_text": "\"What will fastai do if you don't provide a validation set?\"",
            "gold_standard_answer": "\"fastai will automatically create a validation dataset. It will randomly take 20% of the data and assign it as the validation set ( valid_pct = 0.2 ).\"",
            "answer_context": [
                {
                    "answer_component": "fastai will automatically create a validation dataset.", 
                    "scoring_type": "simple",
                    "context": [
                        "so even if you forget, fastai will create a validation set for you!"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "It will randomly take 20% of the data and assign it as the validation set ( valid_pct = 0.2 ).", 
                    "scoring_type": "simple",
                    "context": [
                        "The most important parameter to mention here is `valid_pct=0.2`. This tells fastai to hold out 20% of the data and *not use it for training the model at all*. This 20% of the data is called the *validation set*; the remaining 80% is called the *training set*. The validation set is used to measure the accuracy of the model. By default, the 20% that is held out is selected randomly. The parameter `seed=42` sets the *random seed* to the same value every time we run this code, which means we get the same validation set every time we run it\u2014this way, if we change our model and retrain it, we know that any differences are due to the changes to the model, not due to having a different random validation set."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 22,
            "question_text": "\"Can we always use a random sample for a validation set? Why or why not?\"",
            "gold_standard_answer": "\"A good validation or test set should be representative of new data you will see in the future. Sometimes this isn\u2019t true if a random sample is used. For example, for time series data, selecting sets randomly does not make sense. Instead, defining different time periods for the train, validation, and test set is a better approach.\"",
            "answer_context": [
                {
                    "answer_component": "A good validation or test set should be representative of new data you will see in the future. Sometimes this isn\u2019t true if a random sample is used", 
                    "scoring_type": "simple",
                    "context": [
                        "To do a good job of defining a validation set (and possibly a test set), you will sometimes want to do more than just randomly grab a fraction of your original dataset. Remember: a key property of the validation and test sets is that they must be representative of the new data you will see in the future. This may sound like an impossible order! By definition, you haven\u2019t seen this data yet. But you usually still do know some things."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "For example, for time series data, selecting sets randomly does not make sense. Instead, defining different time periods for the train, validation, and test set is a better approach.", 
                    "scoring_type": "simple",
                    "context": [
                        "One case might be if you are looking at time series data. For a time series, choosing a random subset of the data will be both too easy (you can look at the data both before and after the dates you are trying to predict) and not representative of most business use cases (where you are using historical data to build a model for use in the future). If your data includes the date and you are building a model to use in the future, you will want to choose a continuous section with the latest dates as your validation set (for instance, the last two weeks or last month of available data)."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 23,
            "question_text": "\"What is overfitting? Provide an example.\"",
            "gold_standard_answer": "\"Overfitting is the most challenging issue when it comes to training machine learning models. Overfitting refers to when the model fits too closely to a limited set of data but does not generalize well to unseen data. This is especially important when it comes to neural networks, because neural networks can potentially \u201cmemorize\u201d the dataset that the model was trained on, and will perform abysmally on unseen data because it didn\u2019t \u201cmemorize\u201d the ground truth values for that data. This is why a proper validation framework is needed by splitting the data into training, validation, and test sets.\"",
            "answer_context": [
                {
                    "answer_component": "Overfitting is the most challenging issue when it comes to training machine learning models", 
                    "scoring_type": "simple",
                    "context": [
                        "**Overfitting is the single most important and challenging issue** when training for all machine learning practitioners, and all algorithms"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Overfitting refers to when the model fits too closely to a limited set of data but does not generalize well to unseen data", 
                    "scoring_type": "simple",
                    "context": [
                        "Even when your model has not fully memorized all your data, earlier on in training it may have memorized certain parts of it. As a result, the longer you train for, the better your accuracy will get on the training set; the validation set accuracy will also improve for a while, but eventually it will start getting worse as the model starts to memorize the training set, rather than finding generalizable underlying patterns in the data. When this happens, we say that the model is *overfitting*.",

                        "measure the accuracy of your model only on the validation set. If you train for too long, with not enough data, you will see the accuracy of your model start to get worse; this is called _overfitting_.",

                        "Models using architectures with more layers take longer to train, and are more prone to overfitting (i.e. you can't train them for as many epochs before the accuracy on the validation set starts getting worse).",

                        "|Overfitting | Training a model in such a way that it _remembers_ specific features of the input data, rather than generalizing well to data not seen during training",

                        "The risk is that if we train our model badly, instead of learning general lessons it effectively memorizes what it has already seen, and then it will make poor predictions about new images. Such a failure is called *overfitting*."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "This is especially important when it comes to neural networks, because neural networks can potentially \u201cmemorize\u201d the dataset that the model was trained on, and will perform abysmally on unseen data because it didn\u2019t \u201cmemorize\u201d the ground truth values for that data", 
                    "scoring_type": "simple",
                    "context": [
                        "if you train a large enough model for a long enough time, it will eventually memorize the label of every item in your dataset! The result will not actually be a useful model, because what we care about is how well our model works on *previously unseen images*."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "This is why a proper validation framework is needed by splitting the data into training, validation, and test sets.", 
                    "scoring_type": "simple",
                    "context": [
                        "The solution to this conundrum is to introduce another level of even more highly reserved data, the *test set*. Just as we hold back the validation data from the training process, we must hold back the test set data even from ourselves. It cannot be used to improve the model; it can only be used to evaluate the model at the very end of our efforts. In effect, we define a hierarchy of cuts of our data, based on how fully we want to hide it from training and modeling processes: training data is fully exposed, the validation data is less exposed, and test data is totally hidden. This hierarchy parallels the different kinds of modeling and evaluation processes themselves—the automatic training process with back propagation, the more manual process of trying different hyper-parameters between training sessions, and the assessment of our final result."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 24,
            "question_text": "\"What is a metric? How does it differ from \"loss\"?\"",
            "gold_standard_answer": "\"A metric is a function that measures quality of the model\u2019s predictions using the validation set. This is similar to the \u00ad loss , which is also a measure of performance of the model. However, loss is meant for the optimization algorithm (like SGD) to efficiently update the model parameters, while metrics are human-interpretable measures of performance. Sometimes, a metric may also be a good choice for the loss.\"",
            "answer_context": [
                {
                    "answer_component": "A metric is a function that measures quality of the model\u2019s predictions using the validation set", 
                    "scoring_type": "simple",
                    "context": [
                        "What is a metric? A *metric* is a function that measures the quality of the model's predictions using the validation set",

                        "A measurement of how good the model is, using the validation set, chosen for human consumption"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "loss is meant for the optimization algorithm (like SGD) to efficiently update the model parameters, while metrics are human-interpretable measures of performance", 
                    "scoring_type": "simple",
                    "context": [
                        "The entire purpose of loss is to define a \"measure of performance\" that the training system can use to update weights automatically. In other words, a good choice for loss is a choice that is easy for stochastic gradient descent to use. But a metric is defined for human consumption, so a good metric is one that is easy for you to understand, and that hews as closely as possible to what you want the model to do"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Sometimes, a metric may also be a good choice for the loss", 
                    "scoring_type": "simple",
                    "context": [
                        "At times, you might decide that the loss function is a suitable metric, but that is not necessarily the case"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 25,
            "question_text": "\"How can pretrained models help?\"",
            "gold_standard_answer": "\"Pretrained models have been trained on other problems that may be quite similar to the current task. For example, pretrained image recognition models were often trained on the ImageNet dataset, which has 1000 classes focused on a lot of different types of visual objects. Pretrained models are useful because they have already learned how to handle a lot of simple features like edge and color detection. However, since the model was trained for a different task than already used, this model cannot be used as is.\"",
            "answer_context": [
                {
                    "answer_component": "Pretrained models have been trained on other problems that may be quite similar to the current task.", 
                    "scoring_type": "simple",
                    "context": [
                        "A model that has weights that have already been trained on some other dataset is called a *pretrained model*. You should nearly always use a pretrained model, because it means that your model, before you've even shown it any of your data, is already very capable. And, as you'll see, in a deep learning model many of these capabilities are things you'll need, almost regardless of the details of your project."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "For example, pretrained image recognition models were often trained on the ImageNet dataset, which has 1000 classes focused on a lot of different types of visual objects.", 
                    "scoring_type": "simple",
                    "context": [
                        "`vision_learner` also has a parameter `pretrained`, which defaults to `True` (so it's used in this case, even though we haven't specified it), which sets the weights in your model to values that have already been trained by experts to recognize a thousand different categories across 1.3 million photos (using the famous [*ImageNet* dataset](http://www.image-net.org/))."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Pretrained models are useful because they have already learned how to handle a lot of simple features like edge and color detection.", 
                    "scoring_type": "simple",
                    "context": [
                        "And, as you'll see, in a deep learning model many of these capabilities are things you'll need, almost regardless of the details of your project. For instance, parts of pretrained models will handle edge, gradient, and color detection, which are needed for many tasks."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "However, since the model was trained for a different task than already used, this model cannot be used as is.", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 26,
            "question_text": "\"What is the \"head\" of a model?\"",
            "gold_standard_answer": "\"When using a pretrained model, the later layers of the model, which were useful for the task that the model was originally trained on, are replaced with one or more new layers with randomized weights, of an appropriate size for the dataset you are working with. These new layers are called the \u201chead\u201d of the model.\"",
            "answer_context": [
                {
                    "answer_component": "\"When using a pretrained model, the later layers of the model, which were useful for the task that the model was originally trained on, are replaced with one or more new layers with randomized weights, of an appropriate size for the dataset you are working with. These new layers are called the \u201chead\u201d of the model.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "When using a pretrained model, `vision_learner` will remove the last layer, since that is always specifically customized to the original training task (i.e. ImageNet dataset classification), and replace it with one or more new layers with randomized weights, of an appropriate size for the dataset you are working with. This last part of the model is known as the *head*."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 27,
            "question_text": "\"What kinds of features do the early layers of a CNN find? How about the later layers?\"",
            "gold_standard_answer": "\"Earlier layers learn simple features like diagonal, horizontal, and vertical edges. Later layers learn more advanced features like car wheels, flower petals, and even outlines of animals.\"",
            "answer_context": [
                {
                    "answer_component": "Earlier layers learn simple features like diagonal, horizontal, and vertical edges", 
                    "scoring_type": "simple",
                    "context": [
                        "For layer 1, what we can see is that the model has discovered weights that represent diagonal, horizontal, and vertical edges, as well as various different gradients"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Later layers learn more advanced features like car wheels, flower petals, and even outlines of animals", 
                    "scoring_type": "simple",
                    "context": [
                        "the features are now able to identify and match with higher-level semantic components, such as car wheels, text, and flower petals. Using these components, layers four and five can identify even higher-level concepts"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 28,
            "question_text": "\"Are image models only useful for photos?\"",
            "gold_standard_answer": "\"Nope! Image models can be useful for other types of images like sketches, medical data, etc.\n\nHowever, a lot of information can be represented as images . For example, a sound can be converted into a spectrogram, which is a visual interpretation of the audio. Time series (ex: financial data) can be converted to image by plotting on a graph. Even better, there are various transformations that generate images from time series, and have achieved good results for time series classification. There are many other examples, and by being creative, it may be possible to formulate your problem as an image classification problem, and use pretrained image models to obtain state-of-the-art results!\"",
            "answer_context": [
                {
                    "answer_component": [
                        "Image models can be useful for other types of images like sketches, medical data, etc.\n\nHowever, a lot of information can be represented as images",

                        "There are many other examples, and by being creative, it may be possible to formulate your problem as an image classification problem"
                    ], 
                    "scoring_type": "simple",
                    "context": [
                        "An image recognizer can, as its name suggests, only recognize images. But a lot of things can be represented as images, which means that an image recogniser can learn to complete many tasks."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "For example, a sound can be converted into a spectrogram, which is a visual interpretation of the audio", 
                    "scoring_type": "simple",
                    "context": [
                        "For instance, a sound can be converted to a spectrogram, which is a chart that shows the amount of each frequency at each time in an audio file"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Time series (ex: financial data) can be converted to image by plotting on a graph. Even better, there are various transformations that generate images from time series, and have achieved good results for time series classification", 
                    "scoring_type": "simple",
                    "context": [
                        "A time series can easily be converted into an image by simply plotting the time series on a graph. However, it is often a good idea to try to represent your data in a way that makes it as easy as possible to pull out the most important components. In a time series, things like seasonality and anomalies are most likely to be of interest. There are various transformations available for time series data. For instance, fast.ai student Ignacio Oguiza created images from a time series dataset for olive oil classification, using a technique called Gramian Angular Difference Field (GADF); you can see the result in <<ts_image>>. He then fed those images to an image classification model just like the one you see in this chapter. His results, despite having only 30 training set images, were well over 90% accurate, and close to the state of the art."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "use pretrained image models to obtain state-of-the-art results!", 
                    "scoring_type": "simple",
                    "context": [
                        "For instance, fast.ai student Ignacio Oguiza created images from a time series dataset for olive oil classification, using a technique called Gramian Angular Difference Field (GADF); you can see the result in <<ts_image>>. He then fed those images to an image classification model just like the one you see in this chapter. His results, despite having only 30 training set images, were well over 90% accurate, and close to the state of the art."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 29,
            "question_text": "\"What is an \"architecture\"?\"",
            "gold_standard_answer": "\"The architecture is the template or structure of the model we are trying to fit. It defines the mathematical model we are trying to fit.\"",
            "answer_context": [
                {
                    "answer_component": "\"The architecture is the template or structure of the model we are trying to fit\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Every model starts with a choice of *architecture*, a general template for how that kind of model works internally",

                        "The _template_ of the model that we're trying to fit"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"It defines the mathematical model we are trying to fit.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "The functional form of the *model* is called its *architecture*",

                        "As we've discussed, the architecture only describes a *template* for a mathematical function",

                        "the actual mathematical function that we're passing the input data and parameters to"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 30,
            "question_text": "\"What is segmentation?\"",
            "gold_standard_answer": "\"At its core, segmentation is a pixelwise classification problem. We attempt to predict a label for every single pixel in the image. This provides a mask for which parts of the image correspond to the given label.\"",
            "answer_context": [
                {
                    "answer_component": "\"At its core, segmentation is a pixelwise classification problem. We attempt to predict a label for every single pixel in the image.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "For instance, let's talk about something that is critically important for autonomous vehicles: localizing objects in a picture. If a self-driving car doesn't know where a pedestrian is, then it doesn't know how to avoid one! Creating a model that can recognize the content of every individual pixel in an image is called *segmentation*. Here is how we can train a segmentation model with fastai, using a subset of the [*Camvid* dataset](http://www0.cs.ucl.ac.uk/staff/G.Brostow/papers/Brostow_2009-PRL.pdf) from the paper \"Semantic Object Classes in Video: A High-Definition Ground Truth Database\" by Gabruel J. Brostow, Julien Fauqueur, and Roberto Cipolla:"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "This provides a mask for which parts of the image correspond to the given label", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 31,
            "question_text": "\"What is y_range used for? When do we need it?\"",
            "gold_standard_answer": "\"y_range is being used to limit the values predicted when our problem is focused on predicting a numeric value in a given range (ex: predicting movie ratings, range of 0.5-5).\"",
            "answer_context": [
                {
                    "answer_component": "\"y_range is being used to limit the values predicted when our problem is focused on predicting a numeric value in a given range (ex: predicting movie ratings, range of 0.5-5).\"", 
                    "scoring_type": "simple",
                    "context": [
                        "This model is predicting movie ratings on a scale of 0.5 to 5.0 to within around 0.6 average error. Since we're predicting a continuous number, rather than a category, we have to tell fastai what range our target has, using the `y_range` parameter."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 32,
            "question_text": "\"What are \"hyperparameters\"?\"",
            "gold_standard_answer": "\"Training models requires various other parameters that define how the model is trained. For example, we need to define how long we train for, or what learning rate (how fast the model parameters are allowed to change) is used. These sorts of parameters are hyperparameters.\"",
            "answer_context": [
                {
                    "answer_component": "\"Training models requires various other parameters that define how the model is trained. For example, we need to define how long we train for, or what learning rate (how fast the model parameters are allowed to change) is used. These sorts of parameters are hyperparameters.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "we are likely to explore many versions of a model through various modeling choices regarding network architecture, learning rates, data augmentation strategies, and other factors we will discuss in upcoming chapters. Many of these choices can be described as choices of *hyperparameters*. The word reflects that they are parameters about parameters, since they are the higher-level choices that govern the meaning of the weight parameters."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 1,
            "question_number": 33,
            "question_text": "\"What's the best way to avoid failures when using AI in an organization?\"",
            "gold_standard_answer": "\"Key things to consider when using AI in an organization:\n\nMake sure a training, validation, and testing set is defined properly in order to evaluate the model in an appropriate manner.\nTry out a simple baseline, which future models should hopefully beat. Or even this simple baseline may be enough in some cases.\"",
            "answer_context": [
                {
                    "answer_component": "\"Key things to consider when using AI in an organization:\n\nMake sure a training, validation, and testing set is defined properly in order to evaluate the model in an appropriate manner.\nTry out a simple baseline, which future models should hopefully beat. Or even this simple baseline may be enough in some cases.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "if you ensure that you really understand what test and validation sets are and why they're important, then you'll avoid the single biggest source of failures we've seen when organizations decide to use AI. For instance, if you're considering bringing in an external vendor or service, make sure that you hold out some test data that the vendor *never gets to see*. Then *you* check their model on your test data, using a metric that *you* choose based on what actually matters to you in practice, and *you* decide what level of performance is adequate. (It's also a good idea for you to try out some simple baseline yourself, so you know what a really simple model can achieve. Often it'll turn out that your simple model performs just as well as one produced by an external \"expert\"!)"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "in an organization?", 
                    "context": [
                        "To put it bluntly, if you're a senior decision maker in your organization (or you're advising senior decision makers), the most important takeaway is this:"
                    ]
                }
            ]
        },
        {
            "chapter": 2,
            "question_number": 1,
            "question_text": "\"Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.\"",
            "gold_standard_answer": "\"Working with video data instead of images\nHandling nighttime images, which may not appear in this dataset\nDealing with low-resolution camera images\nEnsuring results are returned fast enough to be useful in practice\nRecognizing bears in positions that are rarely seen in photos that people post online (for example from behind, partially covered by bushes, or when a long way away from the camera)\"",
            "answer_context": [
                {
                    "answer_component": "\"Working with video data instead of images\nHandling nighttime images, which may not appear in this dataset\nDealing with low-resolution camera images\nEnsuring results are returned fast enough to be useful in practice\nRecognizing bears in positions that are rarely seen in photos that people post online (for example from behind, partially covered by bushes, or when a long way away from the camera)\"", 
                    "scoring_type": "simple",
                    "context": [
                        "- Working with video data instead of images\n- Handling nighttime images, which may not appear in this dataset\n- Dealing with low-resolution camera images\n- Ensuring results are returned fast enough to be useful in practice\n- Recognizing bears in positions that are rarely seen in photos that people post online (for example from behind, partially covered by bushes, or when a long way away from the camera)"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "\"Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.\"", 
                    "context": [
                        "This can result in disaster! For instance, let's say we really were rolling out a bear detection system that will be attached to video cameras around campsites in national parks, and will warn campers of incoming bears. If we used a model trained with the dataset we downloaded there would be all kinds of problems in practice, such as:"
                    ]
                }
            ]
        },
        {
            "chapter": 2,
            "question_number": 2,
            "question_text": "\"Where do text models currently have a major deficiency?\"",
            "gold_standard_answer": "\"Text models can generate context-appropriate text (like replies or imitating author style). However, text models still struggle with correct responses. Given factual information (such as a knowledge base), it is still hard to generate responses that utilizes this information to generate factually correct responses, though the text can seem very compelling. This can be very dangerous, as the layman may not be able to evaluate the factual accuracy of the generated text.\"",
            "answer_context": [
                {
                    "answer_component": "\"Text models can generate context-appropriate text (like replies or imitating author style). However, text models still struggle with correct responses. Given factual information (such as a knowledge base), it is still hard to generate responses that utilizes this information to generate factually correct responses, though the text can seem very compelling. This can be very dangerous, as the layman may not be able to evaluate the factual accuracy of the generated text.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Deep learning is also very good at generating context-appropriate text, such as replies to social media posts, and imitating a particular author's style. It's good at making this content compelling to humans too\u2014in fact, even more compelling than human-generated text. However, deep learning is currently not good at generating *correct* responses! We don't currently have a reliable way to, for instance, combine a knowledge base of medical information with a deep learning model for generating medically correct natural language responses. This is very dangerous, because it is so easy to create content that appears to a layman to be compelling, but actually is entirely incorrect."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 2,
            "question_number": 3,
            "question_text": "\"What are possible negative societal implications of text generation models?\"",
            "gold_standard_answer": "\"The ability for text generation models to generate context-aware, highly compelling responses can be used at a massive scale to spread disinformation (\u201cfake news\u201d) and encourage conflict.\n\nModels reinforce bias (like gender bias, racial bias) in training data and create a vicious cycle of biased outputs.\"",
            "answer_context": [
                {
                    "answer_component": "The ability for text generation models to generate context-aware, highly compelling responses can be used at a massive scale to spread disinformation (\u201cfake news\u201d) and encourage conflict", 
                    "scoring_type": "simple",
                    "context": [
                        "Another concern is that context-appropriate, highly compelling responses on social media could be used at massive scale\u2014thousands of times greater than any troll farm previously seen\u2014to spread disinformation, create unrest, and encourage conflict."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Models reinforce bias (like gender bias, racial bias) in training data and create a vicious cycle of biased outputs.", 
                    "scoring_type": "simple",
                    "context": [
                        "Part of the issue in this case is that in the presence of bias (which we'll discuss in depth in the next chapter), *feedback loops* can result in negative implications of that bias getting worse and worse. For instance, there are concerns that this is already happening in the US, where there is significant bias in arrest rates on racial grounds. [According to the ACLU](https://www.aclu.org/issues/smart-justice/sentencing-reform/war-marijuana-black-and-white), \"despite roughly equal usage rates, Blacks are 3.73 times more likely than whites to be arrested for marijuana.\" The impact of this bias, along with the rollout of predictive policing algorithms in many parts of the US, led B\u00e4r\u00ed Williams to [write in the *New York Times*](https://www.nytimes.com/2017/12/02/opinion/sunday/intelligent-policing-and-my-innocent-children.html): \"The same technology that\u2019s the source of so much excitement in my career is being used in law enforcement in ways that could mean that in the coming years, my son, who is 7 now, is more likely to be profiled or arrested\u2014or worse\u2014for no reason other than his race and where we live.\""
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 2,
            "question_number": 4,
            "question_text": "\"In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process?\"",
            "gold_standard_answer": "\"The predictions of the model could be reviewed by human experts for them to evaluate the results and determine what is the best next step. This is especially true for applying machine learning for medical diagnoses. For example, a machine learning model for identifying strokes in CT scans can alert high priority cases for expedited review, while other cases are still sent to radiologists for review. Or other models can also augment the medical professional\u2019s abilities, reducing risk but still improving efficiency of the workflow. For example, deep learning models can provide useful measurements for radiologists or pathologists.\"",
            "answer_context": [
                {
                    "answer_component": "\"The predictions of the model could be reviewed by human experts for them to evaluate the results and determine what is the best next step. This is especially true for applying machine learning for medical diagnoses. For example, a machine learning model for identifying strokes in CT scans can alert high priority cases for expedited review, while other cases are still sent to radiologists for review. Or other models can also augment the medical professional\u2019s abilities, reducing risk but still improving efficiency of the workflow. For example, deep learning models can provide useful measurements for radiologists or pathologists.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Because of this serious issue, we generally recommend that deep learning be used not as an entirely automated process, but as part of a process in which the model and a human user interact closely. This can potentially make humans orders of magnitude more productive than they would be with entirely manual methods, and actually result in more accurate processes than using a human alone. For instance, an automatic system can be used to identify potential stroke victims directly from CT scans, and send a high-priority alert to have those scans looked at quickly. There is only a three-hour window to treat strokes, so this fast feedback loop could save lives. At the same time, however, all scans could continue to be sent to radiologists in the usual way, so there would be no reduction in human input. Other deep learning models could automatically measure items seen on the scans, and insert those measurements into reports, warning the radiologists about findings that they may have missed, and telling them about other cases that might be relevant."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 2,
            "question_number": 5,
            "question_text": "\"What kind of tabular data is deep learning particularly good at?\"",
            "gold_standard_answer": "\"Deep learning is good at analyzing tabular data that includes natural language, or high cardinality categorical columns (containing larger number of discrete choices like zip code).\"",
            "answer_context": [
                {
                    "answer_component": "\"Deep learning is good at analyzing tabular data that includes natural language, or high cardinality categorical columns (containing larger number of discrete choices like zip code).\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Deep learning does greatly increase the variety of columns that you can include\u2014for example, columns containing natural language (book titles, reviews, etc.), and high-cardinality categorical columns (i.e., something that contains a large number of discrete choices, such as zip code or product ID)."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 2,
            "question_number": 6,
            "question_text": "\"What's a key downside of directly using a deep learning model for recommendation systems?\"",
            "gold_standard_answer": "\"Machine learning approaches for recommendation systems will often only tell what products a user might like, and may not be recommendations that would be helpful to the user. For example, if a user is familiar with other books from the same author, it isn\u2019t helpful to recommend those products even though the user bought the author\u2019s book. Or, recommending products a user may have already purchased.\"",
            "answer_context": [
                {
                    "answer_component": "\"Machine learning approaches for recommendation systems will often only tell what products a user might like, and may not be recommendations that would be helpful to the user. For example, if a user is familiar with other books from the same author, it isn\u2019t helpful to recommend those products even though the user bought the author\u2019s book. Or, recommending products a user may have already purchased.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "However, nearly all machine learning approaches have the downside that they only tell you what products a particular user might like, rather than what recommendations would be helpful for a user. Many kinds of recommendations for products a user might like may not be at all helpful\u2014for instance, if the user is already familiar with the products, or if they are simply different packagings of products they have already purchased (such as a boxed set of novels, when they already have each of the items in that set). Jeremy likes reading books by Terry Pratchett, and for a while Amazon was recommending nothing but Terry Pratchett books to him (see <<pratchett>>), which really wasn't helpful because he already was aware of these books!"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 2,
            "question_number": 7,
            "question_text": "\"What are the steps of the Drivetrain Approach?\"",
            "gold_standard_answer": "\"Defined Objective\nLevers\nData\nModels\"",
            "answer_context": [
                {
                    "answer_component": "Objective", 
                    "scoring_type": "simple",
                    "context": [
                        "Start by defining a clear *objective*."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Levers", 
                    "scoring_type": "simple",
                    "context": [
                        "The next step is to consider what *levers* you can pull (i.e., what actions you can take) to better achieve that objective"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Data", 
                    "scoring_type": "simple",
                    "context": [
                        "The third step was to consider what new *data* they would need to produce such a ranking"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Models", 
                    "scoring_type": "simple",
                    "context": [
                        "Only after these first three steps do we begin thinking about building the predictive *models*"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "Drivetrain Approach", 
                    "context": [
                        "We use data not just to generate more data (in the form of predictions), but to produce *actionable outcomes*. That is the goal of the Drivetrain Approach. "
                    ]
                }
            ]
        },
        {
            "chapter": 2,
            "question_number": 8,
            "question_text": "\"How do the steps of the Drivetrain Approach map to a recommendation system?\"",
            "gold_standard_answer": "\"The objective of a recommendation engine is to drive additional sales by surprising and delighting the customer with recommendations of items they would not have purchased without the recommendation. The lever is the ranking of the recommendations. New data must be collected to generate recommendations that will cause new sales . This will require conducting many randomized experiments in order to collect data about a wide range of recommendations for a wide range of customers. This is a step that few organizations take; but without it, you don\u2019t have the information you need to actually optimize recommendations based on your true objective (more sales!)\"",
            "answer_context": [
                {
                    "answer_component": "\"The objective of a recommendation engine is to drive additional sales by surprising and delighting the customer with recommendations of items they would not have purchased without the recommendation. The lever is the ranking of the recommendations. New data must be collected to generate recommendations that will cause new sales . This will require conducting many randomized experiments in order to collect data about a wide range of recommendations for a wide range of customers. This is a step that few organizations take; but without it, you don\u2019t have the information you need to actually optimize recommendations based on your true objective (more sales!)\"", 
                    "scoring_type": "simple",
                    "context": [
                        "The *objective* of a recommendation engine is to drive additional sales by surprising and delighting the customer with recommendations of items they would not have purchased without the recommendation. The *lever* is the ranking of the recommendations. New *data* must be collected to generate recommendations that will *cause new sales*. This will require conducting many randomized experiments in order to collect data about a wide range of recommendations for a wide range of customers. This is a step that few organizations take; but without it, you don't have the information you need to actually optimize recommendations based on your true objective (more sales!)."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "recommendation system", 
                    "context": [
                        "Let's consider another example: recommendation systems. "
                    ]
                }
            ]
        },
        {
            "chapter": 2,
            "question_number": 10,
            "question_text": "\"What is `DataLoaders`?\"",
            "gold_standard_answer": "\"The DataLoaders class is the class that passes the data to the fastai model. It is essentially a class that stores the required Dataloader objects (usually for train and validation sets).\"",
            "answer_context": [
                {
                    "answer_component": "The DataLoaders class is the class that passes the data to the fastai model", 
                    "scoring_type": "simple",
                    "context": [
                        "it provides the data for your model"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "It is essentially a class that stores the required Dataloader objects (usually for train and validation sets)", 
                    "scoring_type": "simple",
                    "context": [
                        "A fastai class that stores multiple `DataLoader` objects you pass to it, normally a `train` and a `valid`, although it's possible to have as many as you like. The first two are made available as properties.",

                        "A `DataLoaders` includes validation and training `DataLoader`s"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "`DataLoaders`", 
                    "context": [
                        "`DataLoaders` is a thin class that just stores whatever `DataLoader` objects you pass to it, and makes them available as `train` and `valid`.",

                        "> jargon: DataLoaders:"
                    ]
                }
            ]
        },
        {
            "chapter": 2,
            "question_number": 11,
            "question_text": "\"What four things do we need to tell fastai to create `DataLoaders`?\"",
            "gold_standard_answer": "\"what kinds of data we are working with\nhow to get the list of items\nhow to label these items\nhow to create the validation set\"",
            "answer_context": [
                {
                    "answer_component": "\"what kinds of data we are working with\nhow to get the list of items\nhow to label these items\nhow to create the validation set\"", 
                    "scoring_type": "simple",
                    "context": [
                        "- What kinds of data we are working with\n- How to get the list of items\n- How to label these items\n- How to create the validation set"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 2,
            "question_number": 12,
            "question_text": "\"What does the `splitter` parameter to `DataBlock` do?\"",
            "gold_standard_answer": "\"In fastai DataBlock, you provide the splitter argument a way for fastai to split up the dataset into subsets (usually train and validation set). For example, to randomly split the data, you can use fastai\u2019s predefined RandomSplitter class, providing it with the proportion of the data used for validation.\"",
            "answer_context": [
                {
                    "answer_component": "\"In fastai DataBlock, you provide the splitter argument a way for fastai to split up the dataset into subsets (usually train and validation set). For example, to randomly split the data, you can use fastai\u2019s predefined RandomSplitter class, providing it with the proportion of the data used for validation.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Often, datasets that you download will already have a validation set defined. Sometimes this is done by placing the images for the training and validation sets into different folders. Sometimes it is done by providing a CSV file in which each filename is listed along with which dataset it should be in. There are many ways that this can be done, and fastai provides a very general approach that allows you to use one of its predefined classes for this, or to write your own"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"In fastai DataBlock, you provide the splitter argument a way for fastai to split up the dataset into subsets (usually train and validation set). For example, to randomly split the data, you can use fastai\u2019s predefined RandomSplitter class, providing it with the proportion of the data used for validation.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "```python\nsplitter=RandomSplitter(valid_pct=0.2, seed=42)\n```"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "to randomly split the data", 
                    "context": [
                        "In this case, however, we simply want to split our training and validation sets randomly. However, we would like to have the same training/validation split each time we run this notebook, so we fix the random seed (computers don't really know how to create random numbers at all, but simply create lists of numbers that look random; if you provide the same starting point for that list each time\u2014called the *seed*\u2014then you will get the exact same list each time):"
                    ]
                }
            ]
        },
        {
            "chapter": 2,
            "question_number": 13,
            "question_text": "\"How do we ensure a random split always gives the same validation set?\"",
            "gold_standard_answer": "\"It turns out it is impossible for our computers to generate truly random numbers. Instead, they use a process known as a pseudo-random generator. However, this process can be controlled using a random seed. By setting a random seed value, the pseudo-random generator will generate the \u201crandom\u201d numbers in a fixed manner and it will be the same for every run. Using a random seed, we can generate a random split that gives the same validation set always.\"",
            "answer_context": [
                {
                    "answer_component": "\"It turns out it is impossible for our computers to generate truly random numbers. Instead, they use a process known as a pseudo-random generator. However, this process can be controlled using a random seed. By setting a random seed value, the pseudo-random generator will generate the \u201crandom\u201d numbers in a fixed manner and it will be the same for every run. Using a random seed, we can generate a random split that gives the same validation set always.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "we fix the random seed (computers don't really know how to create random numbers at all, but simply create lists of numbers that look random; if you provide the same starting point for that list each time\u2014called the *seed*\u2014then you will get the exact same list each time)"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "\"How do we ensure a random split always gives the same validation set?\"", 
                    "context": [
                        "However, we would like to have the same training/validation split each time we run this notebook"
                    ]
                }
            ]
        },
        {
            "chapter": 2,
            "question_number": 14,
            "question_text": "\"What letters are often used to signify the independent and dependent variables?\"",
            "gold_standard_answer": "\"x is independent. y is dependent.\"",
            "answer_context": [
                {
                    "answer_component": "\"x is independent. y is dependent.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "The independent variable is often referred to as `x` and the dependent variable is often referred to as `y`"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 2,
            "question_number": 15,
            "question_text": "\"What's the difference between the crop, pad, and squish resize approaches? When might you choose one over the others?\"",
            "gold_standard_answer": "\"crop is the default Resize() method, and it crops the images to fit a square shape of the size requested, using the full width or height. This can result in losing some important details. For instance, if we were trying to recognize the breed of dog or cat, we may end up cropping out a key part of the body or the face necessary to distinguish between similar breeds.\n\npad is an alternative Resize() method, which pads the matrix of the image\u2019s pixels with zeros (which shows as black when viewing the images). If we pad the images then we have a whole lot of empty space, which is just wasted computation for our model, and results in a lower effective resolution for the part of the image we actually use.\n\nsquish is another alternative Resize() method, which can either squish or stretch the image. This can cause the image to take on an unrealistic shape, leading to a model that learns that things look different to how they actually are, which we would expect to result in lower accuracy.\n\nWhich resizing method to use therefore depends on the underlying problem and dataset. For example, if the features in the dataset images take up the whole image and cropping may result in loss of information, squishing or padding may be more useful.\n\nAnother better method is RandomResizedCrop, in which we crop on a randomly selected region of the image. So every epoch, the model will see a different part of the image and will learn accordingly.\"",
            "answer_context": [
                {
                    "answer_component": "\"crop is the default Resize() method, and it crops the images to fit a square shape of the size requested, using the full width or height. This can result in losing some important details\"", 
                    "scoring_type": "simple",
                    "context": [
                        "By default `Resize` *crops* the images to fit a square shape of the size requested, using the full width or height. This can result in losing some important details"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"crop is the default Resize() method, and it crops the images to fit a square shape of the size requested, using the full width or height. This can result in losing some important details. For instance, if we were trying to recognize the breed of dog or cat, we may end up cropping out a key part of the body or the face necessary to distinguish between similar breeds.\n\npad is an alternative Resize() method, which pads the matrix of the image\u2019s pixels with zeros (which shows as black when viewing the images). If we pad the images then we have a whole lot of empty space, which is just wasted computation for our model, and results in a lower effective resolution for the part of the image we actually use.\n\nsquish is another alternative Resize() method, which can either squish or stretch the image. This can cause the image to take on an unrealistic shape, leading to a model that learns that things look different to how they actually are, which we would expect to result in lower accuracy.\n\nWhich resizing method to use therefore depends on the underlying problem and dataset. For example, if the features in the dataset images take up the whole image and cropping may result in loss of information, squishing or padding may be more useful\"", 
                    "scoring_type": "simple",
                    "context": [
                        "All of these approaches seem somewhat wasteful, or problematic. If we squish or stretch the images they end up as unrealistic shapes, leading to a model that learns that things look different to how they actually are, which we would expect to result in lower accuracy. If we crop the images then we remove some of the features that allow us to perform recognition. For instance, if we were trying to recognize breeds of dog or cat, we might end up cropping out a key part of the body or the face necessary to distinguish between similar breeds. If we pad the images then we have a whole lot of empty space, which is just wasted computation for our model and results in a lower effective resolution for the part of the image we actually use."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Another better method is RandomResizedCrop", 
                    "scoring_type": "simple",
                    "context": [
                        "Here's another example where we replace `Resize` with `RandomResizedCrop`, which is the transform that provides the behavior we just described"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "in which we crop on a randomly selected region of the image. So every epoch, the model will see a different part of the image and will learn accordingly", 
                    "scoring_type": "simple",
                    "context": [
                        "training the neural network with examples of images where the objects are in slightly different places and slightly different sizes helps it to understand the basic concept of what an object is, and how it can be represented in an image."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 2,
            "question_number": 16,
            "question_text": "\"What is data augmentation? Why is it needed?\"",
            "gold_standard_answer": "\"Data augmentation refers to creating random variations of our input data, such that they appear different, but not so different that it changes the meaning of the data. Examples include flipping, rotation, perspective warping, brightness changes, etc. Data augmentation is useful for the model to better understand the basic concept of what an object is and how the objects of interest are represented in images. Therefore, data augmentation allows machine learning models to generalize. This is especially important when it can be slow and expensive to label data.\"",
            "answer_context": [
                {
                    "answer_component": "Data augmentation refers to creating random variations of our input data, such that they appear different, but not so different that it changes the meaning of the data. Examples include flipping, rotation, perspective warping, brightness changes, etc", 
                    "scoring_type": "simple",
                    "context": [
                        "*Data augmentation* refers to creating random variations of our input data, such that they appear different, but do not actually change the meaning of the data. Examples of common data augmentation techniques for images are rotation, flipping, perspective warping, brightness changes and contrast changes"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "it can be slow and expensive to label data", 
                    "scoring_type": "simple",
                    "context": [
                        "One major challenge for object detection systems is that image labelling can be slow and expensive. There is a lot of work at the moment going into tools to try to make this labelling faster and easier, and to require fewer handcrafted labels to train accurate object detection models. One approach that is particularly helpful is to synthetically generate variations of input images, such as by rotating them or changing their brightness and contrast; this is called *data augmentation* and also works well for text and other types of models. We will be discussing it in detail in this chapter."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Data augmentation is useful for the model to better understand the basic concept of what an object is and how the objects of interest are represented in images", 
                    "scoring_type": "simple",
                    "context": [
                        "training the neural network with examples of images where the objects are in slightly different places and slightly different sizes helps it to understand the basic concept of what an object is, and how it can be represented in an image."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Therefore, data augmentation allows machine learning models to generalize", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 2,
            "question_number": 17,
            "question_text": "\"What is the difference between `item_tfms` and `batch_tfms`?\"",
            "gold_standard_answer": "\"item_tfms are transformations applied to a single data sample x on the CPU. Resize() is a common transform because the mini-batch of input images to a cnn must have the same dimensions. Assuming the images are RGB with 3 channels, then Resize() as item_tfms will make sure the images have the same width and height.\nbatch_tfms are applied to batched data samples (aka individual samples that have been collated into a mini-batch) on the GPU. They are faster and more efficient than item_tfms. A good example of these are the ones provided by aug_transforms(). Inside are several batch-level augmentations that help many models.\"",
            "answer_context": [
                {
                    "answer_component": "item_tfms are transformations applied to a single data sample", 
                    "scoring_type": "simple",
                    "context": [
                        "*Item transforms* are pieces of code that run on each individual item, whether it be an image, category, or so forth"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "the mini-batch of input images to a cnn must have the same dimensions", 
                    "scoring_type": "simple",
                    "context": [
                        "Our images are all different sizes, and this is a problem for deep learning: we don't feed the model one image at a time but several of them (what we call a *mini-batch*). To group them in a big array (usually called a *tensor*) that is going to go through our model, they all need to be of the same size",

                        "we need to add a transform which will resize these images to the same size"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Resize()", 
                    "scoring_type": "simple",
                    "context": [
                        "`Resize`",
                        "item_tfms=Resize(128)",
                        "item_tfms=Resize(128, ResizeMethod.Squish)",
                        "item_tfms=Resize(128, ResizeMethod.Pad, pad_mode='zeros')"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Assuming the images are RGB with 3 channels", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "true"
                }, 
                {
                    "answer_component": "batch_tfms are applied to batched data samples (aka individual samples that have been collated into a mini-batch) on the GPU. They are faster and more efficient than item_tfms", 
                    "scoring_type": "simple",
                    "context": [
                        "Because our images are now all the same size, we can apply these augmentations to an entire batch of them using the GPU, which will save a lot of time. To tell fastai we want to use these transforms on a batch, we use the `batch_tfms` parameter"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "A good example of these are the ones provided by aug_transforms(). Inside are several batch-level augmentations that help many models", 
                    "scoring_type": "simple",
                    "context": [
                        "For natural photo images such as the ones we are using here, a standard set of augmentations that we have found work pretty well are provided with the `aug_transforms` function"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 2,
            "question_number": 18,
            "question_text": "\"What is a confusion matrix?\"",
            "gold_standard_answer": "\"A class confusion matrix is a representation of the predictions made vs the correct labels. The rows of the matrix represent the actual labels while the columns represent the predictions. Therefore, the number of images in the diagonal elements represent the number of correctly classified images, while the off-diagonal elements are incorrectly classified images. Class confusion matrices provide useful information about how well the model is doing and which classes the model might be confusing .\"",
            "answer_context": [
                {
                    "answer_component": "A class confusion matrix is a representation of the predictions made vs the correct labels", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "The rows of the matrix represent the actual labels while the columns represent the predictions. Therefore, the number of images in the diagonal elements represent the number of correctly classified images, while the off-diagonal elements are incorrectly classified images", 
                    "scoring_type": "simple",
                    "context": [
                        "The rows represent all the black, grizzly, and teddy bears in our dataset, respectively. The columns represent the images which the model predicted as black, grizzly, and teddy bears, respectively. Therefore, the diagonal of the matrix shows the images which were classified correctly, and the off-diagonal cells represent those which were classified incorrectly"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Class confusion matrices provide useful information about how well the model is doing and which classes the model might be confusing", 
                    "scoring_type": "simple",
                    "context": [
                        "Now let's see whether the mistakes the model is making are mainly thinking that grizzlies are teddies (that would be bad for safety!), or that grizzlies are black bears, or something else"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "confusion matrix", 
                    "context": [
                        "To visualize this, we can create a *confusion matrix*:"
                    ]
                }
            ]
        },
        {
            "chapter": 2,
            "question_number": 19,
            "question_text": "\"What does `export` save?\"",
            "gold_standard_answer": "\"export saves both the architecture, as well as the trained parameters of the neural network architecture. It also saves how the DataLoaders are defined.\"",
            "answer_context": [
                {
                    "answer_component": "export saves both the architecture, as well as the trained parameters of the neural network architecture", 
                    "scoring_type": "simple",
                    "context": [
                        "a model consists of two parts: the *architecture* and the trained *parameters*. The easiest way to save the model is to save both of these, because that way when you load a model you can be sure that you have the matching architecture and parameters. To save both parts, use the `export` method"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "It also saves how the DataLoaders are defined", 
                    "scoring_type": "simple",
                    "context": [
                        "This method even saves the definition of how to create your `DataLoaders`"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "`export`", 
                    "context": [
                        "`export`"
                    ]
                }
            ]
        },
        {
            "chapter": 2,
            "question_number": 20,
            "question_text": "\"What is it called when we use a model for getting predictions, instead of training?\"",
            "gold_standard_answer": "\"Inference\"",
            "answer_context": [
                {
                    "answer_component": "\"Inference\"", 
                    "scoring_type": "simple",
                    "context": [
                        "inference"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "\"What is it called when we use a model for getting predictions, instead of training?\"", 
                    "context": [
                        "When we use a model for getting predictions, instead of training"
                    ]
                }
            ]
        },
        {
            "chapter": 2,
            "question_number": 21,
            "question_text": "\"What are IPython widgets?\"",
            "gold_standard_answer": "\"IPython widgets are JavaScript and Python combined functionalities that let us build and interact with GUI components directly in a Jupyter notebook. An example of this would be an upload button, which can be created with the Python function widgets.FileUpload().\"",
            "answer_context": [
                {
                    "answer_component": "IPython widgets are JavaScript and Python combined functionalities that let us build and interact with GUI components directly in a Jupyter notebook", 
                    "scoring_type": "simple",
                    "context": [
                        "*IPython widgets* are GUI components that bring together JavaScript and Python functionality in a web browser, and can be created and used within a Jupyter notebook"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "An example of this would be an upload button, which can be created with the Python function widgets.FileUpload()", 
                    "scoring_type": "simple",
                    "context": [
                        "btn_upload = widgets.FileUpload()"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 2,
            "question_number": 22,
            "question_text": "\"When might you want to use CPU for deployment? When might GPU be better?\"",
            "gold_standard_answer": "\"GPUs are best for doing identical work in parallel. If you will be analyzing single pieces of data at a time (like a single image or single sentence), then CPUs may be more cost effective instead, especially with more market competition for CPU servers versus GPU servers. GPUs could be used if you collect user responses into a batch at a time, and perform inference on the batch. This may require the user to wait for model predictions. Additionally, there are many other complexities when it comes to GPU inference, like memory management and queuing of the batches.\"",
            "answer_context": [
                {
                    "answer_component": "GPUs are best for doing identical work in parallel", 
                    "scoring_type": "simple",
                    "context": [
                        "As we've seen, GPUs are only useful when they do lots of identical work in parallel"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "If you will be analyzing single pieces of data at a time (like a single image or single sentence), then CPUs may be more cost effective instead", 
                    "scoring_type": "simple",
                    "context": [
                        "If you're doing (say) image classification, then you'll normally be classifying just one user's image at a time, and there isn't normally enough work to do in a single image to keep a GPU busy for long enough for it to be very efficient. So, a CPU will often be more cost-effective."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "especially with more market competition for CPU servers versus GPU servers", 
                    "scoring_type": "simple",
                    "context": [
                        "There's a lot more market competition in CPU than GPU servers, as a result of which there are much cheaper options available for CPU servers"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "GPUs could be used if you collect user responses into a batch at a time, and perform inference on the batch. This may require the user to wait for model predictions", 
                    "scoring_type": "simple",
                    "context": [
                        "An alternative could be to wait for a few users to submit their images, and then batch them up and process them all at once on a GPU. But then you're asking your users to wait, rather than getting answers straight away"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Additionally, there are many other complexities when it comes to GPU inference", 
                    "scoring_type": "simple",
                    "context": [
                        "The complexities of dealing with GPU inference are significant",

                        "Because of the complexity of GPU serving, many systems have sprung up to try to automate this. However, managing and running these systems is also complex, and generally requires compiling your model into a different form that's specialized for that system. It's typically preferable to avoid dealing with this complexity until/unless your app gets popular enough that it makes clear financial sense for you to do so.",

                        "Overall, we'd recommend using a simple CPU-based server approach where possible, for as long as you can get away with it. If you're lucky enough to have a very successful application, then you'll be able to justify the investment in more complex deployment approaches at that time"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "memory management and queuing of the batches", 
                    "scoring_type": "simple",
                    "context": [
                        "In particular, the GPU's memory will need careful manual management, and you'll need a careful queueing system to ensure you only process one batch at a time"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 2,
            "question_number": 23,
            "question_text": "\"What are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC?\"",
            "gold_standard_answer": "\"The application will require network connection, and there will be extra network latency time when submitting input and returning results. Additionally, sending private data to a network server can lead to security concerns.\n\nOn the flip side deploying a model to a server makes it easier to iterate and roll out new versions of a model. This is because you as a developer have full control over the server environment and only need to do it once rather than having to make sure that all the endpoints (phones, PCs) upgrade their version individually.\"",
            "answer_context": [
                {
                    "answer_component": "\"The application will require network connection, and there will be extra network latency time when submitting input and returning results. Additionally, sending private data to a network server can lead to security concerns\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Your application will require a network connection, and there will be some latency each time the model is called. (It takes a while for a neural network model to run anyway, so this additional network latency may not make a big difference to your users in practice. In fact, since you can use better hardware on the server, the overall latency may even be less than if it were running locally!) Also, if your application uses sensitive data then your users may be concerned about an approach which sends that data to a remote server, so sometimes privacy considerations will mean that you need to run the model on the edge device"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"On the flip side deploying a model to a server makes it easier to iterate and roll out new versions of a model rather than having to make sure that all the endpoints (phones, PCs) upgrade their version individually.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "upgrades of that core logic can happen on your server, rather than needing to be distributed to all of your users"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"This is because you as a developer have full control over the server environment\"", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "What are the downsides", 
                    "context": [
                        "There are downsides too, of course. "
                    ]
                }
            ]
        },
        {
            "chapter": 2,
            "question_number": 24,
            "question_text": "\"What are three examples of problems that could occur when rolling out a bear warning system in practice?\"",
            "gold_standard_answer": "\"The model we trained will likely perform poorly when:\n\nHandling night-time images\nDealing with low-resolution images (ex: some smartphone images)\nThe model returns prediction too slowly to be useful\"",
            "answer_context": [
                {
                    "answer_component": "\"The model we trained will likely perform poorly when:\n\nHandling night-time images\nDealing with low-resolution images (ex: some smartphone images)\nThe model returns prediction too slowly to be useful\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Handling nighttime images, which may not appear in this dataset\n- Dealing with low-resolution camera images\n- Ensuring results are returned fast enough to be useful in practice"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 2,
            "question_number": 25,
            "question_text": "\"What is \"out-of-domain data\"?\"",
            "gold_standard_answer": "\"Data that is fundamentally different in some aspect compared to the model\u2019s training data. For example, an object detector that was trained exclusively with outside daytime photos is given a photo taken at night.\"",
            "answer_context": [
                {
                    "answer_component": "Data that is fundamentally different in some aspect compared to the model\u2019s training data", 
                    "scoring_type": "simple",
                    "context": [
                        "This is just one example of the more general problem of *out-of-domain* data. That is to say, there may be data that our model sees in production which is very different to what it saw during training"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "For example, an object detector that was trained exclusively with outside daytime photos is given a photo taken at night", 
                    "scoring_type": "simple",
                    "context": [
                        "Handling nighttime images, which may not appear in this dataset"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 2,
            "question_number": 26,
            "question_text": "\"What is \"domain shift\"?\"",
            "gold_standard_answer": "\"This is when the type of data changes gradually over time. For example, an insurance company is using a deep learning model as part of their pricing algorithm, but over time their customers will be different, with the original training data not being representative of current data, and the deep learning model being applied on effectively out-of-domain data.\"",
            "answer_context": [
                {
                    "answer_component": "\"This is when the type of data changes gradually over time. For example, an insurance company is using a deep learning model as part of their pricing algorithm, but over time their customers will be different, with the original training data not being representative of current data, and the deep learning model being applied on effectively out-of-domain data.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "There are other reasons we need to be careful too. One very common problem is *domain shift*, where the type of data that our model sees changes over time. For instance, an insurance company may use a deep learning model as part of its pricing and risk algorithm, but over time the types of customers that the company attracts, and the types of risks they represent, may change so much that the original training data is no longer relevant."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 2,
            "question_number": 27,
            "question_text": "\"What are the three steps in the deployment process?\"",
            "gold_standard_answer": "\"Manual process \u2013 the model is run in parallel and not directly driving any actions, with humans still checking the model outputs.\nLimited scope deployment \u2013 The model\u2019s scope is limited and carefully supervised. For example, doing a geographically and time-constrained trial of model deployment, that is carefully supervised.\nGradual expansion \u2013 The model scope is gradually increased, while good reporting systems are implemented in order to check for any significant changes to the actions taken compared to the manual process (i.e. the models should perform similarly to the humans, unless it is already anticipated to be better).\"",
            "answer_context": [
                {
                    "answer_component": "Manual process \u2013 the model is run in parallel and not directly driving any actions, with humans still checking the model outputs", 
                    "scoring_type": "simple",
                    "context": [
                        "Where possible, the first step is to use an entirely manual process, with your deep learning model approach running in parallel but not being used directly to drive any actions. The humans involved in the manual process should look at the deep learning outputs and check whether they make sense"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Limited scope deployment \u2013 The model\u2019s scope is limited and carefully supervised. For example, doing a geographically and time-constrained trial of model deployment, that is carefully supervised", 
                    "scoring_type": "simple",
                    "context": [
                        "The second step is to try to limit the scope of the model, and have it carefully supervised by people. For instance, do a small geographically and time-constrained trial of the model-driven approach. Rather than rolling our bear classifier out in every national park throughout the country, we could pick a single observation post, for a one-week period, and have a park ranger check each alert before it goes out."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Gradual expansion \u2013 The model scope is gradually increased, while good reporting systems are implemented in order to check for any significant changes to the actions taken compared to the manual process (i.e. the models should perform similarly to the humans, unless it is already anticipated to be better)", 
                    "scoring_type": "simple",
                    "context": [
                        "Then, gradually increase the scope of your rollout. As you do so, ensure that you have really good reporting systems in place, to make sure that you are aware of any significant changes to the actions being taken compared to your manual process"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "(i.e. the models should perform similarly to the humans, unless it is already anticipated to be better)", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "true"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 2,
            "question_text": "\"How are the files and folders in the MNIST_SAMPLE dataset structured? Why?\"",
            "gold_standard_answer": "\"There are two subfolders, train and valid, the former contains the data for model training, the latter contains the data for validating model performance after each training step. Evaluating the model on the validation set serves two purposes: a) to report a human-interpretable metric such as accuracy (in contrast to the often abstract loss functions used for training), b) to facilitate the detection of overfitting by evaluating the model on a dataset it hasn\u2019t been trained on (in short, an overfitting model performs increasingly well on the training set but decreasingly so on the validation set). Of course, every practicioner could generate their own train/validation-split of the data. Public datasets are usually pre-split to simplifiy comparing results between implementations/publications.\n\nEach subfolder has two subsubfolders 3 and 7 which contain the .jpg files for the respective class of images. This is a common way of organizing datasets comprised of pictures. For the full MNIST dataset there are 10 subsubfolders, one for the images for each digit.\"",
            "answer_context": [
                {
                    "answer_component": "There are two subfolders, train and valid, the former contains the data for model training, the latter contains the data for validating model performance after each training step", 
                    "scoring_type": "simple",
                    "context": [
                        "The MNIST dataset follows a common layout for machine learning datasets: separate folders for the training set and the validation set (and/or test set). Let's see what's inside the training set:"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Each subfolder has two subsubfolders 3 and 7", 
                    "scoring_type": "simple",
                    "context": [
                        "There's a folder of 3s, and a folder of 7s"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "which contain the .jpg files for the respective class of images", 
                    "scoring_type": "simple",
                    "context": [
                        "```python\nthrees = (path/'train'/'3').ls().sorted()\nsevens = (path/'train'/'7').ls().sorted()\nthrees\n```\nOutput:\n(#6131) [Path('train/3/10.png'),Path('train/3/10000.png'),Path('train/3/10011.png'),Path('train/3/10031.png'),Path('train/3/10034.png'),Path('train/3/10042.png'),Path('train/3/10052.png'),Path('train/3/1007.png'),Path('train/3/10074.png'),Path('train/3/10091.png')...]",

                        "As we might expect, it's full of image files"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Evaluating the model on the validation set", 
                    "scoring_type": "simple",
                    "context": [
                        "As we've discussed, we want to calculate our metric over a *validation set*"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "true"
                },
                {
                    "answer_component": "facilitate the detection of overfitting by evaluating the model on a dataset it hasn\u2019t been trained on", 
                    "scoring_type": "simple",
                    "context": [
                        "As we've discussed, we want to calculate our metric over a *validation set*. This is so that we don't inadvertently overfit\u2014that is, train a model to work well only on our training data"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "true"
                },
                {
                    "answer_component": "a) to report a human-interpretable metric such as accuracy (in contrast to the often abstract loss functions used for training)", 
                    "scoring_type": "simple",
                    "context": [
                        "The key difference is that the metric is to drive human understanding and the loss is to drive automated learning"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "true"
                },
                {
                    "answer_component": "Public datasets are usually pre-split to simplifiy comparing results between implementations/publications", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "true"
                },
                {
                    "answer_component": "For the full MNIST dataset there are 10 subsubfolders, one for the images for each digit", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "true"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 3,
            "question_text": "\"Explain how the \"pixel similarity\" approach to classifying digits works.\"",
            "gold_standard_answer": "\"In the \u201cpixel similarity\u201d approach, we generate an archetype for each class we want to identify. In our case, we want to distinguish images of 3\u2019s from images of 7\u2019s. We define the archetypical 3 as the pixel-wise mean value of all 3\u2019s in the training set. Analoguously for the 7\u2019s. You can visualize the two archetypes and see that they are in fact blurred versions of the numbers they represent.\nIn order to tell if a previously unseen image is a 3 or a 7, we calculate its distance to the two archetypes (here: mean pixel-wise absolute difference). We say the new image is a 3 if its distance to the archetypical 3 is lower than two the archetypical 7.\"",

            "answer_context": [
                {
                    "answer_component": "We define the archetypical 3 as the pixel-wise mean value of all 3\u2019s in the training set. Analoguously for the 7\u2019s. You can visualize the two archetypes and see that they are in fact blurred versions of the numbers they represent.\nIn order to tell if a previously unseen image is a 3 or a 7, we calculate its distance to the two archetypes (here: mean pixel-wise absolute difference). We say the new image is a 3 if its distance to the archetypical 3 is lower than two the archetypical 7",
                    "scoring_type": "simple",
                    "context": [
                        "how about we find the average pixel value for every pixel of the 3s, then do the same for the 7s. This will give us two group averages, defining what we might call the \"ideal\" 3 and 7. Then, to classify an image as one digit or the other, we see which of these two ideal digits the image is most similar to"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 4,
            "question_text": "\"What is a list comprehension? Create one now that selects odd numbers from a list and doubles them.\"",
            "gold_standard_answer": "\"Lists (arrays in other programming languages) are often generated using a for-loop. A list comprehension is a Pythonic way of condensing the creation of a list using a for-loop into a single expression. List comprehensions will also often include if clauses for filtering.\n\nlst_in = range(10)\nlst_out = [2*el for el in lst_in if el%2==1]\n# is equivalent to:\nlst_out = []\nfor el in lst_in:\n   if el%2==1:\n       lst_out.append(2*el)\"",
            "answer_context": [
                {
                    "answer_component": "\"Lists (arrays in other programming languages) are often generated using a for-loop. A list comprehension is a Pythonic way of condensing the creation of a list using a for-loop into a single expression. List comprehensions will also often include if clauses for filtering.\n\nlst_in = range(10)\nlst_out = [2*el for el in lst_in if el%2==1]\n# is equivalent to:\nlst_out = []\nfor el in lst_in:\n   if el%2==1:\n       lst_out.append(2*el)\"", 
                    "scoring_type": "simple",
                    "context": [
                        "> note: List Comprehensions: List and dictionary comprehensions are a wonderful feature of Python. Many Python programmers use them every day, including the authors of this book\u2014they are part of \"idiomatic Python.\" But programmers coming from other languages may have never seen them before. There are a lot of great tutorials just a web search away, so we won't spend a long time discussing them now. Here is a quick explanation and example to get you started. A list comprehension looks like this: `new_list = [f(o) for o in a_list if o>0]`. This will return every element of `a_list` that is greater than 0, after passing it to the function `f`. There are three parts here: the collection you are iterating over (`a_list`), an optional filter (`if o>0`), and something to do to each element (`f(o)`). It's not only shorter to write but way faster than the alternative ways of creating the same list with a loop."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 5,
            "question_text": "\"What is a \"rank-3 tensor\"?\"",
            "gold_standard_answer": "\"The rank of a tensor is the number of dimensions it has. An easy way to identify the rank is the number of indices you would need to reference a number within a tensor. A scalar can be represented as a tensor of rank 0 (no index), a vector can be represented as a tensor of rank 1 (one index, e.g., v[i]), a matrix can be represented as a tensor of rank 2 (two indices, e.g., a[i,j]), and a tensor of rank 3 is a cuboid or a \u201cstack of matrices\u201d (three indices, e.g., b[i,j,k]). In particular, the rank of a tensor is independent of its shape or dimensionality, e.g., a tensor of shape 2x2x2 and a tensor of shape 3x5x7 both have rank 3.\nNote that the term \u201crank\u201d has different meanings in the context of tensors and matrices (where it refers to the number of linearly independent column vectors).\"",
            "answer_context": [
                {
                    "answer_component": "The rank of a tensor is the number of dimensions it has", 
                    "scoring_type": "simple",
                    "context": [
                        "_rank_ is the number of axes or dimensions in a tensor",

                        "The number of dimensions of a tensor is its *rank*"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "An easy way to identify the rank is the number of indices you would need to reference a number within a tensor", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "A scalar can be represented as a tensor of rank 0 (no index), a vector can be represented as a tensor of rank 1 (one index, e.g., v[i]), a matrix can be represented as a tensor of rank 2 (two indices, e.g., a[i,j])", 
                    "scoring_type": "simple",
                    "context": [
                        "- Rank zero: scalar\n- Rank one: vector\n- Rank two: matrix"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "and a tensor of rank 3 is a cuboid or a \u201cstack of matrices\u201d", 
                    "scoring_type": "simple",
                    "context": [
                        "a list of matrices (a rank-3 tensor)"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "In particular, the rank of a tensor is independent of its shape or dimensionality, e.g., a tensor of shape 2x2x2 and a tensor of shape 3x5x7 both have rank 3", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "true"
                },
                {
                    "answer_component": "Note that the term \u201crank\u201d has different meanings in the context of tensors and matrices (where it refers to the number of linearly independent column vectors)", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "true"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 6,
            "question_text": "\"What is the difference between tensor rank and shape? How do you get the rank from the shape?\"",
            "gold_standard_answer": "\"Rank is the number of axes or dimensions in a tensor; shape is the size of each axis of a tensor.\n\nThe length of a tensor\u2019s shape is its rank.\n\nSo if we have the images of the 3 folder from the MINST_SAMPLE dataset in a tensor called stacked_threes and we find its shape like this.\n\nIn [ ]: stacked_threes.shape\nOut[ ]: torch.Size([6131, 28, 28])\nWe just need to find its length to know its rank. This is done as follows.\n\nIn [ ]: len(stacked_threes.shape)\nOut[ ]: 3\nYou can also get a tensor\u2019s rank directly with ndim .\n\nIn [ ]: stacked_threes.ndim\nOut[ ]: 3\"",

            "answer_context": [
                {
                    "answer_component": "Rank is the number of axes or dimensions in a tensor", 
                    "scoring_type": "simple",
                    "context": [
                        "_rank_ is the number of axes or dimensions in a tensor",

                        "The number of dimensions of a tensor is its *rank*"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "shape is the size of each axis of a tensor", 
                    "scoring_type": "simple",
                    "context": [
                        "Perhaps the most important attribute of a tensor is its *shape*. This tells you the length of each axis",

                        "_shape_ is the size of each axis of a tensor."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "The length of a tensor\u2019s shape is its rank", 
                    "scoring_type": "simple",
                    "context": [
                        "The *length* of a tensor's shape is its rank"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "So if we have the images of the 3 folder from the MINST_SAMPLE dataset in a tensor called stacked_threes and we find its shape like this.\n\nIn [ ]: stacked_threes.shape\nOut[ ]: torch.Size([6131, 28, 28])\n\"", 
                    "scoring_type": "simple",
                    "context": [
                        "```python\nstacked_sevens = torch.stack(seven_tensors).float()/255\nstacked_threes = torch.stack(three_tensors).float()/255\nstacked_threes.shape\n```\nOutput:\ntorch.Size([6131, 28, 28])"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "In [ ]: stacked_threes.ndim\nOut[ ]: 3", 
                    "scoring_type": "simple",
                    "context": [
                        "```python\nstacked_threes.ndim\n```\nOutput:\n3"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "You can also get a tensor\u2019s rank directly with ndim", 
                    "scoring_type": "simple",
                    "context": [
                        "We can also get a tensor's rank directly with `ndim`:"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 7,
            "question_text": "\"What are RMSE and L1 norm?\"",
            "gold_standard_answer": "\"Root mean square error (RMSE), also called the L2 norm, and mean absolute difference (MAE), also called the L1 norm, are two commonly used methods of measuring \u201cdistance\u201d. Simple differences do not work because some difference are positive and others are negative, canceling each other out. Therefore, a function that focuses on the magnitudes of the differences is needed to properly measure distances. The simplest would be to add the absolute values of the differences, which is what MAE is. RMSE takes the mean of the square (makes everything positive) and then takes the square root (undoes squaring).\"",
            "answer_context": [
                {
                    "answer_component": [
                        "Root mean square error (RMSE), also called the L2 norm, and mean absolute difference (MAE), also called the L1 norm",
                        "The simplest would be to add the absolute values of the differences, which is what MAE is. RMSE takes the mean of the square (makes everything positive) and then takes the square root (undoes squaring)."
                    ], 
                    "scoring_type": "simple",
                    "context": [
                        "- Take the mean of the *absolute value* of differences (absolute value is the function that replaces negative values with positive values). This is called the *mean absolute difference* or *L1 norm*\n- Take the mean of the *square* of differences (which makes everything positive) and then take the *square root* (which undoes the squaring). This is called the *root mean squared error* (RMSE) or *L2 norm*."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "two commonly used methods of measuring \u201cdistance\u201d", 
                    "scoring_type": "simple",
                    "context": [
                        "To avoid this, there are two main ways data scientists measure distance in this context"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Simple differences do not work because some difference are positive and others are negative, canceling each other out. Therefore, a function that focuses on the magnitudes of the differences is needed to properly measure distances", 
                    "scoring_type": "simple",
                    "context": [
                        "How can we determine its distance from our ideal 3? We can't just add up the differences between the pixels of this image and the ideal digit. Some differences will be positive while others will be negative, and these differences will cancel out, resulting in a situation where an image that is too dark in some places and too light in others might be shown as having zero total differences from the ideal. That would be misleading!"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 8,
            "question_text": "\"How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?\"",
            "gold_standard_answer": "\"As loops are very slow in Python, it is best to represent the operations as array operations rather than looping through individual elements. If this can be done, then using NumPy or PyTorch will be thousands of times faster, as they use underlying C code which is much faster than pure Python. Even better, PyTorch allows you to run operations on GPU, which will have significant speedup if there are parallel operations that can be done.\"",
            "answer_context": [
                {
                    "answer_component": "As loops are very slow in Python, it is best to represent the operations as array operations rather than looping through individual elements. If this can be done, then using NumPy or PyTorch will be thousands of times faster, as they use underlying C code which is much faster than pure Python", 
                    "scoring_type": "simple",
                    "context": [
                        "Python is slow compared to many languages. Anything fast in Python, NumPy, or PyTorch is likely to be a wrapper for a compiled object written (and optimized) in another language\u2014specifically C. In fact, **NumPy arrays and PyTorch tensors can finish computations many thousands of times faster than using pure Python.**",

                        "It does the whole calculation in C (or, if you're using a GPU, in CUDA, the equivalent of C on the GPU), tens of thousands of times faster than pure Python (up to millions of times faster on a GPU!)."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Even better, PyTorch allows you to run operations on GPU, which will have significant speedup if there are parallel operations that can be done", 
                    "scoring_type": "simple",
                    "context": [
                        "The vast majority of methods and operators supported by NumPy on these structures are also supported by PyTorch, but PyTorch tensors have additional capabilities. One major capability is that these structures can live on the GPU, in which case their computation will be optimized for the GPU and can run much faster (given lots of values to work on)",

                        "It does the whole calculation in C (or, if you're using a GPU, in CUDA, the equivalent of C on the GPU), tens of thousands of times faster than pure Python (up to millions of times faster on a GPU!)."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 10,
            "question_text": "\"What is broadcasting?\"",
            "gold_standard_answer": "\"Scientific/numerical Python packages like NumPy and PyTorch will often implement broadcasting that often makes code easier to write. In the case of PyTorch, tensors with smaller rank are expanded to have the same size as the larger rank tensor. In this way, operations can be performed between tensors with different rank.\"",
            "answer_context": [
                {
                    "answer_component": "\"Scientific/numerical Python packages like NumPy and PyTorch will often implement broadcasting that often makes code easier to write. In the case of PyTorch, tensors with smaller rank are expanded to have the same size as the larger rank tensor. In this way, operations can be performed between tensors with different rank.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "The magic trick is that PyTorch, when it tries to perform a simple subtraction operation between two tensors of different ranks, will use *broadcasting*. That is, it will automatically expand the tensor with the smaller rank to have the same size as the one with the larger rank. Broadcasting is an important capability that makes tensor code much easier to write."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 11,
            "question_text": "\"Are metrics generally calculated using the training set, or the validation set? Why?\"",
            "gold_standard_answer": "\"Metrics are generally calculated on a validation set. As the validation set is unseen data for the model, evaluating the metrics on the validation set is better in order to determine if there is any overfitting and how well the model might generalize if given similar data.\"",
            "answer_context": [
                {
                    "answer_component": "Metrics are generally calculated on a validation set", 
                    "scoring_type": "simple",
                    "context": [
                        "As we've discussed, we want to calculate our metric over a *validation set*. This is so that we don't inadvertently overfit\u2014that is, train a model to work well only on our training data"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "As the validation set is unseen data for the model, evaluating the metrics on the validation set is better in order to determine if there is any overfitting", 
                    "scoring_type": "simple",
                    "context": [
                        "This is so that we don't inadvertently overfit\u2014that is, train a model to work well only on our training data"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "how well the model might generalize if given similar data", 
                    "scoring_type": "simple",
                    "context": [
                        "This is so that we don't inadvertently overfit\u2014that is, train a model to work well only on our training data"
                    ],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 12,
            "question_text": "\"What is SGD?\"",
            "gold_standard_answer": "\"SGD, or stochastic gradient descent, is an optimization algorithm. Specifically, SGD is an algorithm that will update the parameters of a model in order to minimize a given loss function that was evaluated on the predictions and target. The key idea behind SGD (and many optimization algorithms, for that matter) is that the gradient of the loss function provides an indication of how that loss function changes in the parameter space, which we can use to determine how best to update the parameters in order to minimize the loss function. This is what SGD does.\"",
            "answer_context": [
                {
                    "answer_component": "SGD, or stochastic gradient descent,", 
                    "scoring_type": "simple",
                    "context": [
                        "## Stochastic Gradient Descent (SGD)"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "optimization algorithm", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "SGD is an algorithm that will update the parameters of a model", 
                    "scoring_type": "simple",
                    "context": [
                        "This is known as *stepping* your parameters, using an *optimizer step*. Notice how we _subtract_ the `gradient * lr` from the parameter to update it"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "in order to minimize a given loss function that was evaluated on the predictions and target", 
                    "scoring_type": "simple",
                    "context": [
                        "We need to define first what we mean by \"best.\" We define this precisely by choosing a *loss function*, which will return a value based on a prediction and a target, where lower values of the function correspond to \"better\" predictions. It is important for loss functions to return _lower_ values when predictions are more accurate, as the SGD procedure we defined earlier will try to _minimize_ this loss",

                        "This allows us to adjust the parameter in the direction of the slope by increasing the parameter when the slope is negative and decreasing the parameter when the slope is positive.  We want to adjust our parameters in the direction of the slope because our goal in deep learning is to _minimize_ the loss."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "The key idea behind SGD (and many optimization algorithms, for that matter) is that the gradient of the loss function provides an indication of how that loss function changes in the parameter space, which we can use to determine how best to update the parameters in order to minimize the loss function", 
                    "scoring_type": "simple",
                    "context": [
                        "As we've seen, we need gradients in order to improve our model using SGD, and in order to calculate gradients we need some *loss function* that represents how good our model is. That is because the gradients are a measure of how that loss function changes with small tweaks to the weights."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 13,
            "question_text": "\"Why does SGD use mini-batches?\"",
            "gold_standard_answer": "\"We need to calculate our loss function (and our gradient) on one or more data points. We cannot calculate on the whole datasets due to compute limitations and time constraints. If we iterated through each data point, however, the gradient will be unstable and imprecise, and is not suitable for training. As a compromise, we calculate the average loss for a small subset of the dataset at a time. This subset is called a mini-batch. Using mini-batches are also more computationally efficient than single items on a GPU.\"",
            "answer_context": [
                {
                    "answer_component": "We need to calculate our loss function (and our gradient) on one or more data points. We cannot calculate on the whole datasets due to compute limitations and time constraints. If we iterated through each data point, however, the gradient will be unstable and imprecise, and is not suitable for training", 
                    "scoring_type": "simple",
                    "context": [
                        "In order to take an optimization step we need to calculate the loss over one or more data items. How many should we use? We could calculate it for the whole dataset, and take the average, or we could calculate it for a single data item. But neither of these is ideal. Calculating it for the whole dataset would take a very long time. Calculating it for a single item would not use much information, so it would result in a very imprecise and unstable gradient. That is, you'd be going to the trouble of updating the weights, but taking into account only how that would improve the model's performance on that single item."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "As a compromise, we calculate the average loss for a small subset of the dataset at a time. This subset is called a mini-batch", 
                    "scoring_type": "simple",
                    "context": [
                        "So instead we take a compromise between the two: we calculate the average loss for a few data items at a time. This is called a *mini-batch*"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Using mini-batches are also more computationally efficient than single items on a GPU", 
                    "scoring_type": "simple",
                    "context": [
                        "Another good reason for using mini-batches rather than calculating the gradient on individual data items is that, in practice, we nearly always do our training on an accelerator such as a GPU. These accelerators only perform well if they have lots of work to do at a time, so it's helpful if we can give them lots of data items to work on. Using mini-batches is one of the best ways to do this"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 14,
            "question_text": "\"What are the seven steps in SGD for machine learning?\"",
            "gold_standard_answer": "\"Initialize the parameters \u2013 Random values often work best.\nCalculate the predictions \u2013 This is done on the training set, one mini-batch at a time.\nCalculate the loss \u2013 The average loss over the minibatch is calculated\nCalculate the gradients \u2013 this is an approximation of how the parameters need to change in order to minimize the loss function\nStep the weights \u2013 update the parameters based on the calculated weights\nRepeat the process\nStop \u2013 In practice, this is either based on time constraints or usually based on when the training/validation losses and metrics stop improving.\"",
            "answer_context": [
                {
                    "answer_component": "\"Initialize the parameters \u2013 Random values often work best.\nCalculate the predictions \u2013 This is done on the training set, one mini-batch at a time.\nCalculate the loss \u2013 The average loss over the minibatch is calculated\nCalculate the gradients \u2013 this is an approximation of how the parameters need to change in order to minimize the loss function\nStep the weights \u2013 update the parameters based on the calculated weights\nRepeat the process\nStop \u2013 In practice, this is either based on time constraints or usually based on when the training/validation losses and metrics stop improving.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "1. *Initialize* the weights.\n1. For each image, use these weights to *predict* whether it appears to be a 3 or a 7.\n1. Based on these predictions, calculate how good the model is (its *loss*).\n1. Calculate the *gradient*, which measures for each weight, how changing that weight would change the loss\n1. *Step* (that is, change) all the weights based on that calculation.\n1. Go back to the step 2, and *repeat* the process.\n1. Iterate until you decide to *stop* the training process (for instance, because the model is good enough or you don't want to wait any longer)."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"Initialize the parameters \u2013 Random values often work best.\nCalculate the predictions \u2013 This is done on the training set, one mini-batch at a time.\nCalculate the loss \u2013 The average loss over the minibatch is calculated\nCalculate the gradients \u2013 this is an approximation of how the parameters need to change in order to minimize the loss function\nStep the weights \u2013 update the parameters based on the calculated weights\nRepeat the process\nStop \u2013 In practice, this is either based on time constraints or usually based on when the training/validation losses and metrics stop improving.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "- Initialize:: We initialize the parameters to random values. This may sound surprising. There are certainly other choices we could make, such as initializing them to the percentage of times that pixel is activated for that category\u2014but since we already know that we have a routine to improve these weights, it turns out that just starting with random weights works perfectly well.\n- Loss:: This is what Samuel referred to when he spoke of *testing the effectiveness of any current weight assignment in terms of actual performance*. We need some function that will return a number that is small if the performance of the model is good (the standard approach is to treat a small loss as good, and a large loss as bad, although this is just a convention).\n- Step:: A simple way to figure out whether a weight should be increased a bit, or decreased a bit, would be just to try it: increase the weight by a small amount, and see if the loss goes up or down. Once you find the correct direction, you could then change that amount by a bit more, and a bit less, until you find an amount that works well. However, this is slow! As we will see, the magic of calculus allows us to directly figure out in which direction, and by roughly how much, to change each weight, without having to try all these small changes. The way to do this is by calculating *gradients*. This is just a performance optimization, we would get exactly the same results by using the slower manual process as well.\n- Stop:: Once we've decided how many epochs to train the model for (a few suggestions for this were given in the earlier list), we apply that decision. This is where that decision is applied. For our digit classifier, we would keep training until the accuracy of the model started getting worse, or we ran out of time."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 15,
            "question_text": "\"How do we initialize the weights in a model?\"",
            "gold_standard_answer": "\"Random weights work pretty well.\"",
            "answer_context": [
                {
                    "answer_component": "\"Random weights work pretty well.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "We initialize the parameters to random values. This may sound surprising. There are certainly other choices we could make, such as initializing them to the percentage of times that pixel is activated for that category\u2014but since we already know that we have a routine to improve these weights, it turns out that just starting with random weights works perfectly well"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 16,
            "question_text": "\"What is \"loss\"?\"",
            "gold_standard_answer": "\"The loss function will return a value based on the given predictions and targets, where lower values correspond to better model predictions.\"",
            "answer_context": [
                {
                    "answer_component": "The loss function will return a value based on the given predictions and targets", 
                    "scoring_type": "simple",
                    "context": [
                        "We define this precisely by choosing a *loss function*, which will return a value based on a prediction and a target",

                        "The purpose of the loss function is to measure the difference between predicted values and the true values \\u2014 that is, the targets (aka labels)"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "lower values correspond to better model predictions", 
                    "scoring_type": "simple",
                    "context": [
                        "our goal in deep learning is to _minimize_ the loss",

                        "We need some function that will return a number that is small if the performance of the model is good",

                        "a lower value of a loss function is better",

                        "this function returns a lower number when predictions are more accurate, when accurate predictions are more confident (higher absolute values), and when inaccurate predictions are less confident",

                        "lower values of the function correspond to \"better\" predictions"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 17,
            "question_text": "\"Why can't we always use a high learning rate?\"",
            "gold_standard_answer": "\"The loss may \u201cbounce\u201d around (oscillate) or even diverge, as the optimizer is taking steps that are too large, and updating the parameters faster than it should be.\"",
            "answer_context": [
                {
                    "answer_component": "or even diverge", 
                    "scoring_type": "simple",
                    "context": [
                        "But picking a learning rate that's too high is even worse\u2014it can actually result in the loss getting *worse*, as we see in <<descent_div>>!"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "The loss may \u201cbounce\u201d around (oscillate)", 
                    "scoring_type": "simple",
                    "context": [
                        "If the learning rate is too high, it may also \"bounce\" around, rather than actually diverging"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "the optimizer is taking steps that are too large, and updating the parameters faster than it should be", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 18,
            "question_text": "\"What is a \"gradient\"?\"",
            "gold_standard_answer": "\"The gradients tell us how much we have to change each weight to make our model better. It is essentially a measure of how the loss function changes with changes of the weights of the model (the derivative).\"",
            "answer_context": [
                {
                    "answer_component": "The gradients tell us how much we have to change each weight to make our model better", 
                    "scoring_type": "simple",
                    "context": [
                        "the gradients will tell us how much we have to change each weight to make our model better."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "It is essentially a measure of how the loss function changes with changes of the weights of the model (the derivative)", 
                    "scoring_type": "simple",
                    "context": [
                        "The derivative of the loss with respect to some parameter of the model"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 19,
            "question_text": "\"Do you need to know how to calculate gradients yourself?\"",
            "gold_standard_answer": "\"Manual calculation of the gradients are not required, as deep learning libraries will automatically calculate the gradients for you. This feature is known as automatic differentiation. In PyTorch, if requires_grad=True, the gradients can be returned by calling the backward method: a.backward()\"",
            "answer_context": [
                {
                    "answer_component": "Manual calculation of the gradients are not required, as deep learning libraries will automatically calculate the gradients for you", 
                    "scoring_type": "simple",
                    "context": [
                        "We mentioned just now that you won't have to calculate any gradients yourself. How can that be? Amazingly enough, PyTorch is able to automatically compute the derivative of nearly any function! What's more, it does it very fast. Most of the time, it will be at least as fast as any derivative function that you can create by hand. Let's see an example."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "In PyTorch, if requires_grad=True, the gradients can be returned by calling the backward method: a.backward()", 
                    "scoring_type": "simple",
                    "context": [
                        "Notice the special method `requires_grad_`? That's the magical incantation we use to tell PyTorch that we want to calculate gradients with respect to that variable at that value. It is essentially tagging the variable, so PyTorch will remember to keep track of how to compute gradients of the other, direct calculations on it that you will ask for."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "This feature is known as automatic differentiation", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 20,
            "question_text": "\"Why can't we use accuracy as a loss function?\"",
            "gold_standard_answer": "\"A loss function needs to change as the weights are being adjusted. Accuracy only changes if the predictions of the model change. So if there are slight changes to the model that, say, improves confidence in a prediction, but does not change the prediction, the accuracy will still not change. Therefore, the gradients will be zero everywhere except when the actual predictions change. The model therefore cannot learn from the gradients equal to zero, and the model\u2019s weights will not update and will not train. A good loss function gives a slightly better loss when the model gives slightly better predictions. Slightly better predictions mean if the model is more confident about the correct prediction. For example, predicting 0.9 vs 0.7 for probability that a MNIST image is a 3 would be slightly better prediction. The loss function needs to reflect that.\"",
            "answer_context": [
                {
                    "answer_component": "A loss function needs to change as the weights are being adjusted", 
                    "scoring_type": "simple",
                    "context": [
                        "we need a loss function which, when our weights result in slightly better predictions, gives us a slightly better loss"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Accuracy only changes if the predictions of the model change. So if there are slight changes to the model that, say, improves confidence in a prediction, but does not change the prediction, the accuracy will still not change. Therefore, the gradients will be zero everywhere except when the actual predictions change", 
                    "scoring_type": "simple",
                    "context": [
                        "But accuracy only changes at all when a prediction changes from a 3 to a 7, or vice versa. The problem is that a small change in weights from `x_old` to `x_new` isn't likely to cause any prediction to change, so `(y_new - y_old)` will almost always be 0. In other words, the gradient is 0 almost everywhere",

                        "A very small change in the value of a weight will often not actually change the accuracy at all. This means it is not useful to use accuracy as a loss function\u2014if we do, most of the time our gradients will actually be 0"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "The model therefore cannot learn from the gradients equal to zero, and the model\u2019s weights will not update and will not train", 
                    "scoring_type": "simple",
                    "context": [
                        "this means it is not useful to use accuracy as a loss function\u2014if we do, most of the time our gradients will actually be 0, and the model will not be able to learn from that number",

                        "In mathematical terms, accuracy is a function that is constant almost everywhere (except at the threshold, 0.5), so its derivative is nil almost everywhere (and infinity at the threshold). This then gives gradients that are 0 or infinite, which are useless for updating the model."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "A good loss function gives a slightly better loss when the model gives slightly better predictions", 
                    "scoring_type": "simple",
                    "context": [
                        "Instead, we need a loss function which, when our weights result in slightly better predictions, gives us a slightly better loss"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Slightly better predictions mean if the model is more confident about the correct prediction", 
                    "scoring_type": "simple",
                    "context": [
                        "You can see that this function returns a lower number when predictions are more accurate, when accurate predictions are more confident (higher absolute values), and when inaccurate predictions are less confident"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 22,
            "question_text": "\"What is the difference between a loss function and a metric?\"",
            "gold_standard_answer": "\"The key difference is that metrics drive human understanding and losses drive automated learning. In order for loss to be useful for training, it needs to have a meaningful derivative. Many metrics, like accuracy are not like that. Metrics instead are the numbers that humans care about, that reflect the performance of the model.\"",
            "answer_context": [
                {
                    "answer_component": "\"The key difference is that metrics drive human understanding and losses drive automated learning. In order for loss to be useful for training, it needs to have a meaningful derivative. Many metrics, like accuracy are not like that. Metrics instead are the numbers that humans care about, that reflect the performance of the model.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "The key difference is that the metric is to drive human understanding and the loss is to drive automated learning. To drive automated learning, the loss must be a function that has a meaningful derivative. It can't have big flat sections and large jumps, but instead must be reasonably smooth. This is why we designed a loss function that would respond to small changes in confidence level. This requirement means that sometimes it does not really reflect exactly what we are trying to achieve, but is rather a compromise between our real goal and a function that can be optimized using its gradient. The loss function is calculated for each item in our dataset, and then at the end of an epoch the loss values are all averaged and the overall mean is reported for the epoch."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 23,
            "question_text": "\"What is the function to calculate new weights using a learning rate?\"",
            "gold_standard_answer": "\"The optimizer step function\"",
            "answer_context": [
                {
                    "answer_component": "\"The optimizer step function\"", 
                    "scoring_type": "simple",
                    "context": [
                        "This is known as *stepping* your parameters, using an *optimizer step*"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "calculate new weights using a learning rate", 
                    "context": [
                        "Notice how we _subtract_ the `gradient * lr` from the parameter to update it"
                    ]
                }
            ]
        },
        {
            "chapter": 4,
            "question_number": 24,
            "question_text": "\"What does the DataLoader class do?\"",
            "gold_standard_answer": "\"The DataLoader class can take any Python collection and turn it into an iterator over many batches.\"",
            "answer_context": [
                {
                    "answer_component": "\"The DataLoader class can take any Python collection and turn it into an iterator over many batches.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "A `DataLoader` can take any Python collection and turn it into an iterator over mini-batches, like so:"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 27,
            "question_text": "\"What does view do in PyTorch?\"",
            "gold_standard_answer": "\"It changes the shape of a Tensor without changing its contents.\"",
            "answer_context": [
                {
                    "answer_component": "\"It changes the shape of a Tensor without changing its contents.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "changes the shape of a tensor without changing its contents"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "\"What does view do in PyTorch?\"", 
                    "context": [
                        "We can do this using `view`, which is a PyTorch method"
                    ]
                }
            ]
        },
        {
            "chapter": 4,
            "question_number": 28,
            "question_text": "\"What are the \"bias\" parameters in a neural network? Why do we need them?\"",
            "gold_standard_answer": "\"Without the bias parameters, if the input is zero, the output will always be zero. Therefore, using bias parameters adds additional flexibility to the model.\"",
            "answer_context": [
                {
                    "answer_component": "\"Without the bias parameters, if the input is zero, the output will always be zero. Therefore, using bias parameters adds additional flexibility to the model.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "The function `weights*pixels` won't be flexible enough\u2014it is always equal to 0 when the pixels are equal to 0 (i.e., its *intercept* is 0). You might remember from high school math that the formula for a line is `y=w*x+b`; we still need the `b`"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "\"What are the \"bias\" parameters in a neural network?\"", 
                    "context": [
                        "In neural networks, the `w` in the equation `y=w*x+b` is called the *weights*, and the `b` is called the *bias*. Together, the weights and bias make up the *parameters*."
                    ]
                }
            ]
        },
        {
            "chapter": 4,
            "question_number": 29,
            "question_text": "\"What does the @ operator do in Python?\"",
            "gold_standard_answer": "\"This is the matrix multiplication operator.\"",
            "answer_context": [
                {
                    "answer_component": "\"This is the matrix multiplication operator.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "In Python, matrix multiplication is represented with the `@` operator. Let's try it:"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 30,
            "question_text": "\"What does the backward method do?\"",
            "gold_standard_answer": "\"This method returns the current gradients.\"",
            "answer_context": [
                {
                    "answer_component": "\"This method returns the current gradients.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "|Backward pass | Computing the gradients of the loss with respect to all model parameters.",

                        "To calculate the gradients we call `backward` on the `loss`",

                        "The \"backward\" here refers to *backpropagation*, which is the name given to the process of calculating the derivative of each layer. We'll see how this is done exactly in chapter <<chapter_foundations>>, when we calculate the gradients of a deep neural net from scratch. This is called the \"backward pass\" of the network"
                    ],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 31,
            "question_text": "\"Why do we have to zero the gradients?\"",
            "gold_standard_answer": "\"PyTorch will add the gradients of a variable to any previously stored gradients. If the training loop function is called multiple times, without zeroing the gradients, the gradient of current loss would be added to the previously stored gradient value.\"",
            "answer_context": [
                {
                    "answer_component": "PyTorch will add the gradients of a variable to any previously stored gradients. If the training loop function is called multiple times, without zeroing the gradients, the gradient of current loss would be added to the previously stored gradient value", 
                    "scoring_type": "simple",
                    "context": [
                        "The gradients have changed! The reason for this is that `loss.backward` actually *adds* the gradients of `loss` to any gradients that are currently stored"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "\"Why do we have to zero the gradients?\"", 
                    "context": [
                        "So, we have to set the current gradients to 0 first:"
                    ]
                }
            ]
        },
        {
            "chapter": 4,
            "question_number": 32,
            "question_text": "\"What information do we have to pass to Learner?\"",
            "gold_standard_answer": "\"We need to pass in the DataLoaders, the model, the optimization function, the loss function, and optionally any metrics to print.\"",
            "answer_context": [
                {
                    "answer_component": "\"We need to pass in the DataLoaders, the model, the optimization function, the loss function, and optionally any metrics to print.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "the `DataLoaders`, the model, the optimization function (which will be passed the parameters), the loss function, and optionally any metrics to print"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "pass to Learner", 
                    "context": [
                        "To create a `Learner` without using an application (such as `vision_learner`) we need to pass in all the elements that we've created in this chapter: "
                    ]
                }
            ]
        },
        {
            "chapter": 4,
            "question_number": 34,
            "question_text": "\"What is \"ReLU\"? Draw a plot of it for values from -2 to +2.\"",
            "gold_standard_answer": "\"ReLU just means \u201creplace any negative numbers with zero\u201d. It is a commonly used activation function.\"",
            "answer_context": [
                {
                    "answer_component":  "\"ReLU just means \u201creplace any negative numbers with zero\u201d. It is a commonly used activation function.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "That little function `res.max(tensor(0.0))` is called a *rectified linear unit*, also known as *ReLU*. We think we can all agree that *rectified linear unit* sounds pretty fancy and complicated... But actually, there's nothing more to it than `res.max(tensor(0.0))`\u2014in other words, replace every negative number with a zero",

                        "Function that returns 0 for negative numbers and doesn't change positive numbers"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 35,
            "question_text": "\"What is an \"activation function\"?\"",
            "gold_standard_answer": "\"The activation function is another function that is part of the neural network, which has the purpose of providing non-linearity to the model. The idea is that without an activation function, we just have multiple linear functions of the form y=mx+b. However, a series of linear layers is equivalent to a single linear layer, so our model can only fit a line to the data. By introducing a non-linearity in between the linear layers, this is no longer true. Each layer is somewhat decoupled from the rest of the layers, and the model can now fit much more complex functions. In fact, it can be mathematically proven that such a model can solve any computable problem to an arbitrarily high accuracy, if the model is large enough with the correct weights. This is known as the universal approximation theorem.\"",
            "answer_context": [
                {
                    "answer_component": "The activation function is another function that is part of the neural network, which has the purpose of providing non-linearity to the model", 
                    "scoring_type": "simple",
                    "context": [
                        "the second line of code is known variously as a *nonlinearity*, or *activation function*."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "a series of linear layers is equivalent to a single linear layer, so our model can only fit a line to the data", 
                    "scoring_type": "simple",
                    "context": [
                        "The basic idea is that by using more linear layers, we can have our model do more computation, and therefore model more complex functions. But there's no point just putting one linear layer directly after another one, because when we multiply things together and then add them up multiple times, that could be replaced by multiplying different things together and adding them up just once! That is to say, a series of any number of linear layers in a row can be replaced with a single linear layer with a different set of parameters.",

                        "> S: Mathematically, we say the composition of two linear functions is another linear function. So, we can stack as many linear classifiers as we want on top of each other, and without nonlinear functions between them, it will just be the same as one linear classifier."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "By introducing a non-linearity in between the linear layers, this is no longer true. Each layer is somewhat decoupled from the rest of the layers, and the model can now fit much more complex functions", 
                    "scoring_type": "simple",
                    "context": [
                        "But if we put a nonlinear function between them, such as `max`, then this is no longer true. Now each linear layer is actually somewhat decoupled from the other ones, and can do its own useful work"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "In fact, it can be mathematically proven that such a model can solve any computable problem to an arbitrarily high accuracy, if the model is large enough with the correct weights. This is known as the universal approximation theorem", 
                    "scoring_type": "simple",
                    "context": [
                        "Amazingly enough, it can be mathematically proven that this little function can solve any computable problem to an arbitrarily high level of accuracy, if you can find the right parameters for `w1` and `w2` and if you make these matrices big enough. For any arbitrarily wiggly function, we can approximate it as a bunch of lines joined together; to make it closer to the wiggly function, we just have to use shorter lines. This is known as the *universal approximation theorem*"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "true"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 36,
            "question_text": "\"What's the difference between F.relu and nn.ReLU?\"",
            "gold_standard_answer": "\"F.relu is a Python function for the relu activation function. On the other hand, nn.ReLU is a PyTorch module. This means that it is a Python class that can be called as a function in the same way as F.relu.\"",
            "answer_context": [
                {
                    "answer_component": "\"F.relu is a Python function for the relu activation function. On the other hand, nn.ReLU is a PyTorch module. This means that it is a Python class that can be called as a function in the same way as F.relu.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "`nn.ReLU` is a PyTorch module that does exactly the same thing as the `F.relu` function. Most functions that can appear in a model also have identical forms that are modules. Generally, it's just a case of replacing `F` with `nn` and changing the capitalization. When using `nn.Sequential`, PyTorch requires us to use the module version. Since modules are classes, we have to instantiate them, which is why you see `nn.ReLU()` in this example."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 4,
            "question_number": 37,
            "question_text": "\"The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more?\"",
            "gold_standard_answer": "\"There are practical performance benefits to using more than one nonlinearity. We can use a deeper model with less number of parameters, better performance, faster training, and less compute/memory requirements.\"",
            "answer_context": [
                {
                    "answer_component": "\"There are practical performance benefits to using more than one nonlinearity. We can use a deeper model with less number of parameters, better performance, faster training, and less compute/memory requirements.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "The reason is performance. With a deeper model (that is, one with more layers) we do not need to use as many parameters; it turns out that we can use smaller matrices with more layers, and get better results than we would get with larger matrices, and few layers."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "\"The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more?\"", 
                    "context": [
                        "We already know that a single nonlinearity with two linear layers is enough to approximate any function. So why would we use deeper models? "
                    ]
                }
            ]
        },
        {
            "chapter": 8,
            "question_number": 1,
            "question_text": "\"What problem does collaborative filtering solve?\"",
            "gold_standard_answer": "\"It solves the problem of predicting the interests of users based on the interests of other users and recommending items based on these interests.\"",
            "answer_context": [
                {
                    "answer_component": "\"It solves the problem of predicting the interests of users based on the interests of other users and recommending items based on these interests.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "look at what products the current user has used or liked, find other users that have used or liked similar products, and then recommend other products that those users have used or liked."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "collaborative filtering", 
                    "context": [
                        "There is a general solution to this problem, called *collaborative filtering*"
                    ]
                }
            ]
        },
        {
            "chapter": 8,
            "question_number": 3,
            "question_text": "\"Why might a collaborative filtering predictive model fail to be a very useful recommendation system?\"",
            "gold_standard_answer": "\"If there are not many recommendations to learn from, or enough data about the user to provide useful recommendations, then such collaborative filtering systems may not be useful.\"",
            "answer_context": [
                {
                    "answer_component": "\"If there are not many recommendations to learn from, or enough data about the user to provide useful recommendations, then such collaborative filtering systems may not be useful.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "The most extreme version of this problem is when you have no users, and therefore no history to learn from. What products do you recommend to your very first user?"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "\"Why might a collaborative filtering predictive model fail to be a very useful recommendation system?\"", 
                    "context": [
                        "The biggest challenge with using collaborative filtering models in practice is the *bootstrapping problem*. "
                    ]
                }
            ]
        },
        {
            "chapter": 8,
            "question_number": 6,
            "question_text": "\"What is a latent factor? Why is it \"latent\"?\"",
            "gold_standard_answer": "\"As described above, a latent factor are factors that are important for the prediction of the recommendations, but are not explicitly given to the model and instead learned (hence \u201clatent\u201d).\"",
            "answer_context": [
                {
                    "answer_component": "a latent factor are factors that are important for the prediction of the recommendations", 
                    "scoring_type": "simple",
                    "context": [
                        "In the Netflix example, we started with the assumption that you like old, action-packed sci-fi movies. But you never actually told Netflix that you like these kinds of movies. And Netflix never actually needed to add columns to its movies table saying which movies are of these types. Still, there must be some underlying concept of sci-fi, action, and movie age, and these concepts must be relevant for at least some people's movie watching decisions."
                    ],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "but are not explicitly given to the model and instead learned (hence \u201clatent\u201d)", 
                    "scoring_type": "simple",
                    "context": [
                        "Since we don't know what the latent factors actually are, and we don't know how to score them for each user and movie, we should learn them."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "What is a latent factor?", 
                    "context": [
                        "The key foundational idea is that of *latent factors*."
                    ]
                },
                {
                    "question_component": "\"Why is it \"latent\"?\"", 
                    "context": [
                        "Since we don't know what the latent factors actually are"
                    ]
                }
            ]
        },
        {
            "chapter": 8,
            "question_number": 7,
            "question_text": "\"What is a dot product? Calculate a dot product manually using pure Python with lists.\"",
            "gold_standard_answer": "\"A dot product is when you multiply the corresponding elements of two vectors and add them up. If we represent the vectors as lists of the same size, here is how we can perform a dot product:\n\na = [1, 2, 3, 4]\nb = [5, 6, 7, 8]\ndot_product = sum(i[0]*i[1] for i in zip(a,b))\"",
            "answer_context": [
                {
                    "answer_component": "A dot product is when you multiply the corresponding elements of two vectors and add them up", 
                    "scoring_type": "simple",
                    "context": [
                        "> jargon: dot product: The mathematical operation of multiplying the elements of two vectors together, and then summing up the result."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "If we represent the vectors as lists of the same size, here is how we can perform a dot product:\n\na = [1, 2, 3, 4]\nb = [5, 6, 7, 8]\ndot_product = sum(i[0]*i[1] for i in zip(a,b))", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "true"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 8,
            "question_number": 9,
            "question_text": "\"What is an embedding matrix?\"",
            "gold_standard_answer": "\"It is what you multiply an embedding with, and in the case of this collaborative filtering problem, is learned through training.\"",
            "answer_context": [
                {
                    "answer_component": "It is what you multiply an embedding with", 
                    "scoring_type": "simple",
                    "context": [
                        "The thing that you multiply the one-hot-encoded matrix by (or, using the computational shortcut, index into directly) is called the _embedding matrix_."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "in the case of this collaborative filtering problem, is learned through training", 
                    "scoring_type": "simple",
                    "context": [
                        "How do we determine numbers to characterize those? The answer is, we don't. We will let our model *learn* them. By analyzing the existing relations between users and movies, our model can figure out itself the features that seem important or not."
                    ],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 8,
            "question_number": 10,
            "question_text": "\"What is the relationship between an embedding and a matrix of one-hot-encoded vectors?\"",
            "gold_standard_answer": "\"An embedding is a matrix of one-hot encoded vectors that is computationally more efficient.\"",
            "answer_context": [
                {
                    "answer_component": "\"An embedding is a matrix of one-hot encoded vectors that is computationally more efficient.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "> jargon: Embedding: Multiplying by a one-hot-encoded matrix, using the computational shortcut that it can be implemented by simply indexing directly. This is quite a fancy word for a very simple concept. The thing that you multiply the one-hot-encoded matrix by (or, using the computational shortcut, index into directly) is called the _embedding matrix_."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 8,
            "question_number": 11,
            "question_text": "\"Why do we need Embedding if we could use one-hot-encoded vectors for the same thing?\"",
            "gold_standard_answer": "\"Embedding is computationally more efficient. The multiplication with one-hot encoded vectors is equivalent to indexing into the embedding matrix, and the Embedding layer does this. However, the gradient is calculated such that it is equivalent to the multiplication with the one-hot encoded vectors.\"",
            "answer_context": [
                {
                    "answer_component": "\"Embedding is computationally more efficient. The multiplication with one-hot encoded vectors is equivalent to indexing into the embedding matrix, and the Embedding layer does this. However, the gradient is calculated such that it is equivalent to the multiplication with the one-hot encoded vectors.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "If we do that for a few indices at once, we will have a matrix of one-hot-encoded vectors, and that operation will be a matrix multiplication! This would be a perfectly acceptable way to build models using this kind of architecture, except that it would use a lot more memory and time than necessary. We know that there is no real underlying reason to store the one-hot-encoded vector, or to search through it to find the occurrence of the number one\u2014we should just be able to index into an array directly with an integer. Therefore, most deep learning libraries, including PyTorch, include a special layer that does just this; it indexes into a vector using an integer, but has its derivative calculated in such a way that it is identical to what it would have been if it had done a matrix multiplication with a one-hot-encoded vector. This is called an *embedding*."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 8,
            "question_number": 12,
            "question_text": "\"What does an embedding contain before we start training (assuming we're not using a pretained model)?\"",
            "gold_standard_answer": "\"The embedding is randomly initialized.\"",
            "answer_context": [
                {
                    "answer_component": "\"The embedding is randomly initialized.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "This is what embeddings are. We will attribute to each of our users and each of our movies a random vector of a certain length (here, `n_factors=5`), and we will make those learnable parameters",

                        "So far, we've used `Embedding` without thinking about how it really works. Let's re-create `DotProductBias` *without* using this class. We'll need a randomly initialized weight matrix for each of the embeddings"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 8,
            "question_number": 14,
            "question_text": "\"What does x[:,0] return?\"",
            "gold_standard_answer": "\"The user ids\"",
            "answer_context": [
                {
                    "answer_component": "\"The user ids\"", 
                    "scoring_type": "simple",
                    "context": [
                        "the first column (`x[:, 0]`) contains the user IDs"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 8,
            "question_number": 15,
            "question_text": "\"Rewrite the DotProduct class (without peeking, if possible!) and train a model with it.\"",
            "gold_standard_answer": "\"class DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        return sigmoid_range((users * movies).sum(dim=1), *self.y_range)\"",
            "answer_context": [
                {
                    "answer_component": "\"class DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        return sigmoid_range((users * movies).sum(dim=1), *self.y_range)\"", 
                    "scoring_type": "simple",
                    "context": [
                        "class DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        return (users * movies).sum(dim=1)",

                        "class DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        return sigmoid_range((users * movies).sum(dim=1), *self.y_range)"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 8,
            "question_number": 16,
            "question_text": "\"What is a good loss function to use for MovieLens? Why?\"",
            "gold_standard_answer": "\"We can use Mean Squared Error (MSE), which is a perfectly reasonable loss as we have numerical targets for the ratings and it is one possible way of representing the accuracy of the model.\"",
            "answer_context": [
                {
                    "answer_component": "\"We can use Mean Squared Error (MSE), which is a perfectly reasonable loss as we have numerical targets for the ratings and it is one possible way of representing the accuracy of the model.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Step 3 is to calculate our loss. We can use any loss function that we wish; let's pick mean squared error for now, since that is one reasonable way to represent the accuracy of a prediction."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 8,
            "question_number": 18,
            "question_text": "\"What is the use of bias in a dot product model?\"",
            "gold_standard_answer": "\"A bias will compensate for the fact that some movies are just amazing or pretty bad. It will also compensate for users who often have more positive or negative recommendations in general.\"",
            "answer_context": [
                {
                    "answer_component": "\"A bias will compensate for the fact that some movies are just amazing or pretty bad. It will also compensate for users who often have more positive or negative recommendations in general.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "This is a reasonable start, but we can do better. One obvious missing piece is that some users are just more positive or negative in their recommendations than others, and some movies are just plain better or worse than others. But in our dot product representation we do not have any way to encode either of these things. If all you can say about a movie is, for instance, that it is very sci-fi, very action-oriented, and very not old, then you don't really have any way to say whether most people like it."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"A bias will compensate for the fact that some movies are just amazing or pretty bad. It will also compensate for users who often have more positive or negative recommendations in general.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "That's because at this point we only have weights; we do not have biases. If we have a single number for each user that we can add to our scores, and ditto for each movie, that will handle this missing piece very nicely. So first of all, let's adjust our model architecture:"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 8,
            "question_number": 19,
            "question_text": "\"What is another name for weight decay?\"",
            "gold_standard_answer": "\"L2 regularization\"",
            "answer_context": [
                {
                    "answer_component": "\"L2 regularization\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Weight decay, or *L2 regularization*"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 8,
            "question_number": 20,
            "question_text": "\"Write the equation for weight decay (without peeking!).\"",
            "gold_standard_answer": "\"loss_with_wd = loss + wd * (parameters**2).sum()\"",
            "answer_context": [
                {
                    "answer_component": "\"loss_with_wd = loss + wd * (parameters**2).sum()\"", 
                    "scoring_type": "simple",
                    "context": [
                        "loss_with_wd = loss + wd * (parameters**2).sum()"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 8,
            "question_number": 21,
            "question_text": "\"Write the equation for the gradient of weight decay. Why does it help reduce weights?\"",
            "gold_standard_answer": "\"We add to the gradients 2*wd*parameters. This helps create more shallow, less bumpy/sharp surfaces that generalize better and prevents overfitting.\"",
            "answer_context": [
                {
                    "answer_component": "We add to the gradients 2*wd*parameters", 
                    "scoring_type": "simple",
                    "context": [
                        "parameters.grad += wd * 2 * parameters"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "bumpy/sharp surfaces", 
                    "scoring_type": "simple",
                    "context": [
                        "letting our model learn high parameters might cause it to fit all the data points in the training set with an overcomplex function that has very sharp changes, which will lead to overfitting"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "generalize better and prevents overfitting", 
                    "scoring_type": "simple",
                    "context": [
                        "Limiting our weights from growing too much is going to hinder the training of the model, but it will yield a state where it generalizes better"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 8,
            "question_number": 22,
            "question_text": "\"Why does reducing weights lead to better generalization?\"",
            "gold_standard_answer": "\"This will result is more shallow, less sharp surfaces. If sharp surfaces are allowed, it can very easly overfit, and now this is prevented.\"",
            "answer_context": [
                {
                    "answer_component": "\"This will result is more shallow, less sharp surfaces. If sharp surfaces are allowed, it can very easly overfit, and now this is prevented.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "So, letting our model learn high parameters might cause it to fit all the data points in the training set with an overcomplex function that has very sharp changes, which will lead to overfitting"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"This will result is more shallow, less sharp surfaces. If sharp surfaces are allowed, it can very easly overfit, and now this is prevented.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Limiting our weights from growing too much is going to hinder the training of the model, but it will yield a state where it generalizes better"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 8,
            "question_number": 24,
            "question_text": "\"Does sorting the movie biases give the same result as averaging overall movie ratings by movie? Why/why not?\"",
            "gold_standard_answer": "\"No it means much more than that. It takes into account the genres or actors or other factors. For example, movies with low bias means even if you like these types of movies you may not like this movie (and vice versa for movies with high bias).\"",
            "answer_context": [
                {
                    "answer_component": "\"No it means much more than that. It takes into account the genres or actors or other factors. For example, movies with low bias means even if you like these types of movies you may not like this movie (and vice versa for movies with high bias).\"", 
                    "scoring_type": "simple",
                    "context": [
                        "We could have simply sorted the movies directly by their average rating, but looking at the learned bias tells us something much more interesting. It tells us not just whether a movie is of a kind that people tend not to enjoy watching, but that people tend not to like watching it even if it is of a kind that they would otherwise enjoy"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 8,
            "question_number": 25,
            "question_text": "\"How do you print the names and details of the layers in a model?\"",
            "gold_standard_answer": "\"Just by typing learn.model\"",
            "answer_context": [
                {
                    "answer_component": "\"Just by typing learn.model\"", 
                    "scoring_type": "simple",
                    "context": [
                        "learn.model"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "\"How do you print the names and details of the layers in a model?\"", 
                    "context": [
                        "The names of the layers can be seen by printing the model:"
                    ]
                }
            ]
        },
        {
            "chapter": 8,
            "question_number": 26,
            "question_text": "\"What is the \"bootstrapping problem\" in collaborative filtering?\"",
            "gold_standard_answer": "\"That the model / system cannot make any recommendations or draw any inferences for users or items about which it has not yet gathered sufficient information. It\u2019s also called the cold start problem.\"",
            "answer_context": [
                {
                    "answer_component": "\"That the model / system cannot make any recommendations or draw any inferences for users or items about which it has not yet gathered sufficient information. It\u2019s also called the cold start problem.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "The biggest challenge with using collaborative filtering models in practice is the *bootstrapping problem*. The most extreme version of this problem is when you have no users, and therefore no history to learn from. What products do you recommend to your very first user?"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 8,
            "question_number": 27,
            "question_text": "\"How could you deal with the bootstrapping problem for new users? For new movies?\"",
            "gold_standard_answer": "\"You could solve this by coming up with an average embedding for a user or movie. Or select a particular user/movie to represent the average user/movie. Additionally, you could come up with some questions that could help initialize the embedding vectors for new users and movies.\"",
            "answer_context": [
                {
                    "answer_component": "You could solve this by coming up with an average embedding for a user or movie. Or select a particular user/movie to represent the average user/movie", 
                    "scoring_type": "simple",
                    "context": [
                        "You could assign new users the mean of all of the embedding vectors of your other users, but this has the problem that that particular combination of latent factors may be not at all common (for instance, the average for the science-fiction factor may be high, and the average for the action factor may be low, but it is not that common to find people who like science-fiction without action). Better would probably be to pick some particular user to represent *average taste*"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "Additionally, you could come up with some questions that could help initialize the embedding vectors for new users and movies", 
                    "scoring_type": "simple",
                    "context": [
                        "Better still is to use a tabular model based on user meta data to construct your initial embedding vector. When a user signs up, think about what questions you could ask them that could help you to understand their tastes. Then you can create a model where the dependent variable is a user's embedding vector, and the independent variables are the results of the questions that you ask them, along with their signup metadata. We will see in the next section how to create these kinds of tabular models. (You may have noticed that when you sign up for services such as Pandora and Netflix, they tend to ask you a few questions about what genres of movie or music you like; this is how they come up with your initial collaborative filtering recommendations.)"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 8,
            "question_number": 28,
            "question_text": "\"How can feedback loops impact collaborative filtering systems?\"",
            "gold_standard_answer": "\"The recommendations may suffer from representation bias where a small number of people influence the system heavily. E.g.: Highly enthusiastic anime fans who rate movies much more frequently than others may cause the system to recommend anime more often than expected (incl. to non-anime fans).\"",
            "answer_context": [
                {
                    "answer_component": "\"The recommendations may suffer from representation bias where a small number of people influence the system heavily. E.g.: Highly enthusiastic anime fans who rate movies much more frequently than others may cause the system to recommend anime more often than expected (incl. to non-anime fans).\"", 
                    "scoring_type": "simple",
                    "context": [
                        "One thing to be careful of is that a small number of extremely enthusiastic users may end up effectively setting the recommendations for your whole user base. This is a very common problem, for instance, in movie recommendation systems. People that watch anime tend to watch a whole lot of it, and don't watch very much else, and spend a lot of time putting their ratings on websites. As a result, anime tends to be heavily overrepresented in a lot of *best ever movies* lists. In this particular case, it can be fairly obvious that you have a problem of representation bias"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "feedback loops", 
                    "context": [
                        "Such a problem can change the entire makeup of your user base, and the behavior of your system. This is particularly true because of positive feedback loops. "
                    ]
                }
            ]
        },
        {
            "chapter": 8,
            "question_number": 29,
            "question_text": "\"When using a neural network in collaborative filtering, why can we have different numbers of factors for movies and users?\"",
            "gold_standard_answer": "\"In this case, we are not taking the dot product but instead concatenating the embedding matrices, so the number of factors can be different.\"",
            "answer_context": [
                {
                    "answer_component": "\"In this case, we are not taking the dot product but instead concatenating the embedding matrices, so the number of factors can be different.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Since we'll be concatenating the embeddings, rather than taking their dot product, the two embedding matrices can have different sizes (i.e., different numbers of latent factors). fastai has a function `get_emb_sz` that returns recommended sizes for embedding matrices for your data, based on a heuristic that fast.ai has found tends to work well in practice:"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "When using a neural network", 
                    "context": [
                        "To turn our architecture into a deep learning model, the first step is to take the results of the embedding lookup and concatenate those activations together. This gives us a matrix which we can then pass through linear layers and nonlinearities in the usual way."
                    ]
                }
            ]
        },
        {
            "chapter": 8,
            "question_number": 31,
            "question_text": "\"What kind of model should we use if we want to add metadata about users and items, or information such as date and time, to a collaborative filtering model?\"",
            "gold_standard_answer": "\"We should use a tabular model, which is discussed in the next chapter!\"",
            "answer_context": [
                {
                    "answer_component": "We should use a tabular model", 
                    "scoring_type": "simple",
                    "context": [
                        "Better still is to use a tabular model based on user meta data to construct your initial embedding vector. When a user signs up, think about what questions you could ask them that could help you to understand their tastes. Then you can create a model where the dependent variable is a user's embedding vector, and the independent variables are the results of the questions that you ask them, along with their signup metadata. We will see in the next section how to create these kinds of tabular models. (You may have noticed that when you sign up for services such as Pandora and Netflix, they tend to ask you a few questions about what genres of movie or music you like; this is how they come up with your initial collaborative filtering recommendations.)"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 9,
            "question_number": 1,
            "question_text": "\"What is a continuous variable?\"",
            "gold_standard_answer": "\"This refers to numerical variables that have have a wide range of \u201ccontinuous\u201d values (ex: age)\"",
            "answer_context": [
                {
                    "answer_component": "\"This refers to numerical variables that have have a wide range of \u201ccontinuous\u201d values (ex: age)\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Continuous variables are numerical data, such as \"age,\" that can be directly fed to the model, since you can add and multiply them directly"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 9,
            "question_number": 2,
            "question_text": "\"What is a categorical variable?\"",
            "gold_standard_answer": "\"This refers to variables that can take on discrete levels that correspond to different categories.\"",
            "answer_context": [
                {
                    "answer_component": "\"This refers to variables that can take on discrete levels that correspond to different categories.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Categorical variables contain a number of discrete levels, such as \"movie ID,\" for which addition and multiplication don't have meaning (even if they're stored as numbers)"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 9,
            "question_number": 3,
            "question_text": "\"Provide two of the words that are used for the possible values of a categorical variable.\"",
            "gold_standard_answer": "\"Levels or categories\"",
            "answer_context": [
                {
                    "answer_component": "\"Levels or categories\"", 
                    "scoring_type": "simple",
                    "context": [
                        "For a categorical variable we call the possible values of the variable its \"levels\" (or \"categories\" or \"classes\")"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 9,
            "question_number": 4,
            "question_text": "\"What is a \"dense layer\"?\"",
            "gold_standard_answer": "\"Equivalent to what we call linear layers.\"",
            "answer_context": [
                {
                    "answer_component": "\"Equivalent to what we call linear layers.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Note that \"dense layer\" is a term with the same meaning as \"linear layer,\""
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 9,
            "question_number": 6,
            "question_text": "\"What kinds of datasets are entity embeddings especially useful for?\"",
            "gold_standard_answer": "\"It is especially useful for datasets with features that have high levels of cardinality (the features have lots of possible categories). Other methods often overfit to data like this.\"",
            "answer_context": [
                {
                    "answer_component": "\"It is especially useful for datasets with features that have high levels of cardinality (the features have lots of possible categories). Other methods often overfit to data like this.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "[It] is especially useful for datasets with lots of high cardinality features, where other methods tend to overfit"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "entity embeddings", 
                    "context": [
                        "Entity embedding not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables."
                    ]
                }
            ]
        },
        {
            "chapter": 9,
            "question_number": 7,
            "question_text": "\"What are the two main families of machine learning algorithms?\"",
            "gold_standard_answer": "\"Ensemble of decision trees are best for structured (tabular data)\nMultilayered neural networks are best for unstructured data (audio, vision, text, etc.)\"",
            "answer_context": [
                {
                    "answer_component": "\"Ensemble of decision trees are best for structured (tabular data)\nMultilayered neural networks are best for unstructured data (audio, vision, text, etc.)\"", 
                    "scoring_type": "simple",
                    "context": [
                        "1. Ensembles of decision trees (i.e., random forests and gradient boosting machines), mainly for structured data (such as you might find in a database table at most companies)\n1. Multilayered neural networks learned with SGD (i.e., shallow and/or deep learning), mainly for unstructured data (such as audio, images, and natural language)"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "\"What are the two main families of machine learning algorithms?\"", 
                    "context": [
                        "The good news is that modern machine learning can be distilled down to a couple of key techniques that are widely applicable. Recent studies have shown that the vast majority of datasets can be best modeled with just two methods:"
                    ]
                }
            ]
        },
        {
            "chapter": 9,
            "question_number": 8,
            "question_text": "\"Why do some categorical columns need a special ordering in their classes? How do you do this in Pandas?\"",
            "gold_standard_answer": "\"Ordinal categories may inherently have some order and by using set_categories with the argument ordered=True and passing in the ordered list, this information represented in the pandas DataFrame.\"",
            "answer_context": [
                {
                    "answer_component": "\"Ordinal categories may inherently have some order\"", 
                    "scoring_type": "simple",
                    "context": [
                        "At this point, a good next step is to handle *ordinal columns*. This refers to columns containing strings or similar, but where those strings have a natural ordering"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"by using set_categories with the argument ordered=True and passing in the ordered list, this information represented in the pandas DataFrame.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "set_categories(sizes, ordered=True, inplace=True)"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 9,
            "question_number": 9,
            "question_text": "\"Summarize what a decision tree algorithm does.\"",
            "gold_standard_answer": "\"The basic idea of what a decision tree algorithm does is to determine how to group the data based on \u201cquestions\u201d that we ask about the data. That is, we keep splitting the data based on the levels or values of the features and generate predictions based on the average target value of the data points in that group. Here is the algorithm:\n\nLoop through each column of the dataset in turn\nFor each column, loop through each possible level of that column in turn\nTry splitting the data into two groups, based on whether they are greater than or less than that value (or if it is a categorical variable, based on whether they are equal to or not equal to that level of that categorical variable)\nFind the average sale price for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group. That is, treat this as a very simple \u201cmodel\u201d where our predictions are simply the average sale price of the item\u2019s group\nAfter looping through all of the columns and possible levels for each, pick the split point which gave the best predictions using our very simple model\nWe now have two different groups for our data, based on this selected split. Treat each of these as separate datasets, and find the best split for each, by going back to step one for each group\nContinue this process recursively, and until you have reached some stopping criterion for each group \u2014 for instance, stop splitting a group further when it has only 20 items in it.\"",
            "answer_context": [
                {
                    "answer_component": "\"The basic idea of what a decision tree algorithm does is to determine how to group the data based on \u201cquestions\u201d that we ask about the data. That is, we keep splitting the data based on the levels or values of the features and generate predictions based on the average target value of the data points in that group. Here is the algorithm:\n\nLoop through each column of the dataset in turn\nFor each column, loop through each possible level of that column in turn\nTry splitting the data into two groups, based on whether they are greater than or less than that value (or if it is a categorical variable, based on whether they are equal to or not equal to that level of that categorical variable)\nFind the average sale price for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group. That is, treat this as a very simple \u201cmodel\u201d where our predictions are simply the average sale price of the item\u2019s group\nAfter looping through all of the columns and possible levels for each, pick the split point which gave the best predictions using our very simple model\nWe now have two different groups for our data, based on this selected split. Treat each of these as separate datasets, and find the best split for each, by going back to step one for each group\nContinue this process recursively, and until you have reached some stopping criterion for each group \u2014 for instance, stop splitting a group further when it has only 20 items in it.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "1. Loop through each column of the dataset in turn.\n1. For each column, loop through each possible level of that column in turn.\n1. Try splitting the data into two groups, based on whether they are greater than or less than that value (or if it is a categorical variable, based on whether they are equal to or not equal to that level of that categorical variable).\n1. Find the average sale price for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group. That is, treat this as a very simple \"model\" where our predictions are simply the average sale price of the item's group.\n1. After looping through all of the columns and all the possible levels for each, pick the split point that gave the best predictions using that simple model.\n1. We now have two different groups for our data, based on this selected split. Treat each of these as separate datasets, and find the best split for each by going back to step 1 for each group.\n1. Continue this process recursively, until you have reached some stopping criterion for each group\u2014for instance, stop splitting a group further when it has only 20 items in it."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "\"Summarize what a decision tree algorithm does.\"", 
                    "context": [
                        "The basic steps to train a decision tree can be written down very easily:"
                    ]
                }
            ]
        },
        {
            "chapter": 9,
            "question_number": 10,
            "question_text": "\"Why is a date different from a regular categorical or continuous variable, and how can you preprocess it to allow it to be used in a model?\"",
            "gold_standard_answer": "\"Some dates are different to others (ex: some are holidays, weekends, etc.) that cannot be described as just an ordinal variable. Instead, we can generate many different categorical features about the properties of the given date (ex: is it a weekday? is it the end of the month?, etc.)\"",
            "answer_context": [
                {
                    "answer_component": "\"Some dates are different to others (ex: some are holidays, weekends, etc.) that cannot be described as just an ordinal variable. Instead, we can generate many different categorical features about the properties of the given date (ex: is it a weekday? is it the end of the month?, etc.)\"", 
                    "scoring_type": "simple",
                    "context": [
                        "In order to help our algorithm handle dates intelligently, we'd like our model to know more than whether a date is more recent or less recent than another. We might want our model to make decisions based on that date's day of the week, on whether a day is a holiday, on what month it is in, and so forth. To do this, we replace every date column with a set of date metadata columns, such as holiday, day of week, and month. These columns provide categorical data that we suspect will be useful."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"Some dates are different to others (ex: some are holidays, weekends, etc.) that cannot be described as just an ordinal variable\"", 
                    "scoring_type": "simple",
                    "context": [
                        "But how does this apply to a common data type, the date? You might want to treat a date as an ordinal value, because it is meaningful to say that one date is greater than another. However, dates are a bit different from most ordinal values in that some dates are qualitatively different from others in a way that that is often relevant to the systems we are modeling."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 9,
            "question_number": 11,
            "question_text": "\"Should you pick a random validation set in the bulldozer competition? If no, what kind of validation set should you pick?\"",
            "gold_standard_answer": "\"No, the validation set should be as similar to the test set as possible. In this case, the test set contains data from later data, so we should split the data by the dates and include the later dates in the validation set.\"",
            "answer_context": [
                {
                    "answer_component": "\"No\"", 
                    "scoring_type": "simple",
                    "context": [
                        "In some cases, just randomly choosing a subset of your data points will do that. This is not one of those cases, because it is a time series."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"the validation set should be as similar to the test set as possible. In this case, the test set contains data from later data, so we should split the data by the dates and include the later dates in the validation set.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "If you look at the date range represented in the test set, you will discover that it covers a six-month period from May 2012, which is later in time than any date in the training set. This is a good design, because the competition sponsor will want to ensure that a model is able to predict the future. But it means that if we are going to have a useful validation set, we also want the validation set to be later in time than the training set. The Kaggle training data ends in April 2012, so we will define a narrower training dataset which consists only of the Kaggle training data from before November 2011, and we'll define a validation set consisting of data from after November 2011."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 9,
            "question_number": 12,
            "question_text": "\"What is pickle and what is it useful for?\"",
            "gold_standard_answer": "\"Allows you to save nearly any Python object as a file.\"",
            "answer_context": [
                {
                    "answer_component": "\"Allows you to save nearly any Python object as a file.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "fastai provides a `save` method that uses Python's *pickle* system to save nearly any Python object:"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 9,
            "question_number": 13,
            "question_text": "\"How are mse, samples, and values calculated in the decision tree drawn in this chapter?\"",
            "gold_standard_answer": "\"By traversing the tree based on answering questions about the data, we reach the nodes that tell us the average value of the data in that group, the mse, and the number of samples in that group.\"",
            "answer_context": [
                {
                    "answer_component": "\"By traversing the tree based on answering questions about the data, we reach the nodes that tell us the average value of the data in that group, the mse, and the number of samples in that group.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "The top node represents the *initial model* before any splits have been done, when all the data is in one group. This is the simplest possible model. It is the result of asking zero questions and will always predict the value to be the average value of the whole dataset. In this case, we can see it predicts a value of 10.10 for the logarithm of the sales price. It gives a mean squared error of 0.48. The square root of this is 0.69. (Remember that unless you see `m_rmse`, or a *root mean squared error*, then the value you are looking at is before taking the square root, so it is just the average of the square of the differences.) We can also see that there are 404,710 auction records in this group\u2014that is the total size of our training set. The final piece of information shown here is the decision criterion for the best split that was found, which is to split based on the `coupler_system` column."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"By traversing the tree based on answering questions about the data, we reach the nodes that tell us the average value of the data in that group, the mse, and the number of samples in that group.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Moving down and to the left, this node shows us that there were 360,847 auction records for equipment where `coupler_system` was less than 0.5. The average value of our dependent variable in this group is 10.21. Moving down and to the right from the initial model takes us to the records where `coupler_system` was greater than 0.5."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"By traversing the tree based on answering questions about the data, we reach the nodes that tell us the average value of the data in that group, the mse, and the number of samples in that group.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "The bottom row contains our *leaf nodes*: the nodes with no answers coming out of them, because there are no more questions to be answered. At the far right of this row is the node containing records where `coupler_system` was greater than 0.5. The average value here is 9.21, so we can see the decision tree algorithm did find a single binary decision that separated high-value from low-value auction results. Asking only about `coupler_system` predicts an average value of 9.21 versus 10.1."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"By traversing the tree based on answering questions about the data, we reach the nodes that tell us the average value of the data in that group, the mse, and the number of samples in that group.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Returning back to the top node after the first decision point, we can see that a second binary decision split has been made, based on asking whether `YearMade` is less than or equal to 1991.5. For the group where this is true (remember, this is now following two binary decisions, based on `coupler_system` and `YearMade`) the average value is 9.97, and there are 155,724 auction records in this group. For the group of auctions where this decision is false, the average value is 10.4, and there are 205,123 records. So again, we can see that the decision tree algorithm has successfully split our more expensive auction records into two more groups which differ in value significantly."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 9,
            "question_number": 14,
            "question_text": "\"How do we deal with outliers, before building a decision tree?\"",
            "gold_standard_answer": "\"Finding out of domain data (Outliers)\n\nSometimes it is hard to even know whether your test set is distributed in the same way as your training data or, if it is different, then what columns reflect that difference. There\u2019s actually a nice easy way to figure this out, which is to use a random forest!\n\nBut in this case we don\u2019t use a random forest to predict our actual dependent variable. Instead we try to predict whether a row is in the validation set, or the training set.\"",
            "answer_context": [
                {
                    "answer_component": "\"Finding out of domain data (Outliers)\n\nSometimes it is hard to even know whether your test set is distributed in the same way as your training data or, if it is different, then what columns reflect that difference. There\u2019s actually a nice easy way to figure this out, which is to use a random forest\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Sometimes it is hard to know whether your test set is distributed in the same way as your training data, or, if it is different, what columns reflect that difference. There's actually an easy way to figure this out, which is to use a random forest!"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"But in this case we don\u2019t use a random forest to predict our actual dependent variable. Instead we try to predict whether a row is in the validation set, or the training set\"", 
                    "scoring_type": "simple",
                    "context": [
                        "But in this case we don't use the random forest to predict our actual dependent variable. Instead, we try to predict whether a row is in the validation set or the training set. To see this in action, let's combine our training and validation sets together, create a dependent variable that represents which dataset each row comes from, build a random forest using that data, and get its feature importance:"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 9,
            "question_number": 15,
            "question_text": "\"How do we handle categorical variables in a decision tree?\"",
            "gold_standard_answer": "\"We convert the categorical variables to integers, where the integers correspond to the discrete levels of the categorical variable. Apart from that, there is nothing special that needs to be done to get it to work with decision trees (unlike neural networks, where we use embedding layers).\"",
            "answer_context": [
                {
                    "answer_component": "\"We convert the categorical variables to integers, where the integers correspond to the discrete levels of the categorical variable\"", 
                    "scoring_type": "simple",
                    "context": [
                        "The conversion of categorical columns to numbers is done by simply replacing each unique level with a number. The numbers associated with the levels are chosen consecutively as they are seen in a column, so there's no particular meaning to the numbers in categorical columns after conversion"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "(unlike neural networks, where we use embedding layers)", 
                    "scoring_type": "simple",
                    "context": [
                        "Categorical columns are handled very differently in neural networks, compared to decision tree approaches. As we saw in <<chapter_collab>>, in a neural net a great way to handle categorical variables is by using embeddings"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 9,
            "question_number": 16,
            "question_text": "\"What is bagging?\"",
            "gold_standard_answer": "\"Train multiple models on random subsets of the data, and use the ensemble of models for prediction.\"",
            "answer_context": [
                {
                    "answer_component": "\"Train multiple models on random subsets of the data, and use the ensemble of models for prediction.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions... The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests\u2026 show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.",

                        "1. Randomly choose a subset of the rows of your data (i.e., \"bootstrap replicates of your learning set\").\n1. Train a model using this subset.\n1. Save that model, and then return to step 1 a few times.\n1. This will give you a number of trained models. To make a prediction, predict using all of the models, and then take the average of each of those model's predictions."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 9,
            "question_number": 17,
            "question_text": "\"What is the difference between max_samples and max_features when creating a random forest?\"",
            "gold_standard_answer": "\"When training random forests, we train multiple decision trees on random subsets of the data. max_samples defines how many samples, or rows of the tabular dataset, we use for each decision tree. max_features defines how many features, or columns of the tabular dataset, we use for each decision tree.\"",
            "answer_context": [
                {
                    "answer_component": "\"When training random forests, we train multiple decision trees on random subsets of the data\"", 
                    "scoring_type": "simple",
                    "context": [
                        "We can create a random forest just like we created a decision tree, except now, we are also specifying parameters that indicate how many trees should be in the forest, how we should subset the data items (the rows), and how we should subset the fields (the columns)."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"max_samples defines how many samples, or rows of the tabular dataset, we use for each decision tree. max_features defines how many features, or columns of the tabular dataset, we use for each decision tree.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "`max_samples` defines how many rows to sample for training each tree, and `max_features` defines how many columns to sample at each split point"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 9,
            "question_number": 18,
            "question_text": "\"If you increase n_estimators to a very high value, can that lead to overfitting? Why or why not?\"",
            "gold_standard_answer": "\"A higher n_estimators mean more decision trees are being used. However, since the trees are independent of each other, using higher n_estimators does not lead to overfitting.\"",
            "answer_context": [
                {
                    "answer_component": "A higher n_estimators mean more decision trees are being used", 
                    "scoring_type": "simple",
                    "context": [
                        "`n_estimators` defines the number of trees we want"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "However, since the trees are independent of each other, using higher n_estimators does not lead to overfitting", 
                    "scoring_type": "simple",
                    "context": [
                        "Using more trees in a random forest does not lead to overfitting, because each tree is independent of the others"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 9,
            "question_number": 20,
            "question_text": "\"What is \"out-of-bag-error\"?\"",
            "gold_standard_answer": "\"Only use the models not trained on the row of data when going through the data and evaluating the dataset. No validation set is needed.\"",
            "answer_context": [
                {
                    "answer_component": "\"Only use the models not trained on the row of data when going through the data and evaluating the dataset. No validation set is needed.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Recall that in a random forest, each tree is trained on a different subset of the training data. The OOB error is a way of measuring prediction error on the training set by only including in the calculation of a row's error trees where that row was *not* included in training. This allows us to see whether the model is overfitting, without needing a separate validation set"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 9,
            "question_number": 21,
            "question_text": "\"Make a list of reasons why a model's validation set error might be worse than the OOB error. How could you test your hypotheses?\"",
            "gold_standard_answer": "\"The major reason could be because the model does not generalize well. Related to this is the possibility that the validation data has a slightly different distribution than the data the model was trained on.\"",
            "answer_context": [
                {
                    "answer_component": "\"The major reason could be because the model does not generalize well\"", 
                    "scoring_type": "simple",
                    "context": [
                        "We can see that our OOB error is much lower than our validation set error. This means that something else is causing that error, in *addition* to normal generalization error"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"Related to this is the possibility that the validation data has a slightly different distribution than the data the model was trained on.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Sometimes it is hard to know whether your test set is distributed in the same way as your training data, or, if it is different, what columns reflect that difference"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 9,
            "question_number": 23,
            "question_text": "\"What's the purpose of removing unimportant variables?\"",
            "gold_standard_answer": "\"Sometimes, it is better to have a more interpretable model with less features, so removing unimportant variables helps in that regard.\"",
            "answer_context": [
                {
                    "answer_component": "Sometimes, it is better to have a more interpretable model with less features", 
                    "scoring_type": "simple",
                    "context": [
                        "We've found that generally the first step to improving a model is simplifying it\u201478 columns was too many for us to study them all in depth! Furthermore, in practice often a simpler, more interpretable model is easier to roll out and maintain"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "removing unimportant variables", 
                    "scoring_type": "simple",
                    "context": [
                        "removing the variables of low importance and still get good results"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 9,
            "question_number": 24,
            "question_text": "\"What's a good type of plot for showing tree interpreter results?\"",
            "gold_standard_answer": "\"Waterfall plot\"",
            "answer_context": [
                {
                    "answer_component": "\"Waterfall plot\"", 
                    "scoring_type": "simple",
                    "context": [
                        "The clearest way to display the contributions is with a *waterfall plot*"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 9,
            "question_number": 25,
            "question_text": "\"What is the \"extrapolation problem\"?\"",
            "gold_standard_answer": "\"Hard for a model to extrapolate to data that\u2019s outside the domain of the training data. This is particularly important for random forests. On the other hand, neural networks have underlying Linear layers so it could potentially generalize better.\"",
            "answer_context": [
                {
                    "answer_component": "Hard for a model to extrapolate to data that\u2019s outside the domain of the training data. This is particularly important for random forests", 
                    "scoring_type": "simple",
                    "context": [
                        "A problem with random forests, like all machine learning or deep learning algorithms, is that they don't always generalize well to new data",

                        "Remember, a random forest just averages the predictions of a number of trees. And a tree simply predicts the average value of the rows in a leaf. Therefore, a tree and a random forest can never predict values outside of the range of the training data. This is particularly problematic for data where there is a trend over time, such as inflation, and you wish to make predictions for a future time. Your predictions will be systematically too low."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "On the other hand, neural networks could potentially generalize better", 
                    "scoring_type": "simple",
                    "context": [
                        "We will see in which situations neural networks generalize better"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "underlying Linear layers", 
                    "scoring_type": "simple",
                    "context": [ ],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 9,
            "question_number": 26,
            "question_text": "\"How can you tell if your test or validation set is distributed in a different way than your training set?\"",
            "gold_standard_answer": "\"We can do so by training a model to classify if the data is training or validation data. If the data is of different distributions (out-of-domain data), then the model can properly classify between the two datasets.\"",
            "answer_context": [
                {
                    "answer_component": "We can do so by training a model", 
                    "scoring_type": "simple",
                    "context": [
                        "Sometimes it is hard to know whether your test set is distributed in the same way as your training data, or, if it is different, what columns reflect that difference. There's actually an easy way to figure this out, which is to use a random forest!"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "classify if the data is training or validation data", 
                    "scoring_type": "simple",
                    "context": [
                        "But in this case we don't use the random forest to predict our actual dependent variable. Instead, we try to predict whether a row is in the validation set or the training set"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "If the data is of different distributions (out-of-domain data), then the model can properly classify between the two datasets", 
                    "scoring_type": "simple",
                    "context": [
                        "To see this in action, let's combine our training and validation sets together, create a dependent variable that represents which dataset each row comes from, build a random forest using that data, and get its feature importance"
                    ],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "If the data is of different distributions (out-of-domain data), then the model can properly classify between the two datasets", 
                    "scoring_type": "simple",
                    "context": [
                        "This shows that there are three columns that differ significantly between the training and validation sets: `saleElapsed`, `SalesID`, and `MachineID`. It's fairly obvious why this is the case for `saleElapsed`: it's the number of days between the start of the dataset and each row, so it directly encodes the date. The difference in `SalesID` suggests that identifiers for auction sales might increment over time. `MachineID` suggests something similar might be happening for individual items sold in those auctions"
                    ],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 9,
            "question_number": 27,
            "question_text": "\"Why do we ensure saleElapsed is a continuous variable, even although it has less than 9,000 distinct values?\"",
            "gold_standard_answer": "\"This is a variable that changes over time, and since we want our model to extrapolate for future results, we make this a continuous variable.\"",
            "answer_context": [
                {
                    "answer_component": "\"This is a variable that changes over time, and since we want our model to extrapolate for future results, we make this a continuous variable.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "In this case, there's one variable that we absolutely do not want to treat as categorical: the `saleElapsed` variable. A categorical variable cannot, by definition, extrapolate outside the range of values that it has seen, but we want to be able to predict auction sale prices in the future"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 9,
            "question_number": 28,
            "question_text": "\"What is \"boosting\"?\"",
            "gold_standard_answer": "\"We train a model that underfits the dataset, and train subsequent models that predicts the error of the original model. We then add the predictions of all the models to get the final prediction.\"",
            "answer_context": [
                {
                    "answer_component": "\"We train a model that underfits the dataset, and train subsequent models that predicts the error of the original model. We then add the predictions of all the models to get the final prediction.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "- Train a small model that underfits your dataset.\n- Calculate the predictions in the training set for this model.\n- Subtract the predictions from the targets; these are called the \"residuals\" and represent the error for each point in the training set.\n- Go back to step 1, but instead of using the original targets, use the residuals as the targets for the training"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "\"What is \"boosting\"?\"", 
                    "context": [
                        "There is another important approach to ensembling, called *boosting*, where we add models instead of averaging them. Here is how boosting works:"
                    ]
                }
            ]
        },
        {
            "chapter": 9,
            "question_number": 29,
            "question_text": "\"How could we use embeddings with a random forest? Would we expect this to help?\"",
            "gold_standard_answer": "\"Entity embeddings contains richer representations of the categorical features and definitely can improve the performance of other models like random forests. Instead of passing in the raw categorical columns, the entity embeddings can be passed into the random forest model.\"",
            "answer_context": [
                {
                    "answer_component": "Entity embeddings contains richer representations of the categorical features", 
                    "scoring_type": "simple",
                    "context": [
                        "Entity embedding not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "definitely can improve the performance of other models like random forests. Instead of passing in the raw categorical columns, the entity embeddings can be passed into the random forest model", 
                    "scoring_type": "simple",
                    "context": [
                        "This is showing the mean average percent error (MAPE) compared among four different modeling techniques, three of which we have already seen, along with *k*-nearest neighbors (KNN), which is a very simple baseline method. The first numeric column contains the results of using the methods on the data provided in the competition; the second column shows what happens if you first train a neural network with categorical embeddings, and then use those categorical embeddings instead of the raw categorical columns in the model. As you see, in every case, the models are dramatically improved by using the embeddings instead of the raw categories."
                    ],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 9,
            "question_number": 30,
            "question_text": "\"Why might we not always use a neural net for tabular modeling?\"",
            "gold_standard_answer": "\"We might not use them because they are the hardest to train and longest to train, and less well-understood. Instead, random forests should be the first choice/baseline, and neural networks could be tried to improve these results or add to an ensemble.\"",
            "answer_context": [
                {
                    "answer_component": "We might not use them because they are the hardest to train and longest to train, and less well-understood", 
                    "scoring_type": "simple",
                    "context": [
                        "- *Neural networks* take the longest time to train, and require extra preprocessing, such as normalization; this normalization needs to be used at inference time as well. They can provide great results and extrapolate well, but only if you are careful with your hyperparameters and take care to avoid overfitting."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "random forests should be the first choice/baseline", 
                    "scoring_type": "simple",
                    "context": [
                        "We suggest starting your analysis with a random forest"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "neural networks could be tried to improve these results", 
                    "scoring_type": "simple",
                    "context": [
                        "From that foundation, you can try neural nets and GBMs, and if they give you significantly better results on your validation set in a reasonable amount of time, you can use them. If decision tree ensembles are working well for you, try adding the embeddings for the categorical variables to the data, and see if that helps your decision trees learn better."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "or add to an ensemble", 
                    "scoring_type": "simple",
                    "context": [
                        "As we saw earlier, a random forest is itself an ensemble. But we can then include a random forest in *another* ensemble\u2014an ensemble of the random forest and the neural network! While ensembling won't make the difference between a successful and an unsuccessful modeling process, it can certainly add a nice little boost to any models that you have built."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 10,
            "question_number": 1,
            "question_text": "\"What is \"self-supervised learning\"?\"",
            "gold_standard_answer": "\"Training a model without the use of labels. An example is a language model.\"",
            "answer_context": [
                {
                    "answer_component": "\"Training a model without the use of labels. An example is a language model.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "What we call a language model is a model that has been trained to guess what the next word in a text is (having read the ones before). This kind of task is called *self-supervised learning*: we do not need to give labels to our model, just feed it lots and lots of texts",

                        "Self-supervised learning: Training a model using labels that are embedded in the independent variable, rather than requiring external labels"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 10,
            "question_number": 2,
            "question_text": "\"What is a \"language model\"?\"",
            "gold_standard_answer": "\"A language model is a self-supervised model that tries to predict the next word of a given passage of text.\"",
            "answer_context": [
                {
                    "answer_component": "A language model is a self-supervised model", 
                    "scoring_type": "simple",
                    "context": [
                        "What we call a language model is a model that has been trained to guess what the next word in a text is (having read the ones before). This kind of task is called *self-supervised learning*"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "predict the next word of a given passage of text", 
                    "scoring_type": "simple",
                    "context": [
                        "What we call a language model is a model that has been trained to guess what the next word in a text is (having read the ones before)",

                        "a language model predicts the next word of a document"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 10,
            "question_number": 3,
            "question_text": "\"Why is a language model considered self-supervised?\"",
            "gold_standard_answer": "\"There are no labels (ex: sentiment) provided during training. Instead, the model learns to predict the next word by reading lots of provided text with no labels.\"",
            "answer_context": [
                {
                    "answer_component": "\"There are no labels (ex: sentiment) provided during training. Instead, the model learns to predict the next word by reading lots of provided text with no labels.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "What we call a language model is a model that has been trained to guess what the next word in a text is (having read the ones before). This kind of task is called *self-supervised learning*: we do not need to give labels to our model, just feed it lots and lots of texts"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 10,
            "question_number": 4,
            "question_text": "\"What are self-supervised models usually used for?\"",
            "gold_standard_answer": "\"Sometimes, they are used by themselves. For example, a language model can be used for autocomplete algorithms! But often, they are used as a pre-trained model for transfer learning.\"",
            "answer_context": [
                {
                    "answer_component": "\"Sometimes, they are used by themselves. For example, a language model can be used for autocomplete algorithms\"", 
                    "scoring_type": "simple",
                    "context": [
                        "What we call a language model is a model that has been trained to guess what the next word in a text is (having read the ones before). This kind of task is called *self-supervised learning*",

                        "Self-supervised learning: Training a model using labels that are embedded in the independent variable, rather than requiring external labels. For instance, training a model to predict the next word in a text."
                    ],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"But often, they are used as a pre-trained model for transfer learning.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Self-supervised learning is not usually used for the model that is trained directly, but instead is used for pretraining a model used for transfer learning"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 10,
            "question_number": 5,
            "question_text": "\"Why do we fine-tune language models?\"",
            "gold_standard_answer": "\"We can fine-tune the language model on the corpus of the desired downstream task, since the original pre-trained language model was trained on a corpus that is slightly different than the one for the current task.\"",
            "answer_context": [
                {
                    "answer_component": "\"We can fine-tune the language model on the corpus of the desired downstream task, since the original pre-trained language model was trained on a corpus that is slightly different than the one for the current task.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "In <<chapter_intro>> we saw that deep learning can be used to get great results with natural language datasets. Our example relied on using a pretrained language model and fine-tuning it to classify reviews. That example highlighted a difference between transfer learning in NLP and computer vision: in general in NLP the pretrained model is trained on a different task.",

                        "The language model we used in <<chapter_intro>> to classify IMDb reviews was pretrained on Wikipedia. We got great results by directly fine-tuning this language model to a movie review classifier, but with one extra step, we can do even better. The Wikipedia English is slightly different from the IMDb English, so instead of jumping directly to the classifier, we could fine-tune our pretrained language model to the IMDb corpus and then use *that* as the base for our classifier.",

                        "you get even better results if you fine-tune the (sequence-based) language model prior to fine-tuning the classification model. For instance, for the IMDb sentiment analysis task, the dataset includes 50,000 additional movie reviews that do not have any positive or negative labels attached. Since there are 25,000 labeled reviews in the training set and 25,000 in the validation set, that makes 100,000 movie reviews altogether. We can use all of these reviews to fine-tune the pretrained language model, which was trained only on Wikipedia articles; this will result in a language model that is particularly good at predicting the next word of a movie review"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 10,
            "question_number": 6,
            "question_text": "\"What are the three steps to create a state-of-the-art text classifier?\"",
            "gold_standard_answer": "\"Train a language model on a large corpus of text (already done for ULM-FiT by Sebastian Ruder and Jeremy!)\nFine-tune the language model on text classification dataset\nFine-tune the language model as a text classifier instead.\"",
            "answer_context": [
                {
                    "answer_component": "\"Train a language model on a large corpus of text (already done for ULM-FiT by Sebastian Ruder and Jeremy!)\"", 
                    "scoring_type": "simple",
                    "context": [
                        "The language model we used in <<chapter_intro>> to classify IMDb reviews was pretrained on Wikipedia."
                    ],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"Fine-tune the language model on text classification dataset\"", 
                    "scoring_type": "simple",
                    "context": [
                        "first we need to fine-tune our language model pretrained on Wikipedia to the corpus of IMDb reviews",

                        "The Wikipedia English is slightly different from the IMDb English, so instead of jumping directly to the classifier, we could fine-tune our pretrained language model to the IMDb corpus"
                    ],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"Fine-tune the language model as a text classifier instead\"", 
                    "scoring_type": "simple",
                    "context": [
                        "and then we can use that model to train a classifier",

                        "then use *that* as the base for our classifier",

                        "We're now moving from language model fine-tuning to classifier fine-tuning. To recap, a language model predicts the next word of a document, so it doesn't need any external labels. A classifier, however, predicts some external label\u2014in the case of IMDb, it's the sentiment of a document"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 10,
            "question_number": 7,
            "question_text": "\"How do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset?\"",
            "gold_standard_answer": "\"By learning how to predict the next word of a movie review, the model better understands the language style and structure of the text classification dataset and can, therefore, perform better when fine-tuned as a classifier.\"",
            "answer_context": [
                {
                    "answer_component": "\"By learning how to predict the next word of a movie review, the model better understands the language style and structure of the text classification dataset and can, therefore, perform better when fine-tuned as a classifier.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "One reason, of course, is that it is helpful to understand the foundations of the models that you are using. But there is another very practical reason, which is that you get even better results if you fine-tune the (sequence-based) language model prior to fine-tuning the classification model. For instance, for the IMDb sentiment analysis task, the dataset includes 50,000 additional movie reviews that do not have any positive or negative labels attached. Since there are 25,000 labeled reviews in the training set and 25,000 in the validation set, that makes 100,000 movie reviews altogether. We can use all of these reviews to fine-tune the pretrained language model, which was trained only on Wikipedia articles; this will result in a language model that is particularly good at predicting the next word of a movie review"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 10,
            "question_number": 8,
            "question_text": "\"What are the three steps to prepare your data for a language model?\"",
            "gold_standard_answer": "\"Tokenization\nNumericalization\nLanguage model DataLoader\"",
            "answer_context": [
                {
                    "answer_component": "\"Tokenization\nNumericalization\nLanguage model DataLoader\"", 
                    "scoring_type": "simple",
                    "context": [
                        "- Tokenization:: Convert the text into a list of words (or characters, or substrings, depending on the granularity of your model)\n- Numericalization:: Make a list of all of the unique words that appear (the vocab), and convert each word into a number, by looking up its index in the vocab\n- Language model data loader creation:: fastai provides an `LMDataLoader` class which automatically handles creating a dependent variable that is offset from the independent variable by one token. It also handles some important details, such as how to shuffle the training data in such a way that the dependent and independent variables maintain their structure as required"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "\"What are the three steps to prepare your data for a language model?\"", 
                    "context": [
                        "Each of the steps necessary to create a language model has jargon associated with it from the world of natural language processing, and fastai and PyTorch classes available to help. The steps are:"
                    ]
                }
            ]
        },
        {
            "chapter": 10,
            "question_number": 9,
            "question_text": "\"What is \"tokenization\"? Why do we need it?\"",
            "gold_standard_answer": "\"Tokenization is the process of converting text into a list of words. It is not as simple as splitting on the spaces. Therefore, we need a tokenizer that deals with complicated cases like punctuation, hypenated words, etc.\"",
            "answer_context": [
                {
                    "answer_component": "Tokenization is the process of converting text into a list of words", 
                    "scoring_type": "simple",
                    "context": [
                        "- Tokenization:: Convert the text into a list of words (or characters, or substrings, depending on the granularity of your model)"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "It is not as simple as splitting on the spaces. Therefore, we need a tokenizer that deals with complicated cases like punctuation, hypenated words, etc", 
                    "scoring_type": "simple",
                    "context": [
                        "When we said \"convert the text into a list of words,\" we left out a lot of details. For instance, what do we do with punctuation? How do we deal with a word like \"don't\"? Is it one word, or two? What about long medical or chemical words? Should they be split into their separate pieces of meaning? How about hyphenated words? What about languages like German and Polish where we can create really long words from many, many pieces? What about languages like Japanese and Chinese that don't use bases at all, and don't really have a well-defined idea of *word*?"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 10,
            "question_number": 10,
            "question_text": "\"Name three different approaches to tokenization.\"",
            "gold_standard_answer": "\"Word-based tokenization\nSubword-based tokenization\nCharacter-based tokenization\"",
            "answer_context": [
                {
                    "answer_component": "\"Word-based tokenization\nSubword-based tokenization\nCharacter-based tokenization\"", 
                    "scoring_type": "simple",
                    "context": [
                        "- Word-based:: Split a sentence on spaces, as well as applying language-specific rules to try to separate parts of meaning even when there are no spaces (such as turning \"don't\" into \"do n't\"). Generally, punctuation marks are also split into separate tokens.\n- Subword based:: Split words into smaller parts, based on the most commonly occurring substrings. For instance, \"occasion\" might be tokenized as \"o c ca sion.\"\n- Character-based:: Split a sentence into its individual characters."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "\"Name three different approaches to tokenization.\"", 
                    "context": [
                        "Because there is no one correct answer to these questions, there is no one approach to tokenization. There are three main approaches:"
                    ]
                }
            ]
        },
        {
            "chapter": 10,
            "question_number": 11,
            "question_text": "\"What is xxbos?\"",
            "gold_standard_answer": "\"This is a special token added by fastai that indicated the beginning of the text.\"",
            "answer_context": [
                {
                    "answer_component": "\"This is a special token added by fastai that indicated the beginning of the text.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "- `xxbos`:: Indicates the beginning of a text (here, a review)\n- `xxmaj`:: Indicates the next word begins with a capital (since we lowercased everything)\n- `xxunk`:: Indicates the word is unknown"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 10,
            "question_number": 12,
            "question_text": "\"List four rules that fastai applies to text during tokenization.\"",
            "gold_standard_answer": "\"Here are all the rules:\n\nfix_html :: replace special HTML characters by a readable version (IMDb reviews have quite a few of them for instance) ;\nreplace_rep :: replace any character repeated three times or more by a special token for repetition (xxrep), the number of times it\u2019s repeated, then the character ;\nreplace_wrep :: replace any word repeated three times or more by a special token for word repetition (xxwrep), the number of times it\u2019s repeated, then the word ;\nspec_add_spaces :: add spaces around / and # ;\nrm_useless_spaces :: remove all repetitions of the space character ;\nreplace_all_caps :: lowercase a word written in all caps and adds a special token for all caps (xxcap) in front of it ;\nreplace_maj :: lowercase a capitalized word and adds a special token for capitalized (xxmaj) in front of it ;\nlowercase :: lowercase all text and adds a special token at the beginning (xxbos) and/or the end (xxeos).\"",
            "answer_context": [
                {
                    "answer_component": "\"Here are all the rules:\n\nfix_html :: replace special HTML characters by a readable version (IMDb reviews have quite a few of them for instance) ;\nreplace_rep :: replace any character repeated three times or more by a special token for repetition (xxrep), the number of times it\u2019s repeated, then the character ;\nreplace_wrep :: replace any word repeated three times or more by a special token for word repetition (xxwrep), the number of times it\u2019s repeated, then the word ;\nspec_add_spaces :: add spaces around / and # ;\nrm_useless_spaces :: remove all repetitions of the space character ;\nreplace_all_caps :: lowercase a word written in all caps and adds a special token for all caps (xxcap) in front of it ;\nreplace_maj :: lowercase a capitalized word and adds a special token for capitalized (xxmaj) in front of it ;\nlowercase :: lowercase all text and adds a special token at the beginning (xxbos) and/or the end (xxeos).\"", 
                    "scoring_type": "simple",
                    "context": [
                        "- `fix_html`:: Replaces special HTML characters with a readable version (IMDb reviews have quite a few of these)\n- `replace_rep`:: Replaces any character repeated three times or more with a special token for repetition (`xxrep`), the number of times it's repeated, then the character\n- `replace_wrep`:: Replaces any word repeated three times or more with a special token for word repetition (`xxwrep`), the number of times it's repeated, then the word\n- `spec_add_spaces`:: Adds spaces around / and #\n- `rm_useless_spaces`:: Removes all repetitions of the space character\n- `replace_all_caps`:: Lowercases a word written in all caps and adds a special token for all caps (`xxup`) in front of it\n- `replace_maj`:: Lowercases a capitalized word and adds a special token for capitalized (`xxmaj`) in front of it\n- `lowercase`:: Lowercases all text and adds a special token at the beginning (`xxbos`) and/or the end (`xxeos`)"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 10,
            "question_number": 13,
            "question_text": "\"Why are repeated characters replaced with a token showing the number of repetitions and the character that's repeated?\"",
            "gold_standard_answer": "\"We can expect that repeated characters could have special or different meaning than just a single character. By replacing them with a special token showing the number of repetitions, the model\u2019s embedding matrix can encode information about general concepts such as repeated characters rather than requiring a separate token for every number of repetitions of every character.\"",
            "answer_context": [
                {
                    "answer_component": "\"We can expect that repeated characters could have special or different meaning than just a single character. By replacing them with a special token showing the number of repetitions, the model\u2019s embedding matrix can encode information about general concepts such as repeated characters rather than requiring a separate token for every number of repetitions of every character.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "the rules will replace a sequence of four exclamation points with a special *repeated character* token, followed by the number four, and then a single exclamation point. In this way, the model's embedding matrix can encode information about general concepts such as repeated punctuation rather than requiring a separate token for every number of repetitions of every punctuation mark. Similarly, a capitalized word will be replaced with a special capitalization token, followed by the lowercase version of the word. This way, the embedding matrix only needs the lowercase versions of the words, saving compute and memory resources, but can still learn the concept of capitalization"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 10,
            "question_number": 14,
            "question_text": "\"What is \"numericalization\"?\"",
            "gold_standard_answer": "\"This refers to the mapping of the tokens to integers to be passed into the model.\"",
            "answer_context": [
                {
                    "answer_component": "\"This refers to the mapping of the tokens to integers to be passed into the model.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "*Numericalization* is the process of mapping tokens to integers. The steps are basically identical to those necessary to create a `Category` variable, such as the dependent variable of digits in MNIST:",

                        "Numericalization:: Make a list of all of the unique words that appear (the vocab), and convert each word into a number, by looking up its index in the vocab"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 10,
            "question_number": 15,
            "question_text": "\"Why might there be words that are replaced with the \"unknown word\" token?\"",
            "gold_standard_answer": "\"If all the words in the dataset have a token associated with them, then the embedding matrix will be very large, increase memory usage, and slow down training. Therefore, only words with more than min_freq occurrence are assigned a token and finally a number, while others are replaced with the \u201cunknown word\u201d token.\"",
            "answer_context": [
                {
                    "answer_component": "\"If all the words in the dataset have a token associated with them, then the embedding matrix will be very large, increase memory usage, and slow down training. Therefore, only words with more than min_freq occurrence are assigned a token and finally a number, while others are replaced with the \u201cunknown word\u201d token.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "The defaults to `Numericalize` are `min_freq=3,max_vocab=60000`. `max_vocab=60000` results in fastai replacing all words other than the most common 60,000 with a special *unknown word* token, `xxunk`. This is useful to avoid having an overly large embedding matrix, since that can slow down training and use up too much memory, and can also mean that there isn't enough data to train useful representations for rare words. However, this last issue is better handled by setting `min_freq`; the default `min_freq=3` means that any word appearing less than three times is replaced with `xxunk`."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 10,
            "question_number": 17,
            "question_text": "\"Why do we need padding for text classification? Why don't we need it for language modeling?\"",
            "gold_standard_answer": "\"Since the documents have variable sizes, padding is needed to collate the batch. Other approaches. like cropping or squishing, either to negatively affect training or do not make sense in this context. Therefore, padding is used. It is not required for language modeling since the documents are all concatenated.\"",
            "answer_context": [
                {
                    "answer_component": "Since the documents have variable sizes, padding is needed to collate the batch. Other approaches. like cropping or squishing, either to negatively affect training or do not make sense in this context. Therefore, padding is used", 
                    "scoring_type": "simple",
                    "context": [
                        "Remember, PyTorch `DataLoader`s need to collate all the items in a batch into a single tensor, and a single tensor has a fixed shape (i.e., it has some particular length on every axis, and all items must be consistent). This should sound familiar: we had the same issue with images. In that case, we used cropping, padding, and/or squishing to make all the inputs the same size. Cropping might not be a good idea for documents, because it seems likely we'd remove some key information (having said that, the same issue is true for images, and we use cropping there; data augmentation hasn't been well explored for NLP yet, so perhaps there are actually opportunities to use cropping in NLP too!). You can't really \"squish\" a document. So that leaves padding!",

                        "We will expand the shortest texts to make them all the same size. To do this, we use a special padding token that will be ignored by our model. Additionally, to avoid memory issues and improve performance, we will batch together texts that are roughly the same lengths (with some shuffling for the training set). We do this by (approximately, for the training set) sorting the documents by length prior to each epoch. The result of this is that the documents collated into a single batch will tend to be of similar lengths. We won't pad every batch to the same size, but will instead use the size of the largest document in each batch as the target size. (It is possible to do something similar with images, which is especially useful for irregularly sized rectangular images, but at the time of writing no library provides good support for this yet, and there aren't any papers covering it. It's something we're planning to add to fastai soon, however, so keep an eye on the book's website; we'll add information about this as soon as we have it working well.)"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "It is not required for language modeling since the documents are all concatenated", 
                    "scoring_type": "simple",
                    "context": [
                        "The sorting and padding are automatically done by the data block API for us when using a `TextBlock`, with `is_lm=False`. (We don't have this same issue for language model data, since we concatenate all the documents together first, and then split them into equally sized sections.)"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 10,
            "question_number": 19,
            "question_text": "\"What is \"perplexity\"?\"",
            "gold_standard_answer": "\"Perplexity is a commonly used metric in NLP for language models. It is the exponential of the loss.\"",
            "answer_context": [
                {
                    "answer_component": "\"Perplexity is a commonly used metric in NLP for language models. It is the exponential of the loss.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "The loss function used by default is cross-entropy loss, since we essentially have a classification problem (the different categories being the words in our vocab). The *perplexity* metric used here is often used in NLP for language models: it is the exponential of the loss (i.e., `torch.exp(cross_entropy)`). We  also include the accuracy metric, to see how many times our model is right when trying to predict the next word, since cross-entropy (as we've seen) is both hard to interpret, and tells us more about the model's confidence than its accuracy."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 10,
            "question_number": 20,
            "question_text": "\"Why do we have to pass the vocabulary of the language model to the classifier data block?\"",
            "gold_standard_answer": "\"This is to ensure the same correspondence of tokens to index so the model can appropriately use the embeddings learned during LM fine-tuning.\"",
            "answer_context": [
                {
                    "answer_component": "\"This is to ensure the same correspondence of tokens to index so the model can appropriately use the embeddings learned during LM fine-tuning.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "The reason that we pass the `vocab` of the language model is to make sure we use the same correspondence of token to index. Otherwise the embeddings we learned in our fine-tuned language model won't make any sense to this model, and the fine-tuning step won't be of any use."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 10,
            "question_number": 21,
            "question_text": "\"What is \"gradual unfreezing\"?\"",
            "gold_standard_answer": "\"This refers to unfreezing one layer at a time and fine-tuning the pretrained model.\"",
            "answer_context": [
                {
                    "answer_component": "\"This refers to unfreezing one layer at a time and fine-tuning the pretrained model.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "The last step is to train with discriminative learning rates and *gradual unfreezing*. In computer vision we often unfreeze the model all at once, but for NLP classifiers, we find that unfreezing a few layers at a time makes a real difference:"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 10,
            "question_number": 22,
            "question_text": "\"Why is text generation always likely to be ahead of automatic identification of machine-generated texts?\"",
            "gold_standard_answer": "\"The classification models could be used to improve text generation algorithms (evading the classifier) so the text generation algorithms will always be ahead.\"",
            "answer_context": [
                {
                    "answer_component": "\"The classification models could be used to improve text generation algorithms (evading the classifier)\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Many people assume or hope that algorithms will come to our defense here\u2014that we will develop classification algorithms that can automatically recognise autogenerated content. The problem, however, is that this will always be an arms race, in which better classification (or discriminator) algorithms can be used to create better generation algorithms"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"the text generation algorithms will always be ahead\"", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 1,
            "question_text": "\"What is a \"feature\"?\"",
            "gold_standard_answer": "\"A feature is a transformation of the data which is designed to make it easier to model.\"",
            "answer_context": [
                {
                    "answer_component": "\"A feature is a transformation of the data which is designed to make it easier to model.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "A *feature* is a transformation of the data which is designed to make it easier to model"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 4,
            "question_text": "\"What is the value of a convolutional kernel apply to a 3\u00d73 matrix of zeros?\"",
            "gold_standard_answer": "\"A zero matrix.\"",
            "answer_context": [
                {
                    "answer_component": "\"A zero matrix.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Now we're going to take the top 3\u00d73-pixel square of our image, and multiply each of those values by each item in our kernel. Then we'll add them up, like so:"
                    ],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"A zero matrix.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "im3_t = tensor(im3)\nim3_t[0:3,0:3] * top_edge\n```\nOutput:\ntensor([[-0., -0., -0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])"
                    ],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 5,
            "question_text": "\"What is \"padding\"?\"",
            "gold_standard_answer": "\"Padding is the additional pixels that are added around the outside of the image, allows the kernel to be applied to the edge of the image for a convolution.\"",
            "answer_context": [
                {
                    "answer_component": "\"Padding is the additional pixels that are added around the outside of the image, allows the kernel to be applied to the edge of the image for a convolution.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "It would be nice to not lose those two pixels on each axis. The way we do that is to add *padding*, which is simply additional pixels added around the outside of our image. Most commonly, pixels of zeros are added."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 6,
            "question_text": "\"What is \"stride\"?\"",
            "gold_standard_answer": "\"Stride refers to how many pixels at a time the kernel is moved during the convolution.\"",
            "answer_context": [
                {
                    "answer_component": "\"Stride refers to how many pixels at a time the kernel is moved during the convolution.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "So far, when we have applied the kernel to the grid, we have moved it one pixel over at a time. But we can jump further; for instance, we could move over two pixels after each kernel application, as in <<three_by_five_conv>>. This is known as a *stride-2* convolution"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 8,
            "question_text": "\"What are the shapes of the input and weight parameters to PyTorch's 2D convolution?\"",
            "gold_standard_answer": "\"input: input tensor of shape (minibatch, in_channels, iH, iW)\nweight: filters of shape (out_channels, in_channels, kH, kW)\"",
            "answer_context": [
                {
                    "answer_component": "\"input: input tensor of shape (minibatch, in_channels, iH, iW)\nweight: filters of shape (out_channels, in_channels, kH, kW)\"", 
                    "scoring_type": "simple",
                    "context": [
                        "- input:: input tensor of shape `(minibatch, in_channels, iH, iW)`\n- weight:: filters of shape `(out_channels, in_channels, kH, kW)`"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 9,
            "question_text": "\"What is a \"channel\"?\"",
            "gold_standard_answer": "\"The term channel (and also feature, often used interchangeably) refers to the size of the second axis of a weight matrix, which is, the number of activations per grid cell after a convolution.\"",
            "answer_context": [
                {
                    "answer_component": "\"The term channel (and also feature, often used interchangeably) refers to the size of the second axis of a weight matrix, which is, the number of activations per grid cell after a convolution.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "> jargon: channels and features: These two terms are largely used interchangeably, and refer to the size of the second axis of a weight matrix, which is, the number of activations per grid cell after a convolution. _Features_ is never used to refer to the input data, but _channels_ can refer to either the input data (generally channels are colors) or activations inside the network"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 10,
            "question_text": "\"What is the relationship between a convolution and a matrix multiplication?\"",
            "gold_standard_answer": "\"A convolution can be represented as matrix multiplication. This (weight) matrix has two properties:\n\nIt has various zeros\nSome of the weights are equal. This is referred to as shared weights\"",
            "answer_context": [
                {
                    "answer_component": "A convolution can be represented as matrix multiplication", 
                    "scoring_type": "simple",
                    "context": [
                        "Here's an interesting insight\u2014a convolution can be represented as a special kind of matrix multiplication, as illustrated in <<conv_matmul>>. The weight matrix is just like the ones from traditional neural networks. However, this weight matrix has two special properties:"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "This (weight) matrix has two properties:\n\nIt has various zeros\nSome of the weights are equal. This is referred to as shared weights", 
                    "scoring_type": "simple",
                    "context": [
                        "1. The zeros shown in gray are untrainable. This means that they\u2019ll stay zero throughout the optimization process.\n1. Some of the weights are equal, and while they are trainable (i.e., changeable), they must remain equal. These are called *shared weights*."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 11,
            "question_text": "\"What is a \"convolutional neural network\"?\"",
            "gold_standard_answer": "\"When we use convolutions instead of (or in addition to) regular linear layers we create a convolutional neural network (CNN).\"",
            "answer_context": [
                {
                    "answer_component": "\"When we use convolutions instead of (or in addition to) regular linear layers we create a convolutional neural network (CNN).\"",
                    "scoring_type": "simple",
                    "context": [
                        "When we use convolutions instead of (or in addition to) regular linear layers we create a *convolutional neural network* (CNN)."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 12,
            "question_text": "\"What is the benefit of refactoring parts of your neural network definition?\"",
            "gold_standard_answer": "\"It makes it much less likely you\u2019ll get errors due to inconsistencies in your architectures, and makes it more obvious to the reader which parts of your layers are actually changing.\"",
            "answer_context": [
                {
                    "answer_component": "\"It makes it much less likely you\u2019ll get errors due to inconsistencies in your architectures, and makes it more obvious to the reader which parts of your layers are actually changing.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Refactoring parts of your neural networks like this makes it much less likely you'll get errors due to inconsistencies in your architectures, and makes it more obvious to the reader which parts of your layers are actually changing."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "refactoring", 
                    "context": [
                        "> important: Refactoring: "
                    ]
                }
            ]
        },
        {
            "chapter": 13,
            "question_number": 13,
            "question_text": "\"What is Flatten? Where does it need to be included in the MNIST CNN? Why?\"",
            "gold_standard_answer": "\"It\u2019s basically the same as PyTorch\u2019s squeeze method, but as a module. It is included at the end of the MNIST CNN to remove the extra 1x1 axes.\"",
            "answer_context": [
                {
                    "answer_component": "\"It\u2019s basically the same as PyTorch\u2019s squeeze method, but as a module. It is included at the end of the MNIST CNN to remove the extra 1x1 axes.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Note that the output of the final `Conv2d` layer is `64x2x1x1`. We need to remove those extra `1x1` axes; that's what `Flatten` does. It's basically the same as PyTorch's `squeeze` method, but as a module."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 14,
            "question_text": "\"What does \"NCHW\" mean?\"",
            "gold_standard_answer": "\"It is an abbreviation for the axes of the input of the model. It stands for batch size, channels, height, and width.\"",
            "answer_context": [
                {
                    "answer_component": "\"It is an abbreviation for the axes of the input of the model. It stands for batch size, channels, height, and width.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "We can see from the summary that we have an input of size `64x1x28x28`. The axes are `batch,channel,height,width`. This is often represented as `NCHW` (where `N` refers to batch size). Tensorflow, on the other hand, uses `NHWC` axis order"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 15,
            "question_text": "\"Why does the third layer of the MNIST CNN have 7*7*(1168-16) multiplications?\"",
            "gold_standard_answer": "\"There are 1168 parameters for that layer, and ignoring the 16 parameters (=number of filters) of the bias, the (1168-16) parameters is applied to the 7x7 grid.\"",
            "answer_context": [
                {
                    "answer_component": "There are 1168 parameters for that layer", 
                    "scoring_type": "simple",
                    "context": [
                        "```python\nlearn.summary()\n```\nOutput:\nSequential (Input shape: ['64 x 1 x 28 x 28'])\n================================================================\nLayer (type)         Output Shape         Param #    Trainable \n================================================================\nConv2d               64 x 4 x 14 x 14     40         True      \n________________________________________________________________\nReLU                 64 x 4 x 14 x 14     0          False     \n________________________________________________________________\nConv2d               64 x 8 x 7 x 7       296        True      \n________________________________________________________________\nReLU                 64 x 8 x 7 x 7       0          False     \n________________________________________________________________\nConv2d               64 x 16 x 4 x 4      1,168      True      \n________________________________________________________________\n"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "ignoring the 16 parameters (=number of filters) of the bias, the (1168-16) parameters is applied to the 7x7 grid", 
                    "scoring_type": "simple",
                    "context": [
                        "There is one bias for each channel. (Sometimes channels are called *features* or *filters* when they are not input channels.) The output shape is `64x4x14x14`, and this will therefore become the input shape to the next layer. The next layer, according to `summary`, has 296 parameters. Let's ignore the batch axis to keep things simple. So for each of `14*14=196` locations we are multiplying `296-8=288` weights (ignoring the bias for simplicity), so that's `196*288=56_448` multiplications at this layer. The next layer will have `7*7*(1168-16)=56_448` multiplications."
                    ],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 16,
            "question_text": "\"What is a \"receptive field\"?\"",
            "gold_standard_answer": "\"The receptive field is the area of an image that is involved in the calculation of a layer.\"",
            "answer_context": [
                {
                    "answer_component": "\"The receptive field is the area of an image that is involved in the calculation of a layer.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "The *receptive field* is the area of an image that is involved in the calculation of a layer"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 17,
            "question_text": "\"What is the size of the receptive field of an activation after two stride 2 convolutions? Why?\"",
            "gold_standard_answer": "\"The size of the receptive field increases the deeper we are in the network. After two stride 2 convolutions, the receptive field is 7x7.\"",
            "answer_context": [
                {
                    "answer_component": "After two stride 2 convolutions, the receptive field is 7x7", 
                    "scoring_type": "simple",
                    "context": [
                        "In this example, we have just two convolutional layers, each of stride 2, so this is now tracing right back to the input image. We can see that a 7\u00d77 area of cells in the input layer is used to calculate the single green cell in the Conv2 layer. This 7\u00d77 area is the *receptive field* in the input of the green activation in Conv2. We can also see that a second filter kernel is needed now, since we have two layers."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "The size of the receptive field increases the deeper we are in the network", 
                    "scoring_type": "simple",
                    "context": [
                        "As you see from this example, the deeper we are in the network (specifically, the more stride-2 convs we have before a layer), the larger the receptive field for an activation in that layer"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 20,
            "question_text": "\"How is a color image represented as a tensor?\"",
            "gold_standard_answer": "\"It is a rank-3 tensor of shape (3, height, width)\"",
            "answer_context": [
                {
                    "answer_component": "\"It is a rank-3 tensor of shape (3, height, width)\"", 
                    "scoring_type": "simple",
                    "context": [
                        "One batch contains 64 images, each of 1 channel, with 28\u00d728 pixels. `F.conv2d` can handle multichannel (i.e., color) images too. A *channel* is a single basic color in an image\u2014for regular full-color images there are three channels, red, green, and blue. PyTorch represents an image as a rank-3 tensor, with dimensions `[channels, rows, columns]`."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 21,
            "question_text": "\"How does a convolution work with a color input?\"",
            "gold_standard_answer": "\"The convolutional kernel is of size (ch_out, ch_in, ks, ks). For example, with a color input with a kernel size of 3x3 with 7 output channels, that would be (7,3,3,3). The convolution filter for each of the ch_in=3 channels are applied separately to each of the 3 color channels and summed up, and we have ch_out filters like this, giving us a ch_out convolutional kernel tensors of size ch_in=3 x ks x ks. Thus the final size of this tensor is (ch_out, ch_in, ks, ks). Additionally we would have a bias of size ch_out.\"",
            "answer_context": [
                {
                    "answer_component": [
                        "\"The convolutional kernel is of size (ch_out, ch_in, ks, ks)",
                        
                        "Thus the final size of this tensor is (ch_out, ch_in, ks, ks). Additionally we would have a bias of size ch_out.\""], 
                    "scoring_type": "simple",
                    "context": [
                        "Then we have `ch_out` filters like this, so in the end, the result of our convolutional layer will be a batch of images with `ch_out` channels and a height and width given by the formula outlined earlier. This give us `ch_out` tensors of size `ch_in x ks x ks` that we represent in one big tensor of four dimensions. In PyTorch, the order of the dimensions for those weights is `ch_out x ch_in x ks x ks`."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"The convolution filter for each of the ch_in=3 channels are applied separately to each of the 3 color channels and summed up\"", 
                    "scoring_type": "simple",
                    "context": [
                        "In the preceding example, the final result for our convolutional layer would be $y_{R} + y_{G} + y_{B} + b$ in that case"
                    ],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"For example, with a color input with a kernel size of 3x3 with 7 output channels, that would be (7,3,3,3). The convolution filter for each of the ch_in=3 channels\"", 
                    "scoring_type": "simple",
                    "context": [],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 22,
            "question_text": "\"What method can we use to see that data in DataLoaders?\"",
            "gold_standard_answer": "\"show_batch\"",
            "answer_context": [
                {
                    "answer_component": "\"show_batch\"", 
                    "scoring_type": "simple",
                    "context": [
                        "dls.show_batch(max_n=9, figsize=(4,4))"
                    ],
                    "explicit_context": "false",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "\"What method can we use to see that data in DataLoaders?\"", 
                    "context": [
                        "Remember, it's always a good idea to look at your data before you use it:"
                    ]
                }
            ]
        },
        {
            "chapter": 13,
            "question_number": 23,
            "question_text": "\"Why do we double the number of filters after each stride-2 conv?\"",
            "gold_standard_answer": "\"This is because we\u2019re decreasing the number of activations in the activation map by a factor of 4; we don\u2019t want to decrease the capacity of a layer by too much at a time.\"",
            "answer_context": [
                {
                    "answer_component": "\"This is because we\u2019re decreasing the number of activations in the activation map by a factor of 4; we don\u2019t want to decrease the capacity of a layer by too much at a time.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "When we use a stride-2 convolution, we often increase the number of features at the same time. This is because we're decreasing the number of activations in the activation map by a factor of 4; we don't want to decrease the capacity of a layer by too much at a time."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 24,
            "question_text": "\"Why do we use a larger kernel in the first conv with MNIST (with simple_cnn)?\"",
            "gold_standard_answer": "\"With the first layer, if the kernel size is 3x3, with four output filters, then nine pixels are being used to produce 8 output numbers so there is not much learning since input and output size are almost the same. Neural networks will only create useful features if they\u2019re forced to do so\u2014that is, if the number of outputs from an operation is significantly smaller than the number of inputs. To fix this, we can use a larger kernel in the first layer.\"",
            "answer_context": [
                {
                    "answer_component": "\"With the first layer, if the kernel size is 3x3, with four output filters, then nine pixels are being used to produce 8 output numbers so there is not much learning since input and output size are almost the same. Neural networks will only create useful features if they\u2019re forced to do so\u2014that is, if the number of outputs from an operation is significantly smaller than the number of inputs. To fix this, we can use a larger kernel in the first layer.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "But there is a subtle problem with this. Consider the kernel that is being applied to each pixel. By default, we use a 3\u00d73-pixel kernel. That means that there are a total of 3\u00d73 = 9 pixels that the kernel is being applied to at each location. Previously, our first layer had four output filters. That meant that there were four values being computed from nine pixels at each location. Think about what happens if we double this output to eight filters. Then when we apply our kernel we will be using nine pixels to calculate eight numbers. That means it isn't really learning much at all: the output size is almost the same as the input size. Neural networks will only create useful features if they're forced to do so\u2014that is, if the number of outputs from an operation is significantly smaller than the number of inputs."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"To fix this, we can use a larger kernel in the first layer.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "To fix this, we can use a larger kernel in the first layer. If we use a kernel of 5\u00d75 pixels then there are 25 pixels being used at each kernel application. Creating eight filters from this will mean the neural net will have to find some useful features:"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 25,
            "question_text": "\"What information does ActivationStats save for each layer?\"",
            "gold_standard_answer": "\"It records the mean, standard deviation, and histogram of activations of every trainable layer.\"",
            "answer_context": [
                {
                    "answer_component": "\"It records the mean, standard deviation, and histogram of activations of every trainable layer.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "As you'll see in a moment, we can look inside our models while they're training in order to try to find ways to make them train better. To do this we use the `ActivationStats` callback, which records the mean, standard deviation, and histogram of activations of every trainable layer (as we've seen, callbacks are used to add behavior to the training loop; we'll explore how they work in <<chapter_accel_sgd>>):"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 26,
            "question_text": "\"How can we access a learner's callback after training?\"",
            "gold_standard_answer": "\"They are available with the Learner object with the same name as the callback class, but in snake_case. For example, the Recorder callback is available through learn.recorder.\"",
            "answer_context": [
                {
                    "answer_component": "\"They are available with the Learner object with the same name as the callback class, but in snake_case. For example, the Recorder callback is available through learn.recorder.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "One handy feature of the callbacks passed to `Learner` is that they are made available automatically, with the same name as the callback class, except in `snake_case`. So, our `ActivationStats` callback can be accessed through `activation_stats`. I'm sure you remember `learn.recorder`... can you guess how that is implemented? That's right, it's a callback called `Recorder`!"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 28,
            "question_text": "\"Why are activations near zero problematic?\"",
            "gold_standard_answer": "\"Activations near zero are problematic because it means we have computation in the model that\u2019s doing nothing at all (since multiplying by zero gives zero). When you have some zeros in one layer, they will therefore generally carry over to the next layer\u2026 which will then create more zeros.\"",
            "answer_context": [
                {
                    "answer_component": "\"Activations near zero are problematic because it means we have computation in the model that\u2019s doing nothing at all (since multiplying by zero gives zero). When you have some zeros in one layer, they will therefore generally carry over to the next layer\u2026 which will then create more zeros.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Generally our model should have a consistent, or at least smooth, mean and standard deviation of layer activations during training. Activations near zero are particularly problematic, because it means we have computation in the model that's doing nothing at all (since multiplying by zero gives zero). When you have some zeros in one layer, they will therefore generally carry over to the next layer... which will then create more zeros. Here's the penultimate layer of our network:"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 29,
            "question_text": "\"What are the upsides and downsides of training with a larger batch size?\"",
            "gold_standard_answer": "\"The gradients are more accurate since they\u2019re calculated from more data, but a larger batch size means fewer batches per epoch, which means less opportunities for the model to update weights.\"",
            "answer_context": [
                {
                    "answer_component": "\"The gradients are more accurate since they\u2019re calculated from more data, but a larger batch size means fewer batches per epoch, which means less opportunities for the model to update weights.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "One way to make training more stable is to increase the batch size. Larger batches have gradients that are more accurate, since they're calculated from more data. On the downside, though, a larger batch size means fewer batches per epoch, which means less opportunities for your model to update weights"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 30,
            "question_text": "\"Why should we avoid using a high learning rate at the start of training?\"",
            "gold_standard_answer": "\"Our initial weights are not well suited to the task we\u2019re trying to solve. Therefore, it is dangerous to begin training with a high learning rate: we may very well make the training diverge instantly.\"",
            "answer_context": [
                {
                    "answer_component": "\"Our initial weights are not well suited to the task we\u2019re trying to solve. Therefore, it is dangerous to begin training with a high learning rate: we may very well make the training diverge instantly.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Our initial weights are not well suited to the task we're trying to solve. Therefore, it is dangerous to begin training with a high learning rate: we may very well make the training diverge instantly"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 31,
            "question_text": "\"What is 1cycle training?\"",
            "gold_standard_answer": "\"1cycle training is a type of learning rate schedule developed by Leslie Smith that combines learning rate warmup and annealing, which allows us to train with higher learning rates.\"",
            "answer_context": [
                {
                    "answer_component": "\"1cycle training is a type of learning rate schedule developed by Leslie Smith that combines learning rate warmup and annealing, which allows us to train with higher learning rates.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Leslie Smith (yes, the same guy that invented the learning rate finder!) developed this idea in his article [\"Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates\"](https://arxiv.org/abs/1708.07120). He designed a schedule for learning rate separated into two phases: one where the learning rate grows from the minimum value to the maximum value (*warmup*), and one where it decreases back to the minimum value (*annealing*). Smith called this combination of approaches *1cycle training*."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 32,
            "question_text": "\"What are the benefits of training with a high learning rate?\"",
            "gold_standard_answer": "\"Training with a high learning rate gives two benefits:\n\nBy training with higher learning rates, we train faster\u2014a phenomenon Smith named super-convergence.\nBy training with higher learning rates, we overfit less because we skip over the sharp local minima to end up in a smoother (and therefore more generalizable) part of the loss.\"",
            "answer_context": [
                {
                    "answer_component": "\"Training with a high learning rate gives two benefits:\n\nBy training with higher learning rates, we train faster\u2014a phenomenon Smith named super-convergence.\nBy training with higher learning rates, we overfit less because we skip over the sharp local minima to end up in a smoother (and therefore more generalizable) part of the loss.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "- By training with higher learning rates, we train faster\u2014a phenomenon Smith named *super-convergence*.\n- By training with higher learning rates, we overfit less because we skip over the sharp local minima to end up in a smoother (and therefore more generalizable) part of the loss."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "\"What are the benefits of training with a high learning rate?\"", 
                    "context": [
                        "1cycle training allows us to use a much higher maximum learning rate than other types of training, which gives two benefits:"
                    ]
                }
            ]
        },
        {
            "chapter": 13,
            "question_number": 33,
            "question_text": "\"Why do we want to use a low learning rate at the end of training?\"",
            "gold_standard_answer": "\"A lower learning rate at the end of training allows us to find the best part of loss landscape and further minimize the loss.\"",
            "answer_context": [
                {
                    "answer_component": "\"A lower learning rate at the end of training allows us to find the best part of loss landscape and further minimize the loss.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "Then, once we have found a nice smooth area for our parameters, we want to find the very best part of that area, which means we have to bring our learning rates down again. This is why 1cycle training has a gradual learning rate warmup, and a gradual learning rate cooldown. Many researchers have found that in practice this approach leads to more accurate models and trains more quickly. That is why it is the approach that is used by default for `fine_tune` in fastai."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 34,
            "question_text": "\"What is \"cyclical momentum\"?\"",
            "gold_standard_answer": "\"It suggests that the momentum varies in the opposite direction of the learning rate: when we are at high learning rates, we use less momentum, and we use more again in the annealing phase.\"",
            "answer_context": [
                {
                    "answer_component": "\"It suggests that the momentum varies in the opposite direction of the learning rate: when we are at high learning rates, we use less momentum, and we use more again in the annealing phase.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "It suggests that the momentum varies in the opposite direction of the learning rate: when we are at high learning rates, we use less momentum, and we use more again in the annealing phase."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": [
                {
                    "question_component": "\"What is \"cyclical momentum\"?\"", 
                    "context": [
                        "Leslie Smith introduced the idea of *cyclical momentums* in [\"A Disciplined Approach to Neural Network Hyper-Parameters: Part 1\"](https://arxiv.org/pdf/1803.09820.pdf)."
                    ]
                }
            ]
        },
        {
            "chapter": 13,
            "question_number": 35,
            "question_text": "\"What callback tracks hyperparameter values during training (along with other information)?\"",
            "gold_standard_answer": "\"The Recorder callback.\"",
            "answer_context": [
                {
                    "answer_component": "\"The Recorder callback.\"", 
                    "scoring_type": "simple",
                    "context": [
                        "`learn.recorder` (as the name suggests) records everything that happens during training, including losses, metrics, and hyperparameters"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 36,
            "question_text": "\"What does one column of pixels in the color_dim plot represent?\"",
            "gold_standard_answer": "\"It represents the histogram of activations for the specified layer for that batch.\"",
            "answer_context": [
                {
                    "answer_component": "\"It represents the histogram of activations for the specified layer for that batch.\"", 
                    "scoring_type": "simple",
                    "context": [
                            "The final plot for each layer is made by stacking the histogram of the activations from each batch along the horizontal axis. So each vertical slice in the visualisation represents the histogram of activations for a single batch. The color intensity corresponds to the height of the histogram, in other words the number of activations in each histogram bin"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 37,
            "question_text": "\"What does \"bad training\" look like in color_dim? Why?\"",
            "gold_standard_answer": "\"We would see a cycle of dark blue, bright yellow at the bottom return. This training is not smooth and effectively starts from stratch during these cycles.\"",
            "answer_context": [
                {
                    "answer_component": "\"We would see a cycle of dark blue, bright yellow at the bottom return. This training is not smooth and effectively starts from stratch during these cycles.\"", 
                    "scoring_type": "simple",
                    "context": [
                            "This shows a classic picture of \"bad training.\" We start with nearly all activations at zero\u2014that's what we see at the far left, with all the dark blue. The bright yellow at the bottom represents the near-zero activations. Then, over the first few batches we see the number of nonzero activations exponentially increasing. But it goes too far, and collapses! We see the dark blue return, and the bottom becomes bright yellow again. It almost looks like training restarts from scratch. Then we see the activations increase again, and collapse again. After repeating this a few times, eventually we see a spread of activations throughout the range"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 38,
            "question_text": "\"What trainable parameters does a batch normalization layer contain?\"",
            "gold_standard_answer": "\"There are two learnable parameters, beta and gamma, which allows the model to have any mean and variance for each layer, which are learned during training.\"",
            "answer_context": [
                {
                    "answer_component": "\"There are two learnable parameters, beta and gamma, which allows the model to have any mean and variance for each layer, which are learned during training.\"", 
                    "scoring_type": "simple",
                    "context": [
                            "So they also added two learnable parameters (meaning they will be updated in the SGD step), usually called `gamma` and `beta`. After normalizing the activations to get some new activation vector `y`, a batchnorm layer returns `gamma*y + beta`."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                },
                {
                    "answer_component": "\"which allows the model to have any mean and variance for each layer, which are learned during training.\"", 
                    "scoring_type": "simple",
                    "context": [
                            "That's why our activations can have any mean or variance, independent from the mean and standard deviation of the results of the previous layer. Those statistics are learned separately, making training easier on our model"
                    ],
                    "explicit_context": "false",
                    "extraneous_answer": "true"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 39,
            "question_text": "\"What statistics are used to normalize in batch normalization during training? How about during validation?\"",
            "gold_standard_answer": "\"During training, we use the mean and standard deviation of the batch to normalize the data, while during validation we instead use a running mean of the statistics calculated during training.\"",
            "answer_context": [
                {
                    "answer_component": "\"During training, we use the mean and standard deviation of the batch to normalize the data, while during validation we instead use a running mean of the statistics calculated during training.\"", 
                    "scoring_type": "simple",
                    "context": [
                            "The behavior is different during training and validation: during training, we use the mean and standard deviation of the batch to normalize the data, while during validation we instead use a running mean of the statistics calculated during training."
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        },
        {
            "chapter": 13,
            "question_number": 40,
            "question_text": "\"Why do models with batch normalization layers generalize better?\"",
            "gold_standard_answer": "\"Most researchers believe that the reason for this is that batch normalization adds some extra randomness to the training process.\"",
            "answer_context": [
                {
                    "answer_component": "\"Most researchers believe that the reason for this is that batch normalization adds some extra randomness to the training process.\"", 
                    "scoring_type": "simple",
                    "context": [
                            "An interesting observation about models containing batch normalization layers is that they tend to generalize better than models that don't contain them. Although we haven't as yet seen a rigorous analysis of what's going on here, most researchers believe that the reason for this is that batch normalization adds some extra randomness to the training process. Each mini-batch will have a somewhat different mean and standard deviation than other mini-batches. Therefore, the activations will be normalized by different values each time. In order for the model to make accurate predictions, it will have to learn to become robust to these variations. In general, adding additional randomization to the training process often helps"
                    ],
                    "explicit_context": "true",
                    "extraneous_answer": "false"
                }
            ],
            "question_context": []
        }
    ]
}
