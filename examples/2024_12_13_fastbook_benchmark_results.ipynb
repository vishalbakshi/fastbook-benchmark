{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGET8JJOLCzT"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers -Uqq\n",
        "!pip install -qq RAGatouille\n",
        "!pip install ftfy -qq\n",
        "\n",
        "import sqlite3\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import pandas as pd, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from ftfy import fix_text\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from ragatouille import RAGPretrainedModel\n",
        "from ragatouille.data import CorpusProcessor\n",
        "\n",
        "corpus_processor = CorpusProcessor()\n",
        "emb_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urls = {\n",
        "    '01_intro.ipynb': 'https://drive.google.com/uc?export=view&id=1mmBjFH_plndPBC4iRZHChfMazgBxKK4_',\n",
        "    '02_production.ipynb': 'https://drive.google.com/uc?export=view&id=1Cf5QHthHy1z13H0iu3qrzAWgquCfqVHk',\n",
        "    '04_mnist_basics.ipynb': 'https://drive.google.com/uc?export=view&id=113909_BNulzyLIKUNJHdya0Hhoqie30I',\n",
        "    '08_collab.ipynb': 'https://drive.google.com/uc?export=view&id=1BtvStgFjUtvtqbSZNrL7Y2N-ey3seNZU',\n",
        "    '09_tabular.ipynb': 'https://drive.google.com/uc?export=view&id=1rHFvwl_l-AJLg_auPjBpNrOgG9HDnfqg',\n",
        "    '10_nlp.ipynb': 'https://drive.google.com/uc?export=view&id=1pg1pH7jMMElzrXS0kBBz14aAuDsi2DEP',\n",
        "    '13_convolutions.ipynb': 'https://drive.google.com/uc?export=view&id=19P-eEHpAO3WrOvdxgXckyhHhfv_R-hnS'\n",
        "}\n",
        "\n",
        "def download_file(url, filename):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Open the file in write-binary mode\n",
        "        with open(filename, 'wb') as file:\n",
        "            # Write the content of the response to the file\n",
        "            file.write(response.content)\n",
        "        print(f\"File downloaded successfully: {filename}\")\n",
        "    else:\n",
        "        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
        "\n",
        "for fname, url in urls.items():\n",
        "  download_file(url, fname)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7itmnSdmLnQW",
        "outputId": "2bc9b84c-41de-45a9-c485-9617921c4a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully: 01_intro.ipynb\n",
            "File downloaded successfully: 02_production.ipynb\n",
            "File downloaded successfully: 04_mnist_basics.ipynb\n",
            "File downloaded successfully: 08_collab.ipynb\n",
            "File downloaded successfully: 09_tabular.ipynb\n",
            "File downloaded successfully: 10_nlp.ipynb\n",
            "File downloaded successfully: 13_convolutions.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nbs = {\n",
        "    '1': '01_intro.ipynb',\n",
        "    '2': '02_production.ipynb',\n",
        "    '4': '04_mnist_basics.ipynb',\n",
        "    '8': '08_collab.ipynb',\n",
        "    '9': '09_tabular.ipynb',\n",
        "    '10': '10_nlp.ipynb',\n",
        "    '13': '13_convolutions.ipynb'\n",
        "}"
      ],
      "metadata": {
        "id": "aGUDmPMTLLcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://gist.githubusercontent.com/vishalbakshi/2c22ca69ac7bc4bc845052c1b9d949c8/raw/d498259f2fc75d27c485ddc73933f145987feef3/cs_bm25_baselines.csv'\n",
        "questions = pd.read_csv(url).query(\"is_answerable == 1\")[[\"chapter\", \"question_number\", \"question_text\", \"answer\", \"keywords\"]]\n",
        "\n",
        "# remove double quotations from the question text\n",
        "# as these affect embeddings/cosine similarity: https://vishalbakshi.github.io/blog/posts/2024-11-08-punctuation-cosine-similarity/\n",
        "questions['question_text'] = questions['question_text'].str.strip('\"\\'')\n",
        "assert questions.shape == (191,5)"
      ],
      "metadata": {
        "id": "7E68WNUmLYOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_file(\n",
        "    \"https://gist.githubusercontent.com/vishalbakshi/a507b6e9e893475e93a4141e96b8947d/raw/e32835ba1dbf94384943ed5a65404112e1c89df2/fastbook-benchmark.json\",\n",
        "    \"fastbook-benchmark.json\"\n",
        "    )\n",
        "\n",
        "def load_benchmark():\n",
        "    # Load the benchmark data\n",
        "    with open('fastbook-benchmark.json', 'r') as f:\n",
        "        benchmark = json.load(f)\n",
        "    return benchmark\n",
        "\n",
        "benchmark = load_benchmark()\n",
        "assert len(benchmark['questions']) == 191"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-i67X5gLdCu",
        "outputId": "888d9466-b6be-4cde-de2f-6d6ad5a294a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully: fastbook-benchmark.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(chunks, db_path, chapter=1):\n",
        "    try:\n",
        "        # create virtual table if database doesn't exist\n",
        "        if not os.path.exists(db_path):\n",
        "            with sqlite3.connect(db_path) as conn:\n",
        "              cur = conn.cursor()\n",
        "              cur.execute(\"\"\"\n",
        "              CREATE VIRTUAL TABLE fastbook_text\n",
        "              USING FTS5(chapter, text);\n",
        "              \"\"\")\n",
        "              conn.commit()\n",
        "\n",
        "        # load in the chunks for each chapter\n",
        "        with sqlite3.connect(db_path) as conn:\n",
        "            cur = conn.cursor()\n",
        "\n",
        "            for chunk in chunks:\n",
        "                cur.execute(\"INSERT INTO fastbook_text(chapter, text) VALUES (?, ?)\", (chapter, chunk))\n",
        "\n",
        "            conn.commit()\n",
        "            res = cur.execute(\"SELECT * FROM fastbook_text WHERE chapter = ?\", (chapter,)).fetchall()\n",
        "        # make sure all the data was loaded into the database\n",
        "        if len(res) != len(chunks):\n",
        "            raise ValueError(f\"Number of inserted chunks ({len(res)}) doesn't match input chunks ({len(chunks)})\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except sqlite3.Error as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "UFkd1klYL1aI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def db_search(df, limit=1):\n",
        "  results = []\n",
        "  with sqlite3.connect('fastbook.db') as conn:\n",
        "    cur = conn.cursor()\n",
        "    # concatenate the keywords into a string \"keyword1 OR keyword 2 OR keyword3 ...\"\n",
        "    for _, row in df.iterrows():\n",
        "      keywords = ' OR '.join([f'\"{keyword.strip(\",\")}\"' for keyword in row['keywords'].replace('\"', '').split()])\n",
        "\n",
        "      q = f\"\"\"\n",
        "        SELECT text, rank\n",
        "        FROM fastbook_text\n",
        "        WHERE fastbook_text MATCH ?\n",
        "        AND chapter = ?\n",
        "        ORDER BY rank\n",
        "        LIMIT ?\n",
        "        \"\"\"\n",
        "      res = cur.execute(q, (keywords, str(row['chapter']), limit)).fetchall()\n",
        "      # grab the retrieved chunk from the query results\n",
        "      res = [item[0] for item in res]\n",
        "\n",
        "      # if there are multiple chunks retrieved, combine them into a single string\n",
        "      results.append(res)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "FvUIQBC2L3uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fts_retrieval(data, df, chunk_size):\n",
        "    if os.path.exists(\"fastbook.db\"):\n",
        "        os.remove(\"fastbook.db\")\n",
        "\n",
        "    for chapter, chunks in data.items():\n",
        "        documents = corpus_processor.process_corpus(chunks, chunk_size=chunk_size)\n",
        "        documents = [doc['content'] for doc in documents]\n",
        "        assert load_data(documents, 'fastbook.db', chapter)\n",
        "\n",
        "    results = db_search(df, limit=10)\n",
        "    assert len(results) == 191\n",
        "\n",
        "    for res in results:\n",
        "        assert len(res) <= 10\n",
        "\n",
        "    return results, documents"
      ],
      "metadata": {
        "id": "pALemnCIL5Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def single_vector_retrieval(data, benchmark, chunk_size):\n",
        "    # Group questions by chapter\n",
        "    questions = {}\n",
        "    for q in benchmark[\"questions\"]:\n",
        "        chapter = str(q[\"chapter\"])\n",
        "        if chapter not in questions:\n",
        "            questions[chapter] = []\n",
        "        questions[chapter].append(q['question_text'].strip('\"\\''))\n",
        "\n",
        "    q_embs = {}\n",
        "    for chapter, _ in data.items():\n",
        "        qs = questions[chapter]\n",
        "        q_embs[chapter] = emb_model.encode(qs, convert_to_tensor=True)\n",
        "\n",
        "    results = []\n",
        "    for chapter, chunks in data.items():\n",
        "        # chunk chapter text\n",
        "        documents = corpus_processor.process_corpus(chunks, chunk_size=chunk_size)\n",
        "        documents = [doc['content'] for doc in documents]\n",
        "\n",
        "        # Embed documents\n",
        "        data_embs = emb_model.encode(documents, convert_to_tensor=True)\n",
        "\n",
        "        # Compute cosine similarity and get top 10 indices for each row\n",
        "        idxs = F.cosine_similarity(q_embs[chapter].unsqueeze(1), data_embs.unsqueeze(0), dim=2).sort(descending=True)[1]\n",
        "        top_10_idxs = idxs[:, :10]  # Get the top 10 indices for each row\n",
        "\n",
        "        # Extract top 10 chunks for each row\n",
        "        top_10_chunks = [\n",
        "            [documents[idx.item()] for idx in row_idxs]\n",
        "            for row_idxs in top_10_idxs\n",
        "        ]\n",
        "        results.extend(top_10_chunks)\n",
        "\n",
        "    assert len(results) == 191\n",
        "\n",
        "    for res in results:\n",
        "        assert len(res) <= 10\n",
        "\n",
        "    return results, documents"
      ],
      "metadata": {
        "id": "a5XFUsF4L9Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def index_free_retrieval(data, model_nm, chunk_size, benchmark):\n",
        "    questions_by_chapter = {}\n",
        "    for q in benchmark[\"questions\"]:\n",
        "        chapter = str(q[\"chapter\"])\n",
        "        if chapter not in questions_by_chapter:\n",
        "            questions_by_chapter[chapter] = []\n",
        "        questions_by_chapter[chapter].append(q)\n",
        "\n",
        "    # Dictionary to store results per chapter\n",
        "    chapter_results = {}\n",
        "\n",
        "    # Process each chapter separately\n",
        "    for chapter in nbs.keys():\n",
        "        # instantiate new RAG object\n",
        "        RAG = RAGPretrainedModel.from_pretrained(model_nm)\n",
        "\n",
        "        # Get questions for this chapter\n",
        "        chapter_questions = questions_by_chapter[chapter]\n",
        "\n",
        "        # encode chapter documents\n",
        "        documents = corpus_processor.process_corpus(data[chapter], chunk_size=chunk_size)\n",
        "        RAG.encode([x['content'] for x in documents], document_metadatas=[{\"chapter\": chapter} for _ in range(len(documents))])\n",
        "\n",
        "        # Perform retrieval for each question in this chapter\n",
        "        results = []\n",
        "        for q in chapter_questions:\n",
        "            top_k = min(10, len(documents))\n",
        "            retrieved = RAG.search_encoded_docs(query = q[\"question_text\"].strip('\"\\''), k=top_k)\n",
        "            results.append(retrieved)\n",
        "\n",
        "        # Store results\n",
        "        chapter_results[chapter] = results\n",
        "\n",
        "    results = []\n",
        "    for chapter, res in chapter_results.items():\n",
        "        results.extend(res)\n",
        "\n",
        "    assert len(results) == 191\n",
        "\n",
        "    final_results = []\n",
        "    for res in results:\n",
        "        assert len(res) <= 10\n",
        "        intermediate_results = [r['content'] for r in res]\n",
        "        final_results.append(intermediate_results)\n",
        "\n",
        "    assert len(final_results) == 191\n",
        "    return final_results, documents"
      ],
      "metadata": {
        "id": "pLuKyOxzMINa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mrr(question, retrieved_passages, cutoff=10):\n",
        "    retrieved_passages = retrieved_passages[:cutoff]\n",
        "    highest_rank = 0\n",
        "\n",
        "    for ans_comp in question[\"answer_context\"]:\n",
        "        contexts = ans_comp.get(\"context\", [])\n",
        "        component_found = False\n",
        "\n",
        "        for rank, passage in enumerate(retrieved_passages, start=1):\n",
        "            if any(fix_text(context) in fix_text(passage) for context in contexts):\n",
        "                highest_rank = max(highest_rank, rank)\n",
        "                component_found = True\n",
        "                break\n",
        "\n",
        "        if not component_found:\n",
        "            return 0.0\n",
        "\n",
        "    return 1.0/highest_rank if highest_rank > 0 else 0.0"
      ],
      "metadata": {
        "id": "et0MrvgQOOTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_recall(question, retrieved_passages, cutoff=10):\n",
        "    retrieved_passages = retrieved_passages[:cutoff]\n",
        "\n",
        "    # Track if we've found at least one context for each answer component\n",
        "    ans_comp_found = []\n",
        "\n",
        "    for ans_comp in question[\"answer_context\"]:\n",
        "        contexts = ans_comp.get(\"context\", [])\n",
        "        found = False\n",
        "\n",
        "        # Check if any context for this answer component appears in retrieved passages\n",
        "        for passage in retrieved_passages:\n",
        "            if any(fix_text(context) in fix_text(passage) for context in contexts):\n",
        "                found = True\n",
        "                break\n",
        "\n",
        "        ans_comp_found.append(found)\n",
        "\n",
        "    # Recall is ratio of answer components with at least one found context\n",
        "    return sum(ans_comp_found) / len(ans_comp_found)"
      ],
      "metadata": {
        "id": "Q9RTuA1EOQAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_retrieval(results, benchmark):\n",
        "    q_mrr = []\n",
        "    q_recall = []\n",
        "\n",
        "    for i, question in enumerate(benchmark[\"questions\"]):\n",
        "        mrr = calculate_mrr(question, results[i], cutoff=10)\n",
        "        recall = calculate_recall(question, results[i], cutoff=10)\n",
        "        q_mrr.append(mrr)\n",
        "        q_recall.append(recall)\n",
        "\n",
        "    assert len(q_mrr) == len(benchmark[\"questions\"])\n",
        "    assert len(q_recall) == len(benchmark[\"questions\"])\n",
        "\n",
        "    return q_mrr, q_recall"
      ],
      "metadata": {
        "id": "v1TiA7AtOLw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_results(results, df, q_mrr, q_recall, name):\n",
        "    flat_results = ['\\n\\n'.join(res) for res in results]\n",
        "\n",
        "    assert len(flat_results) == 191\n",
        "\n",
        "    df[f'{name}_retrieval'] = flat_results\n",
        "    df[f'{name}_mrr10'] = q_mrr\n",
        "    df[f'{name}_recall10'] = q_recall\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "aE3VywlZMiXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def do_retrieval(method, chunk_size, data, benchmark, questions=None, benchmark_results=None):\n",
        "    if method == \"bm25\": results, docs = fts_retrieval(data, questions, chunk_size)\n",
        "    if method == \"single_vector\": results, docs = single_vector_retrieval(data, benchmark, chunk_size)\n",
        "    if method == \"colbertv2\": results, docs = index_free_retrieval(data=data, model_nm=\"colbert-ir/colbertv2.0\", chunk_size=chunk_size, benchmark=benchmark)\n",
        "    if method == \"answerai_colbert\": results, docs = index_free_retrieval(data=data, model_nm=\"answerdotai/answerai-colbert-small-v1\", chunk_size=chunk_size, benchmark=benchmark)\n",
        "\n",
        "    name = f\"{method}_{chunk_size}\"\n",
        "    q_mrr, q_recall = score_retrieval(results, benchmark)\n",
        "    #benchmark_results = save_results(results, benchmark_results, q_mrr, q_recall, name=name)\n",
        "    #return pd.Series(q_mrr).mean(), pd.Series(q_recall).mean()\n",
        "\n",
        "    return results, docs"
      ],
      "metadata": {
        "id": "6lcIwguUML4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def notebook_to_string(path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        notebook = json.load(f)\n",
        "\n",
        "    all_text = ''\n",
        "    found_questionnaire = False\n",
        "\n",
        "    for cell in notebook['cells']:\n",
        "        if cell['cell_type'] == 'markdown' and any('## Questionnaire' in line for line in cell['source']):\n",
        "            found_questionnaire = True\n",
        "            break\n",
        "\n",
        "        if cell['cell_type'] in ['markdown', 'code']:\n",
        "            all_text += ''.join(cell['source']) + '\\n'\n",
        "    return all_text"
      ],
      "metadata": {
        "id": "qd39XqNCMraH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_string(text, n):\n",
        "    \"\"\"Split text into n chunks.\"\"\"\n",
        "    skip = int(len(text) / n)\n",
        "    return [text[i:i + skip] for i in range(0, len(text), skip)]"
      ],
      "metadata": {
        "id": "cTG0ObMKM_gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_html(text):\n",
        "    # Step 1: Temporarily replace double-bracketed content with a placeholder\n",
        "    import uuid\n",
        "    placeholder = f\"PLACEHOLDER_{uuid.uuid4()}\"\n",
        "    double_bracketed = re.findall(r'<<[^>]*>>', text)\n",
        "    step1 = re.sub(r'<<[^>]*>>', placeholder, text)\n",
        "\n",
        "    # Step 2: Remove HTML tags\n",
        "    step2 = re.sub(r'<[/]?[a-zA-Z][^>]*>', '', step1)\n",
        "\n",
        "    # Step 3: Restore double-bracketed content\n",
        "    if double_bracketed:\n",
        "        step3 = step2.replace(placeholder, double_bracketed[0])\n",
        "        return step3\n",
        "    return step2"
      ],
      "metadata": {
        "id": "Am8zIK-rNC1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):\n",
        "    import string\n",
        "    return ''.join(char if char.isalnum() else ' ' if char in string.punctuation else char for char in text)"
      ],
      "metadata": {
        "id": "plrykwU6NE-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_contexts(data):\n",
        "    # Process questions\n",
        "    for question in data['questions']:\n",
        "        # Process only answer_context\n",
        "        if 'answer_context' in question:\n",
        "            for context_item in question['answer_context']:\n",
        "                if 'context' in context_item:\n",
        "                    if isinstance(context_item['context'], list):\n",
        "                        # If context is a list, process each string in the list\n",
        "                        context_item['context'] = [\n",
        "                            remove_punctuation(text) if text else text\n",
        "                            for text in context_item['context']\n",
        "                        ]\n",
        "                    elif isinstance(context_item['context'], str):\n",
        "                        # If context is a single string, process it directly\n",
        "                        context_item['context'] = remove_punctuation(context_item['context'])\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "CR0QuCrONpXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Background"
      ],
      "metadata": {
        "id": "INAVs8W-N4mx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, I'm running all retrieval methods so that I can manually check their MRR@10 and Recall@10 and compare with my functions. I'll start with performing retrieval on chapter text as is (with punctuation) so that it's easier to read."
      ],
      "metadata": {
        "id": "5kKd74nEN5zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_no_pp = {}\n",
        "n_chars = 0\n",
        "\n",
        "for chapter, nb in nbs.items():\n",
        "    data_no_pp[chapter] = chunk_string(notebook_to_string(nb), 2)\n",
        "    for c in data_no_pp[chapter]:\n",
        "        n_chars += len(c)\n",
        "\n",
        "assert n_chars == 503769"
      ],
      "metadata": {
        "id": "eOUV1CeFPQFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark = load_benchmark()\n",
        "assert len(benchmark['questions']) == 191\n",
        "\n",
        "processed_benchmark = process_contexts(benchmark)\n",
        "assert len(processed_benchmark['questions']) == 191"
      ],
      "metadata": {
        "id": "0K4zWPBaPVmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sS05oHs1PYel",
        "outputId": "b8fdb797-4bb8-45d9-91b4-ae7bd7bc0650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark_results = questions.copy()\n",
        "benchmark_results.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wj9AwICIQegF",
        "outputId": "42b2ef64-bbc4-45c3-9f46-dadf5f7688a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(191, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "documents = []\n",
        "\n",
        "for method in [\"bm25\", \"single_vector\", \"colbertv2\", \"answerai_colbert\"]:\n",
        "  for chunk_size in [500]:\n",
        "    res, docs = do_retrieval(\n",
        "        method,\n",
        "        chunk_size,\n",
        "        data_no_pp,\n",
        "        load_benchmark(),\n",
        "        questions)\n",
        "\n",
        "    results.append((method, chunk_size, res))\n",
        "    documents.append((method, chunk_size, docs))"
      ],
      "metadata": {
        "id": "f9feyv3SP_Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(results) # four methods, 1 chunk size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9_d_kklXGrI",
        "outputId": "d45a6d4d-bfa1-4997-9201-a00a6cb6cb76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(results[0]) # method, chunk_size, res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXy2SdoTl2cV",
        "outputId": "bad4d491-e758-4474-9d2d-299155a89b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results[3][0], results[3][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZGlPUpxl5Nv",
        "outputId": "32d79d6f-f85b-4f78-8e89-fb788839dd1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('answerai_colbert', 500)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark_results = questions.copy()\n",
        "benchmark_results.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb7nkw4Bmuuu",
        "outputId": "3b0c7e9d-0a2f-401c-a6aa-115f03410530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(191, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark = load_benchmark()\n",
        "for method, chunk_size, res in results:\n",
        "    if chunk_size in [500]:\n",
        "        q_mrr, q_recall = score_retrieval(res, benchmark)\n",
        "        benchmark_results[f'{method}_{chunk_size}'] = ['\\n\\n================================================\\n'.join(r) for r in res]\n",
        "        benchmark_results[f'{method}_{chunk_size}_mrr10'] = q_mrr\n",
        "        benchmark_results[f'{method}_{chunk_size}_recall10'] = q_recall"
      ],
      "metadata": {
        "id": "DwxKzSZgl7pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark_results.to_csv(\"2024-12-13-fastbook-benchmark-results.csv\", index=False)"
      ],
      "metadata": {
        "id": "7hVVXwPxR6Vo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}